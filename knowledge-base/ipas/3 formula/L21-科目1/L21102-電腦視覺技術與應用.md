# L21102 - 電腦視覺技術與應用

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

電腦視覺（Computer Vision, CV）是人工智慧領域中使機器具備「視覺理解能力」的核心技術，目標是讓機器能夠「看懂世界」。CV 技術讓電腦能從靜態影像或動態影片中擷取、辨識並解釋語意資訊，最終進行分析、理解及決策。從早期的手工特徵萃取到現代的深度學習與生成式 AI 整合，電腦視覺已深入滲透到自動駕駛、安防監控、醫療影像、智慧製造與零售服務等領域。

電腦視覺是一個跨領域的整合技術，涵蓋影像處理、模式辨識、機器學習、深度學習等技術。2012 年 AlexNet 在 ImageNet 競賽中的突破性成就，標誌著深度學習主導電腦視覺的時代來臨，此後基於 CNN 的技術迅速成為 AI 領域不可或缺的一部分。

### 1.2 核心概念

**電腦視覺的五大核心概念：**

1. **影像分類（Image Classification）**：將整張影像歸類為一個或多個類別
2. **物件偵測（Object Detection）**：同時進行物體的定位與分類，輸出邊界框
3. **影像分割（Image Segmentation）**：將影像中每個像素分配至特定類別
4. **特徵提取（Feature Extraction）**：從影像中自動學習有意義的特徵表示
5. **生成式視覺（Generative Vision）**：從文字描述生成圖像或進行風格轉換

### 1.3 CFDS 分解

基於 Formula-Contract 方法論，將電腦視覺系統分解為四個基本單元：

```
CV = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = Preprocessing ∘ FeatureExtraction ∘ ModelInference ∘ Postprocessing
  = 前處理 -> 特徵提取 -> 模型推理 -> 後處理
```

**F (Files - 配置資源)**
```
F = {預訓練模型權重, 標註數據, 影像增強配置, 檢測閾值參數}
```

**D (Data - 資料結構)**
```
D = RawImages + AnnotatedData + FeatureMaps + PredictionResults
  = 原始影像 + 標註數據 + 特徵圖 + 預測結果
```

**S (State - 運行狀態)**
```
S = TrainingState | ValidationState | InferenceState
  = 訓練狀態 | 驗證狀態 | 推理狀態
```

### 1.4 技術定位

電腦視覺與自然語言處理（NLP）並列為 AI 的兩大感知技術，CV 處理視覺感知，NLP 處理語言理解。在 AI 技術棧中，CV 處於應用層核心位置，是智慧監控、無人駕駛、醫療影像診斷、工業檢測等應用的基礎技術。

電腦視覺讓機器能「看懂」物理世界，在智慧城市、自動化生產、輔助診斷等領域發揮關鍵作用，成為推動產業智慧化的重要引擎。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 CV 處理流程主公式

```
CV_Pipeline = Output ∘ ModelInference ∘ FeatureExtraction ∘ Preprocessing ∘ Input
            = f(g(h(k(x))))
```

展開為順序流程：
```
InputImage -> Preprocessing -> FeatureExtraction -> ModelInference -> Postprocessing -> Output
原始影像 -> 前處理 -> 特徵提取 -> 模型推理 -> 後處理 -> 輸出結果
```

### 2.2 技術演進公式

電腦視覺技術經歷三個階段的演進：

```
CV_Evolution = HandcraftedFeatures -> CNN_Era -> MultimodalGenAI
```

詳細展開：
```
第一階段：HandcraftedFeatures = EdgeDetection + SIFT + SURF + ORB
         手工特徵 = 邊緣檢測 + 尺度不變特徵轉換 + 加速穩健特徵 + 二進制魯棒獨立基本特徵

第二階段：CNN_Era = AlexNet -> VGG -> ResNet -> EfficientNet
         CNN時代 = 深度學習興起 -> 深層網路 -> 殘差結構 -> 高效網路

第三階段：MultimodalGenAI = CLIP + ViT + DALL·E + SAM
         多模態生成AI = 視覺語言對比學習 + 視覺Transformer + 文生圖 + 通用分割模型
```

### 2.3 CNN 核心公式

**卷積運算（Convolution）**：
```
Conv(I, K) = Σ_m Σ_n I(x+m, y+n) × K(m,n)
```

符號說明：
- **I (Input)**：輸入影像矩陣
- **K (Kernel)**：卷積核（濾波器）
- **(x, y)**：影像上的位置座標
- **(m, n)**：卷積核內的偏移量
- **Σ**：對卷積核所有位置求和

**卷積層輸出維度計算**：
```
Output_size = [(Input_size - Kernel_size + 2 × Padding) / Stride] + 1
```

### 2.4 Vision Transformer (ViT) 核心公式

**圖像分塊（Patch Embedding）**：
```
ViT = ClassificationHead ∘ TransformerEncoder^L ∘ PatchEmbedding

PatchEmbedding：將影像切分為多個 patches
  Image(H×W×C) -> Patches(N × (P²×C))

其中：
  N = (H×W) / P²  (patch 數量)
  P = patch 大小
  C = 通道數
```

**Self-Attention 在視覺中的應用**：
```
Attention(Q, K, V) = softmax(QK^T / √d_k) × V
```

ViT 將影像視為序列處理，每個 patch 類似 NLP 中的 token。

### 2.5 物件偵測公式

**YOLO（You Only Look Once）原理**：
```
YOLO = GridDivision × (BoundingBoxPrediction + ClassPrediction)

流程：
Input Image -> Grid(S×S) -> Predict(B boxes per cell) -> NMS -> Final Detections

每個 Grid Cell 預測：
  - B 個邊界框（Bounding Boxes）
  - 每個框的信心分數（Confidence）
  - C 個類別機率（Class Probabilities）
```

**IOU（Intersection over Union）計算**：
```
IOU = Area(Intersection) / Area(Union)
    = (重疊區域面積) / (總覆蓋區域面積)

判斷標準：
  IOU > 0.5 或 0.7 視為成功預測
```

### 2.6 影像分割公式

**語意分割 vs 實例分割**：
```
SemanticSegmentation：將每個像素分類為類別（不區分個體）
  Output = ClassMap(H×W)  // 每個像素一個類別標籤

InstanceSegmentation：區分同類別的不同個體
  Output = InstanceMap(H×W)  // 每個像素一個實例ID
```

**FCN（Fully Convolutional Network）**：
```
FCN = Upsampling ∘ FeatureExtraction ∘ Convolution

將 CNN 的全連接層替換為卷積層，實現端到端的像素級預測
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 CV 技術演進對比表

| 技術階段 | 代表技術 | 優點 | 缺點 | 適用場景 | 複雜度 |
|---------|---------|------|------|---------|--------|
| **手工特徵** | SIFT、SURF、HOG | 可解釋性高、無需訓練 | 泛化能力差、特徵設計困難 | 簡單物體辨識 | 低 |
| **CNN時代** | AlexNet、VGG、ResNet | 自動特徵學習、效果顯著 | 需大量數據與算力 | 影像分類、物件偵測 | 中高 |
| **多模態生成AI** | CLIP、ViT、DALL·E | 跨模態理解、生成能力 | 計算資源需求極高 | 文生圖、通用視覺理解 | 高 |

### 3.2 影像任務類型對比

| 任務類型 | 輸出形式 | 粒度 | 計算複雜度 | 典型應用 |
|---------|---------|------|-----------|---------|
| **影像分類** | 類別標籤 | 整張影像 | 低 | 相片分類、品質檢測 |
| **物件偵測** | 邊界框+類別 | 物件級別 | 中 | 人臉辨識、車牌辨識 |
| **語意分割** | 像素級類別圖 | 像素級別 | 高 | 自動駕駛場景理解 |
| **實例分割** | 像素級實例圖 | 像素級別（區分個體） | 極高 | 醫療影像分析 |

### 3.3 經典 CNN 架構對比

| 模型 | 年份 | 層數 | 關鍵特色 | ImageNet Top-5 錯誤率 |
|-----|------|------|---------|---------------------|
| **AlexNet** | 2012 | 8 | ReLU + Dropout | 16.4% |
| **VGG** | 2014 | 19 | 小卷積核（3×3） | 7.3% |
| **ResNet** | 2015 | 152 | 殘差連接 | 3.57% |
| **EfficientNet** | 2019 | 可調 | 複合縮放 | 2.9% |

### 3.4 物件偵測模型對比

| 模型 | 類型 | 速度 | 準確度 | 適用場景 |
|-----|------|------|--------|---------|
| **YOLO** | 單階段 | 快（實時） | 中 | 實時監控、自駕車 |
| **Faster R-CNN** | 雙階段 | 慢 | 高 | 精確偵測需求 |
| **SSD** | 單階段 | 中 | 中高 | 平衡速度與準確度 |

### 3.5 選擇決策樹

```
任務需求
├─ 需要實時處理？
│  ├─ 是 -> YOLO（速度優先）
│  └─ 否 -> 繼續判斷
│     ├─ 需要極高準確度？ -> Faster R-CNN（準確度優先）
│     └─ 需要平衡？ -> SSD（速度與準確度平衡）
├─ 需要像素級分割？
│  ├─ 語意分割 -> FCN / U-Net
│  └─ 實例分割 -> Mask R-CNN
└─ 資源有限？
   ├─ 是 -> MobileNet / EfficientNet（輕量化模型）
   └─ 否 -> ResNet / ViT（高性能模型）
```

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一：智慧監控與安防

**場景描述**：企業或公共場所需要即時監控人員進出、異常行為偵測。

**技術應用方式**：
```
SmartSurveillance = FaceRecognition + BehaviorAnalysis + AnomalyDetection
                  = 人臉辨識 + 行為分析 + 異常偵測

流程：攝影機影像 -> YOLO物件偵測 -> 人臉特徵提取 -> 身份比對 -> 行為分析 -> 異常警報
```

**實現要點**：
1. 使用 YOLO 進行即時人員偵測與追蹤
2. 提取人臉特徵並與資料庫比對（如 FaceNet 嵌入）
3. 透過姿態估計（Pose Estimation）分析行為模式
4. 設定異常行為規則（如跌倒、聚眾、闖入禁區）
5. 即時警報系統與影像回放

### 4.2 應用場景二：自動駕駛視覺感知

**場景描述**：無人駕駛車輛需要即時理解道路環境，偵測行人、車輛、交通標誌。

**技術應用方式**：
```
AutonomousDriving = LaneDetection + ObjectDetection + SemanticSegmentation
                  = 車道檢測 + 物件偵測 + 場景分割

多任務學習：
  Camera Input -> CNN Backbone -> {Lane, Objects, Segmentation} -> Decision Making
```

**實現要點**：
1. **語意分割**：識別道路、人行道、建築物、天空等場景元素
2. **物件偵測**：即時偵測車輛、行人、自行車、交通標誌
3. **車道線檢測**：確定車道位置，輔助轉向決策
4. **多感測器融合**：結合攝影機、雷達、光達（LiDAR）數據
5. **端到端模型**：直接從影像輸出駕駛指令

### 4.3 應用場景三：醫療影像診斷輔助

**場景描述**：醫師需要從 X 光、CT、MRI 影像中快速準確地發現病灶。

**技術應用方式**：
```
MedicalImaging = LesionDetection + ImageSegmentation + DiagnosisSupport
               = 病灶檢測 + 影像分割 + 診斷輔助

流程：醫療影像 -> 前處理(正規化) -> U-Net分割 -> 病灶定位 -> 特徵分析 -> 輔助診斷
```

**實現要點**：
1. 使用 U-Net 進行精確的器官與病灶分割
2. 針對醫療影像的特殊性進行數據增強（旋轉、縮放、亮度調整）
3. 少樣本學習（Few-shot Learning）應對罕見疾病
4. 可解釋性 AI（Explainable AI）提供診斷依據視覺化
5. 與醫師專業知識結合，作為輔助工具而非替代

### 4.4 實作步驟（通用流程）

**電腦視覺專案實作八步驟**：
```
Step1：需求分析 -> 確定任務類型（分類/偵測/分割）
Step2：數據收集 -> 獲取標註影像數據
Step3：數據標註 -> Bounding Box / Mask / Class Label
Step4：數據前處理 -> Resize + Normalize + Augmentation
Step5：模型選擇 -> 根據任務選擇架構（CNN/ViT/YOLO）
Step6：模型訓練 -> Transfer Learning + Fine-tuning
Step7：模型評估 -> Accuracy / mAP / IOU 等指標
Step8：部署優化 -> 模型量化 + 推理加速 + API封裝
```

### 4.5 常見陷阱與解決方案

**陷阱1：標註不一致導致模型混亂**
- 問題：不同標註者對邊界框大小判斷不一致
- 解決：建立詳細標註指南 + 雙重標註驗證 + 品質控制機制

**陷阱2：數據不平衡導致模型偏差**
- 問題：某類別樣本過多，模型傾向預測該類別
- 解決：數據增強（Data Augmentation） + 類別權重調整 + 焦點損失（Focal Loss）

**陷阱3：過擬合訓練數據**
- 問題：訓練準確度高，但測試數據表現差
- 解決：Dropout + L2正則化 + 數據增強 + Early Stopping

**陷阱4：邊緣設備推理速度慢**
- 問題：模型過大，無法在邊緣設備即時推理
- 解決：模型剪枝（Pruning） + 量化（Quantization） + 知識蒸餾（Knowledge Distillation） + 使用輕量化模型（MobileNet）

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**CV 三階段演進口訣**：
```
手工卷神生，三代更替
手工特徵一代傳
卷積神經二代權
生成多模三代全
```
- **手工**：手工特徵工程時代（SIFT/SURF）
- **卷神**：卷積神經網路時代（CNN/ResNet）
- **生**：生成式與多模態時代（CLIP/DALL·E）

**CV 任務四層次口訣**：
```
分類偵測割實例
粒度漸細層層深
分類看全圖
偵測框物件
分割到像素
實例分個體
```

### 5.2 記憶技巧

**卷積運算記憶法（滑動窗口）**：
```
想像用「印章」在紙上蓋章：
卷積核 = 印章圖案
滑動過程 = 從左到右、從上到下蓋章
特徵圖 = 蓋完章的結果

每蓋一次 = 提取一個局部特徵
```

**IOU 計算記憶法（文氏圖）**：
```
想像兩個圓圈（預測框 vs 真實框）：
重疊部分 = Intersection（交集）
總覆蓋區 = Union（聯集）
IOU = 交集 / 聯集

IOU 越大 = 兩個框越接近
```

### 5.3 快速回憶提示

**考試快速回想關鍵字**：
- CV 演進 = 手工 -> CNN -> 多模態
- 分類模型 = AlexNet / VGG / ResNet
- 偵測模型 = YOLO（快） / Faster R-CNN（準）
- 分割模型 = FCN / U-Net / Mask R-CNN
- 評估指標 = Accuracy / mAP / IOU

### 5.4 易混淆辨析

**語意分割 vs 實例分割**：
- **語意分割**：「所有貓都標為貓」（不分個體）
- **實例分割**：「貓1、貓2、貓3」（區分每隻貓）
- 記憶：語意 = 語義相同即可，實例 = 實體獨立

**YOLO vs Faster R-CNN**：
- **YOLO**：「一眼看穿」（One Look），速度快
- **Faster R-CNN**：「兩步走」（先提議再分類），準確高
- 記憶：YOLO 像快速瀏覽，Faster R-CNN 像仔細檢查

**Conv vs Pooling**：
- **卷積（Convolution）**：提取特徵
- **池化（Pooling）**：降低維度
- 記憶：Conv 像放大鏡（看細節），Pooling 像縮小鏡（看大局）

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**題目1**：下列哪種技術是電腦視覺進入深度學習時代的標誌？
A. SIFT
B. AlexNet
C. YOLO
D. Transformer

**答案**：B
**解析**：AlexNet 在 2012 年 ImageNet 競賽中取得突破性成就，標誌著深度學習主導電腦視覺的時代來臨。SIFT 是手工特徵時代的技術，YOLO 是物件偵測模型，Transformer 應用於視覺是後來的 ViT。

---

**題目2**：IOU（Intersection over Union）的計算公式是什麼？
A. IOU = Intersection + Union
B. IOU = Intersection / Union
C. IOU = Intersection - Union
D. IOU = Intersection × Union

**答案**：B
**解析**：IOU = Intersection / Union，即重疊區域面積除以總覆蓋區域面積，用於衡量預測框與真實框的重合程度。

---

**題目3**：下列哪種模型主要用於像素級的影像分割？
A. AlexNet
B. YOLO
C. U-Net
D. VGG

**答案**：C
**解析**：U-Net 專為醫療影像分割設計，具有對稱的編碼器-解碼器結構和跳躍連接，適合像素級分割任務。AlexNet 和 VGG 主要用於分類，YOLO 用於物件偵測。

---

**題目4**：ResNet 的核心創新是什麼？
A. 使用小卷積核
B. 引入殘差連接
C. 增加網路深度
D. 使用 Dropout

**答案**：B
**解析**：ResNet 的核心創新是引入殘差連接（Residual Connection），解決了深層網路訓練中的梯度消失問題，使網路深度可達 152 層。

---

**題目5**：YOLO 和 Faster R-CNN 的主要差異是？
A. YOLO 是單階段，Faster R-CNN 是雙階段
B. YOLO 是雙階段，Faster R-CNN 是單階段
C. 兩者完全相同
D. YOLO 只能分類，不能偵測

**答案**：A
**解析**：YOLO 是單階段偵測器，一次性預測邊界框和類別，速度快。Faster R-CNN 是雙階段偵測器，先生成候選框再分類，準確度高但速度慢。

### 6.2 簡答題

**題目6**：請說明卷積神經網路（CNN）在影像處理中的優勢，並解釋卷積層的作用。

**答案解析**：
- **CNN 優勢**：
  1. **自動特徵學習**：無需手工設計特徵，自動從數據中學習
  2. **參數共享**：卷積核在整張影像上共享，減少參數量
  3. **局部連接**：每個神經元只連接局部區域，降低計算複雜度
  4. **平移不變性**：對影像的平移具有魯棒性
- **卷積層作用**：
  - 提取局部特徵（邊緣、紋理、形狀等）
  - 通過多層卷積逐步提取高層語意特徵
  - 生成特徵圖（Feature Map），為後續分類或偵測提供基礎

---

**題目7**：為什麼 Vision Transformer（ViT）能在電腦視覺中取得成功？與 CNN 相比有何優劣？

**答案解析**：
- **ViT 成功原因**：
  1. **全局語境建模**：Self-Attention 機制能同時關注影像所有區域，捕捉長距離依賴
  2. **可擴展性強**：隨著數據量和模型規模增大，性能持續提升
  3. **統一架構**：與 NLP 共享 Transformer 架構，便於跨模態學習
- **與 CNN 對比**：
  - **優勢**：全局感受野、可擴展性強、跨模態能力
  - **劣勢**：需要大量數據、計算資源消耗大、缺乏 CNN 的歸納偏置（inductive bias）

### 6.3 易錯點提醒

**易錯點1**：混淆語意分割與實例分割
- **正確理解**：語意分割只區分類別（所有貓標為「貓」），實例分割區分個體（貓1、貓2、貓3）
- **記憶提示**：Semantic 看語義，Instance 看實體

**易錯點2**：認為 IOU 計算時分母是兩個框面積之和
- **正確理解**：分母是聯集（Union），即 Area1 + Area2 - Intersection
- **記憶提示**：用文氏圖想像，聯集 = 總覆蓋區域（不重複計算重疊部分）

**易錯點3**：以為 YOLO 只能偵測不能分類
- **正確理解**：YOLO 同時預測邊界框和類別機率，既偵測又分類
- **記憶提示**：YOLO = Bounding Box + Class Prediction

**易錯點4**：將池化層（Pooling）與卷積層混淆
- **正確理解**：卷積層提取特徵，池化層降維（減少計算量、增強泛化）
- **記憶提示**：Conv 像「放大鏡」（提取細節），Pooling 像「縮小鏡」（降低解析度）
