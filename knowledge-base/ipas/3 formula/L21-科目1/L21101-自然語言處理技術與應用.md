# L21101 - 自然語言處理技術與應用

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

自然語言處理（Natural Language Processing, NLP）是人工智慧領域中的核心技術，旨在讓電腦能夠理解、處理並生成人類的自然語言。NLP 的核心目標是建立人類與機器之間的語言溝通橋樑，從早期的規則式語言處理到現代的生成式 AI，NLP 持續拓展 AI 的應用範圍與技術邊界。

NLP 技術賦予機器三項核心能力：理解（Understand）、處理（Process）、生成（Generate）。從文字或語音中解析語法結構與語意、轉換為電腦可操作的結構、再根據情境自動產出自然流暢的語句，形成完整的語言處理流程。

### 1.2 核心概念

**NLP 的三大核心概念：**

1. **自然語言理解（NLU）**：專注於機器如何理解人類語言的語法結構與語意邏輯，判讀使用者意圖或隱含情緒
2. **自然語言處理（NLP）**：作為總體框架，涵蓋從基本文字處理到複雜語意分析的所有語言技術
3. **自然語言生成（NLG）**：專注於機器如何產生自然語言，將數據、知識轉換為人類可讀且自然流暢的語句
4. **預訓練語言模型**：先在大規模語料庫上預訓練，再針對特定任務微調，成為當前 NLP 的主流方法
5. **語境型表示**：詞彙的向量會根據語境動態變化，有效解決多義詞問題

### 1.3 CFDS 分解

基於 Formula-Contract 方法論，將 NLP 系統分解為四個基本單元：

```
NLP = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = Tokenization ∘ Embedding ∘ Model ∘ Decoding
  = 分詞 -> 向量化 -> 模型推理 -> 解碼輸出
```

**F (Files - 配置資源)**
```
F = {預訓練模型, 詞彙表, 停用詞表, 微調參數, 系統配置}
```

**D (Data - 資料結構)**
```
D = InputText + TrainingCorpus + Embeddings + OutputText
  = 輸入文本 + 訓練語料 + 詞向量 + 輸出文本
```

**S (State - 運行狀態)**
```
S = PretrainingState | FinetuningState | InferenceState
  = 預訓練狀態 | 微調狀態 | 推理狀態
```

### 1.4 技術定位

NLP 在 AI 技術棧中處於應用層核心位置，與電腦視覺（CV）並列為 AI 的兩大感知技術。NLP 處理非結構化文本數據，連接人類語言世界與機器計算世界，是智慧客服、搜尋引擎、對話系統、內容生成等應用的基礎技術。

在企業應用中，NLP 能精準分析用戶意圖、洞察市場趨勢、提升決策效率，自然語言數據的價值已逐漸超越傳統結構化數據。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 NLP 處理流程主公式

```
NLP_Pipeline = Output ∘ Model ∘ Embedding ∘ Tokenization ∘ Input
             = f(g(h(k(x))))
```

展開為順序流程：
```
Input -> Tokenization -> Embedding -> Model -> Decoding -> Output
輸入文本 -> 分詞 -> 向量化 -> 模型處理 -> 解碼 -> 輸出結果
```

### 2.2 技術演進公式

NLP 技術經歷四個階段的演進：

```
NLP_Evolution = RuleBased -> Statistical -> DeepLearning -> PretrainedModels
```

詳細展開：
```
第一階段：RuleBased = ExpertKnowledge + ManualRules + Dictionary
         規則式 = 專家知識 + 手工規則 + 詞彙辭典

第二階段：Statistical = NGram + HMM + CRF
         統計式 = N元語法 + 隱馬可夫模型 + 條件隨機場

第三階段：DeepLearning = RNN + LSTM + GRU
         深度學習 = 循環神經網路 + 長短期記憶網路 + 門控循環單元

第四階段：PretrainedModels = Transformer × (BERT | GPT | T5)
         預訓練模型 = Transformer × (BERT或GPT或T5)
```

### 2.3 Transformer 核心公式

Transformer 是現代 NLP 的基礎架構：

```
Transformer = MultiHeadAttention ∘ FeedForward ∘ LayerNorm
```

**Self-Attention 機制**（核心運算）：
```
Attention(Q, K, V) = softmax(QK^T / √d_k) × V
```

符號說明：
- **Q (Query)**：查詢矩陣，代表當前詞要查詢什麼資訊
- **K (Key)**：鍵矩陣，代表其他詞提供的索引資訊
- **V (Value)**：值矩陣，代表其他詞的實際語意內容
- **d_k**：鍵向量的維度，用於縮放避免梯度消失
- **softmax**：將注意力分數轉為機率分佈
- **最終結果**：加權後的語意向量

**運算步驟分解**：
```
步驟1：計算相似度 = QK^T (矩陣乘法)
步驟2：縮放處理 = (QK^T) / √d_k (避免梯度問題)
步驟3：機率分佈 = softmax(縮放結果) (轉為權重)
步驟4：加權求和 = softmax結果 × V (得到最終向量)
```

### 2.4 詞向量化技術公式

**Word2Vec 原理**：
```
Word2Vec = CBOW | Skip-gram

CBOW：Context -> CenterWord
      周圍詞彙 -> 預測中心詞

Skip-gram：CenterWord -> Context
           中心詞 -> 預測周圍詞彙
```

**語意推理能力**：
```
king - man + woman ≈ queen
國王 - 男人 + 女人 ≈ 女王
```

這展示了詞向量空間中的語意運算能力。

### 2.5 預訓練語言模型公式

**BERT（雙向編碼）**：
```
BERT = BidirectionalTransformerEncoder
     = MLM + NSP
     = 遮蔽語言模型 + 句子關聯預測
```

**GPT（單向生成）**：
```
GPT = UnidirectionalTransformerDecoder
P(x) = ∏(t=1 to T) P(x_t | x_{<t})
     = 自迴歸生成機率連乘
```

### 2.6 文本表示公式

**TF-IDF 權重計算**：
```
tf-idf(t, d, D) = TF(t, d) × IDF(t, D)

其中：
TF(t, d) = 詞t在文件d中出現次數 / 文件d總詞數
IDF(t, D) = log(文件總數 / 包含詞t的文件數)
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 NLP 技術演進對比表

| 技術階段 | 代表技術 | 優點 | 缺點 | 適用場景 | 複雜度 |
|---------|---------|------|------|---------|--------|
| **規則式** | ELIZA、專家系統 | 可解釋性高、穩定可控 | 維護成本高、擴展性差 | 特定領域簡單任務 | 低 |
| **統計式** | N-gram、HMM、CRF | 自動學習、靈活性高 | 缺乏語意理解、需大量標註 | 詞性標注、NER | 中 |
| **深度學習** | RNN、LSTM、GRU | 捕捉長距離依賴、特徵自動提取 | 訓練資源大、難以並行 | 機器翻譯、序列標注 | 中高 |
| **預訓練模型** | BERT、GPT、T5 | 強大泛化能力、遷移學習 | 計算資源需求極高 | 所有 NLP 任務 | 高 |

### 3.2 詞向量技術對比

| 技術 | 類型 | 語境感知 | 多義詞處理 | 訓練成本 | 推理速度 |
|-----|------|---------|----------|---------|---------|
| **One-hot** | 離散表示 | 否 | 否 | 極低 | 極快 |
| **Word2Vec** | 靜態詞向量 | 否 | 否 | 低 | 快 |
| **GloVe** | 靜態詞向量 | 否 | 否 | 低 | 快 |
| **FastText** | 靜態詞向量 | 否 | 否 | 低 | 快 |
| **ELMo** | 動態詞向量 | 是 | 是 | 中 | 中 |
| **BERT** | 動態詞向量 | 是 | 是 | 高 | 慢 |

### 3.3 BERT vs GPT 對比

| 維度 | BERT | GPT |
|-----|------|-----|
| **架構** | 雙向 Transformer 編碼器 | 單向 Transformer 解碼器 |
| **訓練方式** | MLM（遮蔽語言模型） | 自迴歸預測 |
| **語境** | 雙向（上下文同時） | 單向（僅前文） |
| **擅長任務** | 分類、問答、NER | 生成、對話、摘要 |
| **應用場景** | 語言理解為主 | 語言生成為主 |

### 3.4 選擇決策樹

```
任務需求
├─ 需要生成文本？
│  ├─ 是 -> GPT 系列（生成能力強）
│  └─ 否 -> 繼續判斷
│     ├─ 需要分類/標注？ -> BERT 系列（理解能力強）
│     └─ 資源有限？
│        ├─ 是 -> Word2Vec/FastText（輕量級）
│        └─ 否 -> 大型預訓練模型（效果最佳）
└─ 需要處理未知詞？ -> FastText（子詞處理）
```

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一：智慧客服系統

**場景描述**：企業客服部門每天收到大量客戶諮詢，需要自動分類、理解意圖並給予適當回覆。

**技術應用方式**：
```
客服系統 = IntentClassification + EntityRecognition + ResponseGeneration
        = 意圖分類 + 實體識別 + 回覆生成

流程：用戶輸入 -> BERT意圖分類 -> NER提取關鍵資訊 -> GPT生成回覆 -> 輸出答案
```

**實現要點**：
1. 使用 BERT 進行用戶意圖分類（如：查詢訂單、退換貨、產品諮詢）
2. 透過 NER 提取訂單編號、產品名稱等關鍵實體
3. 結合知識庫檢索與 GPT 生成自然流暢的回覆
4. 持續微調模型以適應企業特定領域術語

### 4.2 應用場景二：情感分析與輿情監控

**場景描述**：電商平台需要即時分析商品評論情感傾向，監控品牌聲譽。

**技術應用方式**：
```
SentimentAnalysis = Preprocessing -> BERT_Encoding -> Classification
                  = 前處理 -> BERT編碼 -> 分類輸出

情感分類 = Positive | Negative | Neutral
        = 正面 | 負面 | 中性
```

**實現要點**：
1. 文本前處理：分詞、去除停用詞、處理表情符號
2. 使用預訓練 BERT 模型提取語意特徵
3. 在頂層加入分類器（如 softmax）輸出情感類別
4. 針對特定領域（如 3C 產品評論）進行微調

### 4.3 應用場景三：機器翻譯系統

**場景描述**：跨國企業需要即時翻譯多語言文件與溝通內容。

**技術應用方式**：
```
Translation = Encoder(SourceLang) -> Decoder(TargetLang)
           = 編碼器(源語言) -> 解碼器(目標語言)

基於 Transformer：
SourceText -> TransformerEncoder -> ContextVector -> TransformerDecoder -> TargetText
源語言文本 -> Transformer編碼 -> 語境向量 -> Transformer解碼 -> 目標語言文本
```

**實現要點**：
1. 採用 Transformer 的 Encoder-Decoder 架構
2. 使用大規模平行語料進行預訓練
3. 引入 Attention 機制處理長距離依賴
4. 針對特定領域（如醫療、法律）進行領域適應

### 4.4 實作步驟（通用流程）

**NLP 專案實作八步驟**：
```
Step1：需求分析 -> 確定任務類型（分類/生成/標注等）
Step2：數據收集 -> 獲取訓練語料與標註數據
Step3：數據前處理 -> Tokenization + Cleaning + Normalization
Step4：特徵工程 -> 選擇詞向量方法（Word2Vec/BERT等）
Step5：模型選擇 -> 根據任務選擇架構（BERT/GPT/Transformer）
Step6：模型訓練 -> Pretraining + Finetuning
Step7：模型評估 -> 準確率/F1/BLEU等指標
Step8：部署上線 -> API封裝 + 效能優化
```

### 4.5 常見陷阱與解決方案

**陷阱1：中文分詞錯誤**
- 問題：「自然語言處理」被切成「自然」「語言」「處理」
- 解決：使用專業分詞工具（jieba/CKIP）+ 自訂詞典

**陷阱2：忽略語境導致多義詞誤判**
- 問題：「銀行」在「河岸」與「金融機構」意義不同
- 解決：使用語境型詞嵌入（BERT/ELMo）

**陷阱3：訓練數據不平衡**
- 問題：正面評論遠多於負面評論，導致模型偏差
- 解決：過採樣/欠採樣 + 損失函數加權

**陷阱4：過度依賴預訓練模型而忽略領域特性**
- 問題：通用模型在專業領域（醫療/法律）表現不佳
- 解決：領域數據微調 + 專業詞彙擴充

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**NLP 三部曲口訣**：
```
理處生，三步走
理解語意第一手
處理轉換機器收
生成語句自然流
```
- **理**：理解（NLU）- 抓語意
- **處**：處理（NLP）- 轉數值
- **生**：生成（NLG）- 產文本

**技術演進口訣**：
```
規統深預四階段
規則統計深度站
預訓模型當主管
Transformer 掌大權
```
- 規則式 -> 統計式 -> 深度學習 -> 預訓練模型

### 5.2 記憶技巧

**Transformer 記憶法（QKV 查字典）**：
```
Q（Query）查詢：我要找什麼？
K（Key）索引：你能提供什麼？
V（Value）內容：實際的語意資訊

想像查字典：
Q = 你要查的字
K = 字典的索引
V = 字的解釋內容
```

**BERT vs GPT 記憶法**：
```
BERT：雙向理解，完形填空高手
      「我___吃飯」-> 看前後文猜「在」

GPT：單向生成，接龍寫作專家
     「我在吃」-> 接著寫「飯」
```

### 5.3 快速回憶提示

**考試快速回想關鍵字**：
- NLP = 理 + 處 + 生
- Transformer = QKV 查字典
- 靜態詞向量 = Word2Vec/GloVe/FastText
- 動態詞向量 = ELMo/BERT
- 生成任務 = GPT
- 理解任務 = BERT

### 5.4 易混淆辨析

**NLP vs NLU vs NLG**：
- **NLP（總框架）**：包含一切語言技術
- **NLU（理解端）**：只負責理解語意
- **NLG（生成端）**：只負責產生文本
- 記憶：NLP 是爸爸，NLU 和 NLG 是兩個兒子

**Word2Vec vs BERT 詞向量**：
- **Word2Vec**：一個詞永遠一個向量（靜態）
- **BERT**：一個詞在不同句子有不同向量（動態）
- 記憶：Word2Vec 像身分證號碼（固定），BERT 像表情（會變）

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**題目1**：下列哪種技術屬於「語境型詞嵌入」？
A. Word2Vec
B. GloVe
C. BERT
D. TF-IDF

**答案**：C
**解析**：BERT 根據上下文動態生成詞向量，屬於語境型詞嵌入。Word2Vec 和 GloVe 是靜態詞向量，TF-IDF 是統計方法。

---

**題目2**：Transformer 的核心機制是什麼？
A. 循環神經網路
B. 卷積神經網路
C. 自注意力機制
D. 隱馬可夫模型

**答案**：C
**解析**：Transformer 的核心是 Self-Attention（自注意力機制），允許模型同時關注序列中所有位置，解決長距離依賴問題。

---

**題目3**：BERT 和 GPT 的主要差異是？
A. BERT 是單向，GPT 是雙向
B. BERT 是雙向，GPT 是單向
C. BERT 用於生成，GPT 用於理解
D. 兩者完全相同

**答案**：B
**解析**：BERT 使用雙向 Transformer 編碼器，同時看前後文；GPT 使用單向 Transformer 解碼器，只看前文。BERT 擅長理解，GPT 擅長生成。

---

**題目4**：下列哪個公式正確表示 Attention 機制？
A. Attention(Q, K, V) = QK^T × V
B. Attention(Q, K, V) = softmax(QK^T / √d_k) × V
C. Attention(Q, K, V) = Q + K + V
D. Attention(Q, K, V) = Q × K × V

**答案**：B
**解析**：Attention 公式為 softmax(QK^T / √d_k) × V，其中 √d_k 用於縮放，softmax 轉為機率分佈。

---

**題目5**：NLP 技術演進的正確順序是？
A. 深度學習 -> 統計式 -> 規則式 -> 預訓練模型
B. 規則式 -> 統計式 -> 深度學習 -> 預訓練模型
C. 預訓練模型 -> 深度學習 -> 統計式 -> 規則式
D. 統計式 -> 規則式 -> 預訓練模型 -> 深度學習

**答案**：B
**解析**：正確演進順序為：規則式（1980s-1990s）-> 統計式（1990s-2010s）-> 深度學習（2010s）-> 預訓練模型（2018-至今）。

### 6.2 簡答題

**題目6**：請說明 Word2Vec 的 CBOW 和 Skip-gram 兩種模型的差異，並舉例說明。

**答案解析**：
- **CBOW（Continuous Bag of Words）**：根據上下文詞彙預測中心詞
  - 示例：「The cat **sits** on the mat」，用「The, cat, on, the, mat」預測「sits」
- **Skip-gram**：根據中心詞預測周圍上下文詞彙
  - 示例：用「sits」預測「The, cat, on, the, mat」
- **差異**：CBOW 訓練速度快，適合小語料；Skip-gram 對低頻詞效果更好，語意捕捉能力強

---

**題目7**：為什麼 BERT 能處理多義詞問題，而 Word2Vec 不能？

**答案解析**：
- **Word2Vec**：每個詞只有一個固定向量（靜態詞向量），無論「銀行」在什麼語境，向量都相同
- **BERT**：根據上下文動態生成詞向量（語境型詞嵌入），能區分「河岸的銀行」與「金融機構的銀行」
- **關鍵**：BERT 使用雙向 Transformer，同時看前後文，根據語境調整詞向量

### 6.3 易錯點提醒

**易錯點1**：混淆 NLP、NLU、NLG 三者關係
- **正確理解**：NLP 是總框架，NLU 和 NLG 是其中的子領域
- **記憶提示**：NLP = NLU + NLG + 其他處理技術

**易錯點2**：認為 Attention 公式中不需要 √d_k 縮放
- **正確理解**：√d_k 縮放是為了避免梯度消失，當 d_k 很大時，QK^T 的值會很大，導致 softmax 梯度過小
- **記憶提示**：√d_k 像「減速帶」，防止數值爆炸

**易錯點3**：以為 GPT 完全無法理解語意
- **正確理解**：GPT 雖然是單向模型，但透過大規模預訓練，也具備強大的語言理解能力，只是在某些需要雙向語境的任務上不如 BERT
- **記憶提示**：GPT 像「只看前文的聰明人」，雖然看不到後文，但仍能推理

**易錯點4**：將所有詞向量技術都歸類為「詞嵌入」
- **正確理解**：詞嵌入（Word Embedding）通常指分布式向量表示（如 Word2Vec、GloVe），但 One-hot 和 TF-IDF 不屬於詞嵌入
- **記憶提示**：詞嵌入 = 稠密向量（dense vector），One-hot = 稀疏向量（sparse vector）
