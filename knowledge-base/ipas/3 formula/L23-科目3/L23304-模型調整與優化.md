# L23304 - 模型調整與優化

## 1. 核心定義 (20%)

模型調整與優化是提升機器學習模型效能的關鍵階段,包含超參數調校、學習率策略、正則化技術、模型融合等方法。核心目標是在訓練效率、泛化能力、推理速度之間取得最佳平衡。主要技術包含:學習率衰減(Step Decay/Cosine Annealing)、Early Stopping、Dropout、Batch Normalization、模型融合(Bagging/Boosting/Stacking)、知識蒸餾、模型壓縮。

### CFDS分解
```
ModelOptimization = f(C, F, D, S)
C = HyperparameterTuning ∘ RegularizationApply ∘ LearningRateSchedule ∘ ModelEnsemble
F = {OptimizedHyperparams, LRScheduleConfig, EnsembleWeights}
D = TrainingData + ValidationMetrics + OptimizationHistory
S = Tuning | Optimizing | Converged | Deployed
```

---

## 2. 關鍵公式 (25%)

### 2.1 學習率調整策略

**Step Decay（階梯衰減）**:
```
η_t = η₀ × γ^(⌊t/s⌋)

其中:
- η₀: 初始學習率
- γ: 衰減因子（通常0.1-0.5）
- s: 衰減步長（每s個epoch衰減一次）
- t: 當前epoch

範例: η₀=0.1, γ=0.1, s=30
  epoch 0-29: η=0.1
  epoch 30-59: η=0.01
  epoch 60-89: η=0.001
```

**Exponential Decay（指數衰減）**:
```
η_t = η₀ × e^(-λt)

其中λ為衰減率

特點: 平滑連續衰減
```

**Cosine Annealing（餘弦退火）**:
```
η_t = η_min + (η_max - η_min) × (1 + cos(πt/T)) / 2

其中:
- η_max: 最大學習率
- η_min: 最小學習率
- T: 週期長度
- t: 當前步數

特點: 週期性重啟,跳出局部最優
```

**ReduceLROnPlateau（驗證損失停滯時降低）**:
```
if val_loss未改善 for patience epochs:
    η_new = η × factor

參數:
- patience: 容忍epoch數（如10）
- factor: 衰減因子（如0.1）

特點: 自適應,根據驗證表現調整
```

### 2.2 正則化技術

**Dropout（隨機丟棄）**:
```
訓練時:
h_dropout = h ⊙ Bernoulli(p) / p

其中:
- p: 保留機率（通常0.5）
- ⊙: 逐元素乘法
- 除以p: 期望值匹配

推理時:
h_inference = h（不丟棄）

等價於2^N個子網路集成
```

**L1/L2正則化**:
```
L1正則化（Lasso）:
L_total = L_data + λ Σ|wᵢ|

效果: 稀疏性（部分w=0）

L2正則化（Ridge）:
L_total = L_data + λ Σwᵢ²

效果: 權重縮小（w接近0）

Elastic Net（L1+L2）:
L_total = L_data + λ₁Σ|wᵢ| + λ₂Σwᵢ²
```

**Batch Normalization**:
```
訓練時:
μ_B = (1/m) Σᵢ xᵢ  # batch均值
σ²_B = (1/m) Σᵢ (xᵢ - μ_B)²  # batch方差
x̂ᵢ = (xᵢ - μ_B) / √(σ²_B + ε)
yᵢ = γx̂ᵢ + β  # 可學習參數

推理時:
μ, σ²使用訓練時移動平均

好處:
- 加速訓練（允許更大學習率）
- 緩解梯度消失
- 輕微正則化
```

**Early Stopping**:
```
if val_loss未改善 for patience epochs:
    停止訓練,恢復最佳模型

參數:
- patience: 容忍epoch數（10-50）
- min_delta: 最小改善量（如0.001）

好處:
- 防止過擬合
- 節省訓練時間
```

### 2.3 模型融合

**Bagging（Bootstrap Aggregating）**:
```
訓練M個模型（Bootstrap採樣）:
  D₁, D₂, ..., D_M ~ Bootstrap(D)
  h₁, h₂, ..., h_M訓練於各D_m

預測:
  分類: ŷ = mode(h₁(x), h₂(x), ..., h_M(x))  # 投票
  迴歸: ŷ = (1/M) Σ h_m(x)  # 平均

典型: 隨機森林
```

**Boosting（提升）**:
```
串行訓練M個模型:
  h₁訓練於D
  h₂訓練於D（側重h₁錯誤樣本）
  ...
  h_M訓練於D（側重h_{M-1}錯誤樣本）

預測:
  ŷ = Σ α_m h_m(x)  # 加權聚合

其中α_m根據h_m準確率決定

典型: AdaBoost、GBDT、XGBoost
```

**Stacking**:
```
第一層（基學習器）:
  h₁, h₂, ..., h_M訓練於D

第二層（元學習器）:
  輸入: [h₁(x), h₂(x), ..., h_M(x)]
  輸出: 最終預測ŷ

元學習器可選: 線性迴歸、邏輯迴歸、神經網路

好處:
- 組合多樣化模型（XGBoost + 神經網路 + SVM）
- 性能通常優於單模型
```

---

## 3. 對比矩陣 (15%)

### 學習率策略對比

| 策略 | 特點 | 適用場景 | 穩定性 | 收斂速度 |
|------|------|---------|--------|---------|
| Step Decay | 階梯下降 | 標準訓練 | 高 | 中 |
| Exponential | 平滑衰減 | 長期訓練 | 高 | 慢 |
| Cosine Annealing | 週期重啟 | 跳出局部最優 | 中 | 快 |
| ReduceLROnPlateau | 自適應 | 驗證停滯 | 高 | 中 |

### 正則化方法對比

| 方法 | 機制 | 防過擬合效果 | 計算成本 | 適用層 |
|------|------|-------------|---------|--------|
| Dropout | 隨機丟棄 | 高 | 訓練+50% | 全連接 |
| Batch Norm | 歸一化 | 中 | +20% | 卷積/全連接 |
| L1 | 稀疏懲罰 | 中 | 低 | 全部 |
| L2 | 平滑懲罰 | 中 | 低 | 全部 |
| Early Stopping | 提前停止 | 高 | 無 | 全部 |

### 模型融合策略對比

| 策略 | 訓練方式 | 多樣性來源 | 性能提升 | 計算成本 |
|------|---------|-----------|---------|---------|
| Bagging | 並行 | 資料採樣 | +2-5% | M倍 |
| Boosting | 串行 | 錯誤聚焦 | +5-10% | M倍（無法並行） |
| Stacking | 兩層 | 模型多樣化 | +3-8% | M+1倍 |

---

## 4. 實務應用 (20%)

### 4.1 學習率調整實踐

```python
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 策略1: Step Decay
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
# 每30 epoch學習率×0.1

# 策略2: Cosine Annealing
scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)
# 100 epoch週期,最小1e-6

# 策略3: ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.1)

# 訓練循環
for epoch in range(100):
    train_loss = train(model, train_loader, optimizer)
    val_loss = validate(model, val_loader)

    scheduler.step(val_loss)  # ReduceLROnPlateau需驗證損失
    # scheduler.step()  # StepLR、CosineAnnealingLR

    print(f"Epoch {epoch}, LR: {optimizer.param_groups[0]['lr']:.6f}")
```

### 4.2 正則化組合

```python
import torch.nn as nn

class RegularizedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(128, 256)
        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization
        self.dropout1 = nn.Dropout(0.5)  # Dropout
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x)
        return self.fc2(x)

# L2正則化（權重衰減）
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# Early Stopping
best_val_loss = float('inf')
patience_counter = 0
patience = 10

for epoch in range(100):
    train(model, train_loader, optimizer)
    val_loss = validate(model, val_loader)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')
        patience_counter = 0
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        model.load_state_dict(torch.load('best_model.pth'))
        break
```

### 4.3 模型融合實踐

**Bagging範例（隨機森林）**:
```python
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier

# 手動Bagging
base_model = DecisionTreeClassifier(max_depth=10)
bagging = BaggingClassifier(base_model, n_estimators=100, max_samples=0.8)
bagging.fit(X_train, y_train)

# 隨機森林（Bagging + 隨機特徵）
rf = RandomForestClassifier(n_estimators=100, max_features='sqrt')
rf.fit(X_train, y_train)
```

**Stacking範例**:
```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier

# 第一層：多樣化基學習器
estimators = [
    ('rf', RandomForestClassifier(n_estimators=100)),
    ('xgb', XGBClassifier(n_estimators=100)),
    ('svm', SVC(probability=True))
]

# 第二層：元學習器
meta_model = LogisticRegression()

# Stacking
stacking = StackingClassifier(estimators=estimators, final_estimator=meta_model, cv=5)
stacking.fit(X_train, y_train)

print(f"Stacking AUC: {roc_auc_score(y_test, stacking.predict_proba(X_test)[:, 1]):.4f}")
# 通常比單模型提升2-5%
```

---

## 5. 記憶口訣 (10%)

**「學習率降,正則化防,早停省時間」**
- 學習率衰減: Step/Cosine/Plateau
- 正則化: Dropout/L1/L2/Batch Norm
- Early Stopping: 防過擬合+節省時間

**「Bagging並行降方差,Boosting串行降偏差,Stacking組合最強大」**
- Bagging: 隨機森林,並行訓練
- Boosting: XGBoost,串行提升
- Stacking: 兩層模型,性能最優

**「Dropout訓練丟,推理全保留,期望值匹配很重要」**
- 訓練: 隨機丟棄神經元
- 推理: 保留全部,乘以p匹配期望

---

## 6. 自我驗證 (10%)

**Q1: Early Stopping的主要作用是?**
A. 加速訓練
B. 防止過擬合
C. 提升準確率
D. 減少參數

**答案: B（防止過擬合）**
解析: Early Stopping在驗證損失停止改善時停止訓練,防止模型過度擬合訓練集。

**Q2: Dropout在推理時應該?**
A. 保持丟棄
B. 關閉丟棄
C. 丟棄率減半
D. 隨機決定

**答案: B（關閉丟棄）**
解析: 推理時不丟棄神經元,但輸出需乘以p匹配訓練時期望值（PyTorch自動處理）。

**Q3: 以下哪個不是學習率衰減策略?**
A. Step Decay
B. Exponential Decay
C. Dropout Decay
D. Cosine Annealing

**答案: C（Dropout Decay）**
解析: Dropout是正則化方法,非學習率調整策略。

**簡答: 解釋Bagging和Boosting的核心差異,並說明何時使用Bagging、何時使用Boosting。**

**答案:**

**Bagging vs Boosting核心差異:**

**訓練方式:**
- Bagging: 並行訓練M個獨立模型（可並行化）
- Boosting: 串行訓練M個模型（後一個依賴前一個）

**資料處理:**
- Bagging: Bootstrap採樣（有放回）,每個模型看到不同資料子集
- Boosting: 調整樣本權重,聚焦前一輪錯誤樣本

**目標:**
- Bagging: 降低方差（減少過擬合）
- Boosting: 降低偏差（提升準確率）

**聚合方式:**
- Bagging: 簡單投票/平均（權重相等）
- Boosting: 加權聚合（準確率高的模型權重大）

**使用場景:**

**使用Bagging:**
1. 模型過擬合（高方差）:決策樹容易過擬合,隨機森林降低方差
2. 資料充足:需多次採樣
3. 計算資源充足:可並行訓練加速
4. 追求穩定性:減少模型方差,預測更穩定

**使用Boosting:**
1. 模型欠擬合（高偏差）:弱學習器逐步提升準確率
2. 追求高準確率:Boosting通常效能更優（Kaggle常勝軍）
3. 不介意訓練時間:串行訓練較慢
4. 表格資料:XGBoost/LightGBM在表格資料上表現最佳

**實務選擇:**
- **穩定性優先**: 隨機森林（Bagging）
- **準確率優先**: XGBoost（Boosting）
- **兩者結合**: Stacking（RF + XGBoost + 神經網路）

**易錯點提醒:**
1. Dropout推理時仍啟用（錯誤,應關閉）
2. Early Stopping未保存最佳模型（應保存驗證損失最低的模型）
3. Batch Normalization推理時用batch統計（應用訓練時移動平均）
4. 學習率衰減過快（如初始就很小,無需快速衰減）
5. 模型融合用相同模型（應多樣化:樹模型+線性+神經網路）
