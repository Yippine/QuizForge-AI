# L23402 - 演算法偏見與公平性

## 1. 核心定義 (20%)

演算法偏見與公平性是機器學習應用中不可迴避的倫理與技術挑戰，涵蓋演算法偏見來源、公平性定義、偏見檢測方法、公平性指標（Demographic Parity、Equal Opportunity、Equalized Odds）與去偏見技術（預處理、訓練中、後處理）等。核心問題是：如何在追求準確性的同時，確保模型對所有人口群體的決策公正無歧視？

**偏見來源的三層結構**：
1. **資料偏見(Data Bias)**：訓練數據反映歷史歧視、不完整或不代表性
2. **特徵偏見(Feature Bias)**：特徵與受保護屬性相關（代理變數問題）
3. **演算法偏見(Algorithm Bias)**：模型設計或訓練方式對某群體不公平

### CFDS 分解

```
AlgorithmicFairness = f(C, F, D, S)
C = BiasDetection ∘ FairnessMetrics ∘ DebiasAlgorithm ∘ FairnessAudit
  = {DataAudit, FeatureAnalysis, ModelCalibration, PerformanceMonitoring}
F = {FairnessConstraints, BiasWeights, ThresholdsConfig, AuditLogs, CompliancePolicy}
D = TrainingData + ProtectedAttributes + DecisionOutcomes + HistoricalData
S = Preprocessing | Training | Postprocessing | Monitoring
```

**核心概念**：
- **統計公平(Statistical Fairness)**：基於統計指標的公平性（DP、EO、EO+）
- **個體公平(Individual Fairness)**：相似個體應受相似對待
- **因果公平(Causal Fairness)**：排除受保護屬性的因果影響

**技術定位**：機器學習治理層級（倫理保障）
- 層級1：演算法本身（模型架構、損失函數）
- 層級2：訓練流程（資料選擇、樣本權重）
- 層級3：部署監控（公平性指標、模型解釋性、持續審計）

---

## 2. 關鍵公式 (25%)

### 2.1 Demographic Parity（人口均等）

**定義**：保護屬性A與決策無關，各人口群體獲得正決策的比例相同
```
P(Ŷ=1|A=0) = P(Ŷ=1|A=1)

其中：
- A ∈ {0, 1}：保護屬性（如：A=0為女性，A=1為男性）
- Ŷ ∈ {0, 1}：模型預測決策
```

直觀解釋：不同人口群體（A=0 vs A=1，如男性vs女性）獲得正決策的比例完全相同，體現「形式上的平等」

**Disparate Impact (DI) 指標**：
```
DI = P(Ŷ=1|A=protected) / P(Ŷ=1|A=unprotected)

計算步驟：
1. 計算保護群體的正決策率：p_protected = count(Ŷ=1 in protected) / count(protected)
2. 計算非保護群體的正決策率：p_unprotected = count(Ŷ=1 in unprotected) / count(unprotected)
3. 計算比率：DI = p_protected / p_unprotected

判定標準（美國80規則）：
- DI ≥ 0.8: 通常視為公平
- DI < 0.8: 違反公平，存在顯著歧視
- DI ≥ 1.0: 保護群體獲益

範例：
若男性獲批率80%，女性獲批率64%
DI = 64% / 80% = 0.8 ✓ 臨界但可接受（剛好達到80規則）
若女性獲批率48%
DI = 48% / 80% = 0.6 ✗ 嚴重歧視（遠低於0.8）
```

**優點**：
- 容易計算與解釋
- 無需真實標籤Y，適用於無標籤場景

**缺點**：
- 忽視真實標籤Y，可能導致「均等但不準確」的決策
- 若基礎比例不同（如男性的合格率本身就高），強制DP反而不公平

### 2.2 Equal Opportunity（機會平等）

**定義**：給定真實正例（Y=1）時，模型給予不同群體相同的正預測概率（相等的識別率）
```
P(Ŷ=1|Y=1, A=0) = P(Ŷ=1|Y=1, A=1)

即 TPR（真正率）在各群體中相等
TPR_0 = TPR_1
```

**詳細公式**：
```
TPR_a = P(Ŷ=1|Y=1, A=a) = TP_a / (TP_a + FN_a)

其中：
- TP_a: 在群體a中正確預測為正的樣本數
- FN_a: 在群體a中錯誤預測為負的樣本數（Type II Error）
- TPR = 敏感度 = 召回率 (Recall)

計算範例：
群體0（女性）：TP_0=70, FN_0=30 → TPR_0 = 70/(70+30) = 70%
群體1（男性）：TP_1=85, FN_1=15 → TPR_1 = 85/(85+15) = 85%
差距：Δ_TPR = |70% - 85%| = 15% ✗ 不平等

調整後（通過預處理或訓練中方法）：
女性和男性 TPR 都達到 75%，則 Equal Opportunity 滿足 ✓
```

**直觀解釋**：在真正合格的候選人中，各群體被正確識別的概率相同。特別重要的場景是「不能遺漏」，如醫療診斷、法律判決等。

**優點**：
- 確保最優人群（真正正例）不被遺漏
- 基於真實標籤Y，更符合決策公正

**缺點**：
- 可能導致不同群體的偽正率(FPR)不同
- 可能為了提高弱勢群體TPR而接受更多誤判

### 2.3 Equalized Odds（賠率均等）

**定義**：同時滿足TPR和FPR相等
```
P(Ŷ=1|Y=1, A=0) = P(Ŷ=1|Y=1, A=1)  [TPR相等]
AND
P(Ŷ=1|Y=0, A=0) = P(Ŷ=1|Y=0, A=1)  [FPR相等]

即：TPR_0 = TPR_1 且 FPR_0 = FPR_1
```

**公式展開**：
```
均等化賠率 = (TPR_0 = TPR_1) ∧ (FPR_0 = FPR_1)

偏差度量：
Δ_TPR = |TPR_0 - TPR_1|
Δ_FPR = |FPR_0 - FPR_1|

公平性評分 = 1 - (Δ_TPR + Δ_FPR)/2

若評分 ≥ 0.95，視為達成均等化賠率
```

**優點**：最嚴格的公平性保證
**缺點**：實務中難以同時滿足，通常需平衡權衡

### 2.4 公平性-準確性權衡（Pareto Frontier）

```
目標函數：
min L = λ × Loss_accuracy + (1-λ) × Loss_fairness

其中：
- Loss_accuracy: 模型準確性損失（MSE、交叉熵）
- Loss_fairness: 不公平程度（Δ_DP、Δ_EO、Δ_CO）
- λ ∈ [0,1]: 權重參數（控制準確與公平的平衡）

權衡策略：
- λ = 1: 只優化準確性（可能不公平）
- λ = 0.5: 等權平衡（推薦起點）
- λ = 0: 只優化公平性（準確性可能下降）

Pareto前沿：
無法同時改進準確性與公平性的解集合
實務：選擇Pareto前沿上符合業務目標的點
```

### 2.5 去偏見技術

**預處理（Reweighting）**：
```
訓練時調整樣本權重，使受保護群體代表性增加

新權重 = w_original × R_a
其中 R_a = 目標比例 / 現有比例

例：若目標性別比1:1，但數據中男性70%、女性30%
男性權重乘以 0.5/0.7 ≈ 0.714
女性權重乘以 0.5/0.3 ≈ 1.667

實現方式：
1. 過採樣(Oversampling)：複製少數群體樣本
2. 欠採樣(Undersampling)：刪除多數群體樣本
3. SMOTE：合成少數群體樣本
4. 成本敏感學習：為不同群體設置不同損失權重
```

**訓練中（Fairness Constraints）**：
```
目標函數加入公平性約束：

min L = Loss(Y, Ŷ)
s.t. |Δ_DP| ≤ ε_dp  或  |Δ_EO| ≤ ε_eo

拉格朗日鬆弛：
L_lagrange = Loss + λ × FairnessViolation

梯度更新包含公平性導數：
θ ← θ - η × (∇Loss + λ × ∇FairnessViolation)

典型約束：
- Demographic Parity 約束：|P(Ŷ=1|A=0) - P(Ŷ=1|A=1)| ≤ ε
- Equal Opportunity 約束：|TPR_0 - TPR_1| ≤ ε
- 可同步多個約束，通過權重λ_i分別控制
```

**後處理（Threshold Adjustment）**：
```
調整不同群體的分類閾值，使預測結果更公平

閾值調整基本原理：
Ŷ_new = 1 if P(Y=1|X,A=a) ≥ threshold_a else 0

其中 threshold_0 ≠ threshold_1 可調整以達成公平

實現步驟：
1. 在驗證集上評估當前的不公平程度
2. 計算目標公平性指標
3. 搜尋最佳閾值組合(threshold_0, threshold_1)
4. 在測試集上驗證

例：男性用0.50，女性用0.45
效果：降低女性被拒的可能，提升女性通過率
代價：男性中False Positive增加

限制：後處理無法消除根本的訓練數據偏見
```

**特徵工程去偏見**：
```
移除或去除敏感特徵的影響：

1. 移除敏感屬性：直接刪除受保護特徵（如性別欄）
   缺點：難以完全移除，代理特徵仍可推斷

2. 移除代理特徵(Proxy Features)：
   識別與受保護屬性高度相關的特徵
   例：郵遞區號與種族相關 → 考慮移除或轉換

3. 特徵正交化(Orthogonalization)：
   確保特徵與受保護屬性無相關
   X_debiased = X - Cov(X,A) / Var(A) × A

4. 公平性特徵轉換：
   使用專門的去偏見編碼技術(如FAE - Fair Auto-Encoder)
```

---

## 3. 對比矩陣 (15%)

### 公平性指標對比

| 公平性指標 | 核心定義 | 適用場景 | 優點 | 缺點 |
|----------|---------|---------|------|------|
| Demographic Parity | P(Ŷ=1\|A=0)=P(Ŷ=1\|A=1) | 高風險應用（招聘、貸款） | 易計算，不需標籤 | 忽視準確性，可能不公正 |
| Equal Opportunity | TPR_0 = TPR_1 | 涉及篩選（人才識別） | 保護優秀人才不遺漏 | FPR可能不等 |
| Equalized Odds | TPR_0=TPR_1 且 FPR_0=FPR_1 | 最嚴格場景（司法判決） | 最全面的公平保證 | 實務難以達成，準確性損失大 |
| Predictive Parity | P(Y=1\|Ŷ=1,A=0)=P(Y=1\|Ŷ=1,A=1) | 預測精準度要求高 | 保證預測精確度 | 與Equal Opportunity不兼容 |

### 去偏見方法對比

| 方法 | 執行時機 | 難度 | 計算成本 | 準確性損失 | 適用性 |
|------|---------|------|---------|-----------|--------|
| 預處理（Reweighting） | 訓練前 | 低 | 低 | 中等 | 適用所有演算法 |
| 訓練中（Fairness Loss） | 訓練中 | 中 | 中 | 低-中 | 需改進損失函數 |
| 後處理（Threshold Adjust） | 訓練後 | 低 | 低 | 中 | 不改動模型 |
| 特徵去除 | 前期 | 低 | 低 | 中-高 | 可能損失重要資訊 |

### 公平性與準確性的權衡矩陣

| 應用領域 | 推薦公平指標 | 推薦去偏見方法 | 理由 | 優先級 |
|---------|-------------|-------------|------|--------|
| 醫療診斷 | Equal Opportunity | 訓練中方法 | 不能漏掉真正患者（TPR重要） | 公平性>準確性 |
| 招聘系統 | Demographic Parity | 預處理+訓練中 | 整體機會平等（錄取率相同） | 均衡 |
| 信用評分 | Equalized Odds | 訓練中+後處理 | 最嚴格，批准與拒絕都公平 | 公平性>準確性 |
| 推薦系統 | 個體公平 | 後處理方法 | 相似用戶相似推薦 | 準確性≈公平性 |
| 司法判決 | Equalized Odds | 訓練中方法+人工審核 | 最嚴格，影響人身自由 | 公平性>>準確性 |
| 廣告投放 | Demographic Parity | 預處理方法 | 各群體曝光機會相同 | 準確性≈公平性 |

### 關鍵適用場景選擇

```
優先準確性（公平性要求中等）：
  高準確性要求 → 訓練中方法 + Equalized Odds + Pareto優化

優先公平性（準確性要求中等）：
  易解釋性要求 → 後處理方法 + Demographic Parity + 人工審核

快速部署（中等要求）：
  快速部署需求 → 預處理方法 + Equal Opportunity + 定期審計
```

---

## 4. 實務應用 (20%)

### 4.1 應用場景一：招聘系統中的性別偏見

**場景描述**：AI篩選簡歷系統對女性申請者產生歧視（拒絕率高於男性）

**技術應用**：
```
1. 偏見檢測：
   P(Ŷ=HIRE|Gender=F) = 15%
   P(Ŷ=HIRE|Gender=M) = 25%
   DI = 15%/25% = 0.6 << 0.8 [違反公平]

2. 公平性指標選擇：Equal Opportunity
   因需確保優秀女性候選人不被遺漏
   TPR_F 應接近 TPR_M

3. 去偏見策略：訓練中方法
   在損失函數加入性別公平約束
   L_total = L_accuracy - λ × L_fairness

4. 評估結果：
   調校後 P(HIRE|F)=22%, P(HIRE|M)=24%
   DI = 0.92 ✓ 達成公平
```

### 4.2 應用場景二：信用評分中的種族偏見

**場景描述**：銀行信用評分模型對某種族申請者的通過率更低

**技術應用**：
```
1. 定量化：
   TPR(種族A) = 70% vs TPR(種族B) = 85%
   FPR(種族A) = 15% vs FPR(種族B) = 8%

2. 根因分析：
   - 訓練數據中種族B的樣本更多（50% vs 20%）
   - 歷史貸款決策本身就有歧視（偏見數據）

3. 去偏見方案：預處理+後處理混合
   a) 預處理：過採樣種族A樣本，權重調整
   b) 訓練：使用平衡的損失函數
   c) 後處理：為種族A降低決策閾值（0.50→0.45）

4. 效果驗證：
   最終 TPR_A=83%, TPR_B=84%, FPR_A=9%, FPR_B=8%
   接近Equalized Odds標準
```

### 4.3 應用場景三：刑事司法預測系統

**場景描述**：再犯預測模型對某族群重刑機率預測偏高（自證現象）

**技術應用**：
```
1. 挑戰：公平性指標本身矛盾
   - Demographic Parity: 各族群再犯率預測相同（不現實）
   - Equal Opportunity: TPR相同（可能導致漏判）

2. 決策：選擇Equalized Odds + Equal Opportunity平衡
   允許基礎再犯率不同，但給予所有人相同的優化機會

3. 實現步驟：
   a) 分群模型：為不同族群訓練參數不同的子模型
   b) 公平約束：
      min L = Σ Loss(Y,Ŷ)
      s.t. |TPR_r1 - TPR_r2| ≤ 0.05, r ∈ [Race1, Race2]
   c) 人工審核：高風險決策由人類法官終審

4. 長期監控：
   部署後每月審計，監測Δ_EO、Δ_DP變化
```

### 4.4 實作步驟

```
第1步：數據探索
- 識別受保護屬性（性別、種族、年齡等）
- 計算基線公平性指標（DP、EO、EO）
- 視覺化群體間差異

第2步：選擇公平性標準
- 諮詢領域專家、倫理委員會
- 根據應用風險級別選擇指標
  高風險：Equalized Odds
  中風險：Equal Opportunity
  低風險：Demographic Parity

第3步：選擇去偏見技術
- 預處理（快速）：樣本重權、特徵工程
- 訓練中（精細）：損失函數改進、約束優化
- 後處理（簡單）：閾值調整

第4步：訓練與驗證
- 在驗證集上評估準確性與公平性
- 繪製Pareto前沿，選擇合適的權重λ
- 進行敏感性分析（λ變化時的影響）

第5步：部署與監控
- A/B測試：新模型 vs 舊模型
- 實時監控公平性指標
- 建立反饋機制：申訴與模型更新
```

### 4.5 常見陷阱

1. **公平性指標不兼容**：
   - Equal Opportunity和Predictive Parity通常不能同時滿足
   - 需明確選擇，而非希望「全部滿足」

2. **過度去偏見導致準確性崩潰**：
   - 如果λ=0（只優化公平），準確率從85%降至60%
   - 應找Pareto前沿上的平衡點

3. **數據偏見難以完全消除**：
   - 若訓練數據本身反映歷史歧視，修正困難
   - 需要高品質的人工標籤或重新採集數據

4. **受保護屬性隱含在特徵中**：
   - 移除「性別」欄位，但郵遞區號、教育等可推斷性別
   - 需移除代理特徵(proxy features)

5. **忽視群體內差異**：
   - 僅基於性別分群，忽視性別×年齡交叉性
   - 應進行交叉分析(intersectionality)

---

## 5. 記憶口訣 (10%)

### 核心口訣

**「人口均等DP，機會平等EO，賠率均等全面保」**
- Demographic Parity (DP)：整體獲批率相同 → P(Ŷ=1|A)相等
- Equal Opportunity (EO)：真正案例通過率相同 → TPR相等
- Equalized Odds (EO+)：通過率與拒絕率都相同 → TPR和FPR都相等

**「預處理改數據，訓練中改損失，後處理改閾值」**
- 預處理（Pre）：調整訓練資料集，樣本重權、特徵處理
- 訓練中（In）：加入公平性約束到損失函數，聯合優化
- 後處理（Post）：訓練後調整分類決策閾值

### 記憶技巧

**三層遞進（強度遞增）**：
```
層級1（弱）：Demographic Parity
  只看決策的邊際分佈，不看真實標籤
  易於計算，但可能不公正

層級2（中）：Equal Opportunity
  加入真實標籤，確保識別能力平等
  實用性與公平性平衡

層級3（強）：Equalized Odds
  全面控制真正率與偽正率
  最嚴格但實務難以達成
```

**實務選擇心法**：
```
風險分級 → 公平性指標選擇 → 去偏見方法

高風險（人身自由、重大決策）：
  司法判決 → Equalized Odds + 訓練中 + 人工審核

中風險（經濟利益、機會）：
  招聘系統 → Demographic Parity + 預處理+訓練中
  信用評分 → Equalized Odds + 訓練中+後處理

低風險（推薦、排序）：
  推薦系統 → Individual Fairness + 後處理
  廣告投放 → Demographic Parity + 預處理
```

### 易混淆辨析

**Q: DP和EO有何根本區別？**
- **DP** (Demographic Parity)：
  定義：P(Ŷ=1|A=0) = P(Ŷ=1|A=1)
  意義：各人口群體的獲批率相同
  何時用：無標籤或追求「形式平等」

- **EO** (Equal Opportunity)：
  定義：P(Ŷ=1|Y=1,A=0) = P(Ŷ=1|Y=1,A=1)
  意義：各人口群體識別合格人的能力相同
  何時用：有標籤且不能遺漏優秀者

**結論**：DP可能導致「均等但不準確」，EO確保「公正且有能力」

**Q: Equal Opportunity和Equalized Odds差別？**
- **EO** (Equal Opportunity)：
  只控制 TPR_0 = TPR_1（真正率相等）
  允許 FPR_0 ≠ FPR_1（偽正率可以不等）

- **EO+** (Equalized Odds)：
  同時控制 TPR_0 = TPR_1 AND FPR_0 = FPR_1
  最嚴格的統計公平性

**結論**：EO+是EO的加強版，要求完全對稱的公平性

---

## 6. 自我驗證 (10%)

### 選擇題

**Q1: Demographic Parity的核心定義是什麼？**
A. P(Ŷ=1|Y=1,A=0) = P(Ŷ=1|Y=1,A=1)
B. P(Ŷ=1|A=0) = P(Ŷ=1|A=1)
C. TPR_0 = TPR_1
D. Accuracy_0 = Accuracy_1

**答案：B** - DP不考慮真實標籤Y，只要求預測結果的邊際分佈相同。（第2.1章）

**Q2: 在Equal Opportunity下，下列何者應該相等？**
A. P(Ŷ=1|A=0)和P(Ŷ=1|A=1)
B. Accuracy_0和Accuracy_1
C. P(Ŷ=1|Y=1,A=0)和P(Ŷ=1|Y=1,A=1)
D. Precision_0和Precision_1

**答案：C** - Equal Opportunity要求真正率(TPR)在各群體相同，即P(Ŷ=1|Y=1,A)相等。（第2.2章）

**Q3: Disparate Impact (DI)指標在什麼情況下視為公平？**
A. DI ≥ 1.0
B. DI ≥ 0.9
C. DI ≥ 0.8
D. DI ≥ 0.5

**答案：C** - 美國80% Rule認為DI ≥ 0.8為公平。（第2.1章）

**Q4: 哪種去偏見方法無需修改模型，只調整決策閾值？**
A. 特徵選擇
B. 樣本重權
C. 後處理(Threshold Adjustment)
D. 損失函數約束

**答案：C** - 後處理在訓練完成後執行，通過調整閾值達成公平。（第2.5章）

### 簡答題

**Q6: 解釋為什麼Demographic Parity和Equal Opportunity通常不能同時滿足？舉例說明。**

**答案：**

假設有招聘數據：
```
男性：合格者100人（應聘200人），合格率50%
女性：合格者50人（應聘200人），合格率25%

目標人數：10人

情況1 - 滿足DP（各群體錄取率相同）：
男性錄取5人（5%）→ DP_M = 5%
女性錄取5人（2.5%）→ DP_F = 2.5% ✗ 不相等

若強制DP_M = DP_F（都5%）：
男性錄取5人（5%）
女性錄取5人（2.5%）不可能都5%

情況2 - 滿足EO（各群體真正率相同）：
男性中：真正率 = 錄取合格者/全部合格者
女性中：真正率 = 錄取合格者/全部合格者
若要相等，需錄取女性合格者的比例 = 男性的比例

但若男性合格率高，強制EO會導致DP_M > DP_F（不公平）
```

**根本原因**：
基礎合格率不同（男50%，女25%）時，公平性指標無法同時滿足。需要選擇一個優先級。

**Q7: 在去偏見過程中，為什麼需要同時監控訓練集和測試集上的公平性指標？**

**答案：**

**訓練集上的公平性指標**：
- 用於驗證去偏見方法是否有效
- 調整λ權重時的參考
- 監控訓練過程中的公平性進展

**驗證集上的公平性指標**：
- 超參數調校的主要參考
- 防止過度擬合公平性約束
- 找到準確性與公平性的最佳平衡點

**測試集上的公平性指標**：
- 真實衡量模型部署後的公平性表現
- 模擬真實環境的未見數據
- 最終的部署決策依據

**三集合的監控策略**：
```
訓練集: |DP|_train < 0.05, |EO|_train < 0.03
驗證集: |DP|_val < 0.08, |EO|_val < 0.05  ← 超參數調校
測試集: |DP|_test < 0.10, |EO|_test < 0.07 ← 部署決策
```

**常見問題與診斷**：
- 若訓練集公平但驗證集/測試集不公平：表示訓練數據不代表性
- 若訓練集不公平但測試集公平：可能是偶然，不可信任，需多次驗證
- 若三個集合都不公平：根本方法失效，需重新考慮策略

### 易錯點提醒

1. **混淆公平性指標含義**：
   - 易錯：認為DP最強（其實最弱，不考慮真實標籤）
   - 正確：EO > DP在強度上，EO+ (Equalized Odds) 最強

2. **忽視Pareto權衡**：
   - 易錯：期待準確與公平同時達到最大
   - 正確：在Pareto前沿上選擇，接受某些準確性損失

3. **代理特徵問題**：
   - 易錯：移除「性別」欄位就達到公平
   - 正確：需同時移除可推斷性別的特徵（如職業、郵遞區號等）

4. **交叉性被忽視**：
   - 易錯：只檢查「性別」公平，忽視「性別×種族」的交互效應
   - 正確：應進行分層分析，檢查多維度交叉公平性

5. **監控間隔過長**：
   - 易錯：模型上線後3個月才審計一次
   - 正確：至少週檢測，高風險應用日檢測公平性指標

6. **偏見來源認知不足**：
   - 易錯：認為偏見僅來自演算法本身
   - 正確：偏見來自三層：資料偏見（歷史數據反映過去歧視）、特徵偏見（特徵與敏感屬性相關）、演算法偏見（模型設計不公平）

7. **法律合規與倫理混淆**：
   - 易錯：認為數學上滿足某個指標就達成「合法」
   - 正確：法律合規（GDPR、個資法）、倫理原則（透明度、可解釋性、問責制）與統計公平性是不同維度，都需重視
