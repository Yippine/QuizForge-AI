# L23102 - 線性代數之機器學習基礎應用

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

線性代數是機器學習的數學骨幹，提供了描述與操作高維度資料的數學工具。機器學習中的資料本質上是向量與矩陣——每一筆資料是一個向量、整個資料集是一個矩陣、模型參數是向量或矩陣、神經網路的每一層運算都是矩陣乘法。線性代數賦予機器學習處理高維空間、進行線性轉換、提取資料結構的核心能力。

從基礎的向量運算到進階的矩陣分解（特徵分解、奇異值分解），線性代數貫穿機器學習的每個環節：資料表示（向量空間）、模型表達（權重矩陣）、演算法設計（矩陣運算）、降維技術（PCA、SVD）、優化過程（梯度向量、Hessian 矩陣）。理解線性代數不僅是掌握機器學習的前提，更是深入理解模型行為與優化策略的關鍵。

### 1.2 核心概念

**線性代數在機器學習中的五大核心概念：**

1. **向量與向量空間**：資料點的數學表示，支撐特徵向量、梯度向量、權重向量的運算
2. **矩陣運算**：資料集的批次處理、線性轉換、神經網路層間傳播的核心機制
3. **特徵分解（Eigendecomposition）**：將矩陣分解為特徵向量與特徵值，揭示資料的主要方向與變異程度
4. **奇異值分解（SVD）**：將任意矩陣分解為三個矩陣的乘積，是降維、推薦系統、影像壓縮的數學基礎
5. **線性轉換與投影**：PCA 主成分分析、資料降維、特徵提取的幾何本質

### 1.3 CFDS 分解

基於 Formula-Contract 方法論，將線性代數在機器學習中的應用分解為四個基本單元:

```
LinearAlgebra_ML = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = VectorOperations ∘ MatrixComputation ∘ Decomposition ∘ Transformation
  = 向量運算 -> 矩陣計算 -> 矩陣分解 -> 線性轉換
```

**F (Files - 配置資源)**
```
F = {WeightMatrices, CovarianceMatrices, TransformationMatrices, DecompositionResults}
  = {權重矩陣, 共變異數矩陣, 轉換矩陣, 分解結果}
```

**D (Data - 資料結構)**
```
D = Vectors + Matrices + Tensors + EigenValues + SingularValues
  = 向量 + 矩陣 + 張量 + 特徵值 + 奇異值
```

**S (State - 運行狀態)**
```
S = MatrixDecomposition | DimensionReduction | LinearTransformation
  = 矩陣分解狀態 | 降維狀態 | 線性轉換狀態
```

### 1.4 技術定位

線性代數在機器學習技術棧中處於基礎數學層，與機率統計、數值優化並列為三大數學支柱。線性代數專注於資料的結構化表示與高效運算，提供資料的向量化表達、模型的矩陣化實現、演算法的批次化處理。

在企業應用中，線性代數驅動深度學習的矩陣運算（GPU 加速的核心）、支撐推薦系統的協同過濾（矩陣分解）、實現影像處理的降維壓縮（SVD）、優化大規模資料的特徵提取（PCA）。掌握線性代數是理解現代機器學習系統的必要條件。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 向量運算主公式

**向量內積（Dot Product）**：
```
x · y = ∑(x_i × y_i) = ||x|| × ||y|| × cos(θ)
```
- **幾何意義**：衡量兩向量的相似程度與方向相關性
- **應用**：相似度計算、注意力機制、餘弦相似度

**向量範數（Norm）**：
```
L1 範數: ||x||_1 = ∑|x_i|
L2 範數: ||x||_2 = √(∑x_i²)
```
- **應用**：正則化（L1/L2 Regularization）、距離度量

### 2.2 矩陣運算主公式

**矩陣乘法**：
```
C = A × B
C_ij = ∑(A_ik × B_kj)
```
- **維度要求**：A 為 m×n，B 為 n×p，則 C 為 m×p
- **應用**：神經網路層間傳播、批次資料處理

**矩陣轉置與逆矩陣**：
```
轉置: (A^T)_ij = A_ji
逆矩陣: A × A^(-1) = I (單位矩陣)
```
- **應用**：線性迴歸求解、共變異數矩陣計算

### 2.3 特徵分解（Eigendecomposition）

**特徵值與特徵向量定義**：
```
A × v = λ × v
```
- **A**：方陣（n×n）
- **v**：特徵向量（非零向量）
- **λ**：特徵值（純量）

**矩陣特徵分解**：
```
A = Q × Λ × Q^(-1)
```
- **Q**：特徵向量組成的矩陣
- **Λ**：對角矩陣，對角線元素為特徵值

**應用**：PCA 主成分分析、圖譜分析、馬可夫鏈穩態

### 2.4 奇異值分解（SVD）

**SVD 分解主公式**：
```
A = U × Σ × V^T
```
- **A**：任意 m×n 矩陣
- **U**：m×m 左奇異向量矩陣（正交矩陣）
- **Σ**：m×n 對角矩陣，對角線為奇異值（σ_1 ≥ σ_2 ≥ ... ≥ 0）
- **V^T**：n×n 右奇異向量矩陣轉置（正交矩陣）

**截斷 SVD（降維）**：
```
A_k = U_k × Σ_k × V_k^T
```
- **k**：保留前 k 個最大奇異值，實現降維（k << min(m, n)）

**應用**：
- 推薦系統：協同過濾的矩陣分解
- 影像壓縮：保留主要奇異值
- 自然語言處理：潛在語意分析（LSA）
- 降維：類似 PCA 但適用於任意矩陣

### 2.5 主成分分析（PCA）

**PCA 核心公式**：
```
PCA_Process = CovarianceMatrix ∘ EigenDecomposition ∘ Projection
```

**步驟拆解**：

**Step 1: 資料標準化**
```
X_standardized = (X - mean(X)) / std(X)
```

**Step 2: 計算共變異數矩陣**
```
Cov(X) = (1/(n-1)) × X^T × X
```

**Step 3: 特徵分解**
```
Cov(X) = Q × Λ × Q^T
```
- **Q**：特徵向量（主成分方向）
- **Λ**：特徵值（各主成分的變異量）

**Step 4: 選擇主成分**
```
累積變異解釋比例 = ∑(λ_i) / ∑(λ_total)
```
- 通常保留累積解釋 95% 變異的主成分

**Step 5: 投影降維**
```
X_reduced = X × Q_k
```
- **Q_k**：保留前 k 個主成分

**幾何意義**：PCA 找出資料變異最大的方向（特徵向量），將資料投影到這些方向，實現降維同時保留最多資訊。

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 技術對比表

| 技術 | 適用矩陣 | 分解形式 | 計算複雜度 | 主要應用 | 優點 | 限制 |
|------|---------|---------|-----------|---------|------|------|
| **特徵分解** | 方陣 | A = QΛQ^(-1) | O(n³) | PCA、馬可夫鏈 | 揭示矩陣內在結構 | 僅適用方陣 |
| **SVD** | 任意矩陣 | A = UΣV^T | O(min(mn², m²n)) | 推薦系統、降維 | 適用所有矩陣 | 計算成本高 |
| **PCA** | 資料矩陣 | Cov(X) 特徵分解 | O(p³ + np²) | 特徵降維 | 保留最多變異 | 線性假設 |

### 3.2 優缺點分析

**特徵分解 (Eigendecomposition)**
- **優點**：數學優雅、揭示矩陣本質結構、支撐 PCA 與譜分析
- **缺點**：僅適用方陣、數值穩定性較差
- **適用場景**：PCA、圖譜聚類、馬可夫穩態分析

**奇異值分解 (SVD)**
- **優點**：適用任意矩陣、數值穩定、理論保證強
- **缺點**：計算成本高（大型矩陣）、不可解釋性（商業場景）
- **適用場景**：推薦系統（Netflix Prize）、影像壓縮、潛在語意分析

**主成分分析 (PCA)**
- **優點**：降維高效、可視化友善、去除共線性
- **缺點**：線性假設限制、對尺度敏感（需標準化）、不保證可解釋性
- **適用場景**：特徵降維、資料可視化、去噪

### 3.3 適用場景

**何時使用特徵分解？**
- 分析方陣的內在性質（如共變異數矩陣）
- 需要理解資料的主要方向與變異量
- 圖結構分析（拉普拉斯矩陣）

**何時使用 SVD？**
- 矩陣不是方陣（如用戶-物品評分矩陣）
- 需要穩定的數值分解
- 推薦系統的協同過濾
- 文字分析的潛在語意提取

**何時使用 PCA？**
- 資料維度過高需要降維（如影像 784 維 → 50 維）
- 特徵間存在高度共線性
- 需要可視化高維資料（降至 2D/3D）

### 3.4 性能比較

**計算效率**：
- 小規模資料（< 10,000 維）：三者差異不大
- 大規模資料：PCA > SVD截斷版 > 完整SVD > 特徵分解

**記憶體需求**：
- PCA：需存儲共變異數矩陣（p×p）
- SVD：需存儲完整分解結果（m×m + m×n + n×n）
- 截斷 SVD：僅存儲前 k 個分量（顯著降低）

**資訊保留**：
- PCA/SVD：可量化保留比例（累積變異解釋）
- 特徵分解：依賴於矩陣性質

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一：影像辨識的降維加速

**場景描述**：手寫數字辨識（MNIST 資料集），原始影像 28×28 = 784 維，直接訓練模型計算成本高且容易過擬合。

**技術應用**：使用 PCA 將 784 維降至 50 維，保留 95% 變異資訊。

**實現要點**：
```
ImageRecognition_Optimized = DataLoading -> PCA_Reduction(n_components=50) ->
                               ModelTraining -> Evaluation

PCA_Reduction = Standardization ∘ CovarianceMatrix ∘ EigenDecomposition ∘
                Projection(top_50_components)
```

**效果**：
- 訓練速度提升 15 倍
- 模型參數量減少 93.6%（784 → 50）
- 準確率僅降低 1-2%（98% → 96%）

### 4.2 應用場景二：推薦系統的協同過濾

**場景描述**：Netflix 電影推薦，用戶-電影評分矩陣（100萬用戶 × 2萬電影），矩陣極度稀疏（99% 缺失值）。

**技術應用**：使用 SVD 矩陣分解，將評分矩陣分解為用戶潛在特徵與電影潛在特徵的乘積。

**實現要點**：
```
RecommendationSystem = RatingMatrix -> SVD_Decomposition(k=50) ->
                        LatentFactors -> PredictRatings

SVD_Decomposition:
  R ≈ U_k × Σ_k × V_k^T
  R: 用戶-電影評分矩陣 (m×n)
  U_k: 用戶潛在特徵矩陣 (m×k)
  V_k: 電影潛在特徵矩陣 (n×k)
  k=50: 潛在因子數量（用戶與電影的隱藏特性維度）
```

**預測公式**：
```
predicted_rating(user_i, movie_j) = U[i,:] · V[j,:]
```

**效果**：
- Netflix Prize 競賽關鍵技術
- RMSE 降低 10%+
- 可解釋性：潛在因子代表電影類型偏好

### 4.3 應用場景三：深度學習的梯度計算

**場景描述**：神經網路訓練，需要計算損失函數對權重矩陣的梯度，進行反向傳播。

**技術應用**：利用矩陣微分與鏈式法則，高效計算梯度矩陣。

**實現要點**：
```
NeuralNetworkTraining = ForwardPass -> LossComputation ->
                         BackwardPass(MatrixGradients) -> WeightUpdate

ForwardPass:
  Z = W × X + b  (線性轉換)
  A = σ(Z)       (非線性啟動)

BackwardPass:
  ∂L/∂W = ∂L/∂A × ∂A/∂Z × ∂Z/∂W = δ × X^T
  W_new = W - η × ∂L/∂W  (梯度下降更新)
```

**矩陣化優勢**：
- 批次處理：一次計算 batch_size 筆資料
- GPU 加速：矩陣運算高度並行化
- 記憶體效率：避免逐筆迴圈

### 4.4 實作步驟

**通用線性代數應用流程**：

1. **資料向量化**：將原始資料轉換為向量/矩陣表示
2. **標準化處理**：統一尺度（特別是 PCA 前）
3. **矩陣分解**：根據需求選擇特徵分解/SVD/PCA
4. **維度選擇**：根據累積變異解釋比例或業務需求決定保留維度
5. **轉換應用**：將分解結果應用於降維、預測或優化
6. **效果驗證**：評估資訊保留率與下游任務效能

### 4.5 常見陷阱

**陷阱 1：未標準化直接 PCA**
- **問題**：特徵尺度差異大（如年齡 0-100，收入 0-1000000），大尺度特徵主導主成分
- **解決**：先標準化（Z-score）再進行 PCA

**陷阱 2：過度降維導致資訊損失**
- **問題**：為加速而過度降維（如 1000 維 → 5 維），丟失關鍵資訊
- **解決**：觀察累積變異解釋比例，通常保留 90-95%

**陷阱 3：非方陣使用特徵分解**
- **問題**：對 m×n（m≠n）矩陣直接特徵分解會失敗
- **解決**：使用 SVD 或先計算共變異數矩陣再特徵分解

**陷阱 4：忽略數值穩定性**
- **問題**：矩陣近奇異（行列式接近 0）導致計算不穩
- **解決**：加入正則化項（如 A + λI），使用穩定的 SVD 替代特徵分解

**陷阱 5：訓練/測試集分別降維**
- **問題**：訓練集和測試集分別計算 PCA，導致主成分不一致
- **解決**：在訓練集上擬合 PCA，將轉換應用於測試集

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**「向量矩陣三分解，降維轉換齊驅動」**
- **向量**：內積範數基礎功
- **矩陣**：乘法轉置逆運算
- **三分解**：特徵分解、SVD、PCA
- **降維轉換**：從高維到低維的映射

**「特徵 SVD 與 PCA，方陣任意與資料」**
- **特徵分解**：方陣專用
- **SVD**：任意矩陣
- **PCA**：資料矩陣降維

### 5.2 記憶技巧

**SVD 三矩陣記憶**：「左奇-值-右奇轉」
- **U**：左奇異向量（Left Singular Vectors）
- **Σ**：奇異值對角矩陣（Singular Values）
- **V^T**：右奇異向量轉置（Right Singular Vectors Transposed）

**PCA 五步驟記憶**：「標協特選投」
1. **標**：標準化資料
2. **協**：計算共變異數矩陣
3. **特**：特徵分解
4. **選**:選擇主成分（根據變異解釋）
5. **投**：投影降維

**特徵方程記憶**：「矩陣乘向量，等於值乘向量」
```
A × v = λ × v
矩陣 乘 特徵向量 = 特徵值 乘 特徵向量
```

**維度法則記憶**：「前列乘後行，結果前行後列」
```
(m×n) × (n×p) = (m×p)
A的列數 必須等於 B的行數
```

### 5.3 快速回憶

**線性代數核心公式速查**：
- **向量相似**：內積 x·y = ||x||||y||cosθ
- **矩陣乘法**：C_ij = Σ(A_ik × B_kj)
- **特徵分解**：A = QΛQ^(-1)（方陣）
- **SVD**：A = UΣV^T（任意矩陣）
- **PCA**：降維公式 X_reduced = X × Q_k

**應用場景速配**：
- **推薦系統** → SVD 矩陣分解
- **影像降維** → PCA
- **神經網路** → 矩陣乘法 + 梯度計算
- **相似度** → 向量內積/餘弦相似度

### 5.4 易混淆辨析

**特徵分解 vs SVD**：
- **特徵分解**：僅方陣、分解為 QΛQ^(-1)、特徵向量不一定正交
- **SVD**：任意矩陣、分解為 UΣV^T、U 和 V 必正交

**PCA vs 特徵分解**：
- **PCA**：降維技術（流程），先算共變異數矩陣再特徵分解
- **特徵分解**：矩陣分解方法（技術），PCA 的核心步驟

**內積 vs 外積**：
- **內積**（Dot Product）：x·y = 純量（單一數值）
- **外積**（Outer Product）：x⊗y = 矩陣（m×n）

**範數 L1 vs L2**：
- **L1**：絕對值和 Σ|x_i|，產生稀疏性（Lasso 正則化）
- **L2**：平方和開根 √Σx_i²，平滑懲罰（Ridge 正則化）

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**Q1：以下哪個矩陣分解技術適用於非方陣？**
A. 特徵分解（Eigendecomposition）
B. 對角化（Diagonalization）
C. 奇異值分解（SVD）
D. 以上皆是

**Q2：PCA 降維過程中，如何決定保留多少主成分？**
A. 固定保留前 10 個
B. 根據累積變異解釋比例（如 95%）
C. 隨機選擇
D. 保留所有主成分

**Q3：神經網路訓練中，前向傳播 Z = W × X + b 的矩陣維度要求為何？**
A. W 為 m×n，X 為 n×p，則 Z 為 m×p
B. W 和 X 維度任意
C. X 必須是方陣
D. W 和 X 維度必須相同

**Q4：以下哪個應用場景最適合使用 SVD？**
A. 圖片標準化
B. 推薦系統的協同過濾
C. 類別資料編碼
D. 缺失值填補（非矩陣分解方法）

**Q5：特徵向量 v 滿足方程式 A×v = λ×v，其中 λ 代表什麼？**
A. 特徵向量
B. 特徵值
C. 矩陣 A 的行列式
D. 單位矩陣

### 6.2 簡答題

**Q1：解釋為什麼 PCA 降維前需要先標準化資料？**

**Q2：Netflix 推薦系統使用 SVD 分解用戶-電影評分矩陣 R ≈ UΣV^T，請說明 U、Σ、V 各代表什麼意義，以及如何用它們預測評分？**

### 6.3 答案解析

**選擇題答案**：

**A1：C（奇異值分解 SVD）**
- **解析**：特徵分解僅適用方陣（n×n），SVD 可分解任意 m×n 矩陣。推薦系統的用戶-物品矩陣通常非方陣（如 100萬×2萬），必須使用 SVD。

**A2：B（根據累積變異解釋比例）**
- **解析**：PCA 通常根據累積變異解釋比例決定保留多少主成分，常見閾值為 90-95%。例如前 50 個主成分解釋了 95% 變異，即可停止。

**A3：A（W 為 m×n，X 為 n×p，則 Z 為 m×p）**
- **解析**：矩陣乘法要求「前矩陣列數 = 後矩陣行數」，即 W 的 n 列必須等於 X 的 n 行。結果維度為「前矩陣行數 × 後矩陣列數」= m×p。

**A4：B（推薦系統的協同過濾）**
- **解析**：SVD 矩陣分解是協同過濾的經典方法（Netflix Prize），將稀疏評分矩陣分解為低維潛在因子，實現個性化推薦。

**A5：B（特徵值）**
- **解析**：方程式 A×v = λ×v 中，λ 是特徵值（eigenvalue），v 是特徵向量（eigenvector）。特徵值代表矩陣 A 在特徵向量方向上的縮放因子。

**簡答題答案**：

**A1：PCA 標準化的必要性**

PCA 降維前必須標準化資料，原因有三：

1. **消除尺度影響**：不同特徵的尺度差異會導致大尺度特徵主導主成分。例如「收入（0-100萬）」和「年齡（0-100）」，收入的變異量遠大於年齡，PCA 會錯誤地認為收入是最重要的維度。

2. **確保公平比較**：標準化將所有特徵轉換為均值 0、標準差 1 的分佈，使每個特徵在相同基準上比較變異量。

3. **提升數值穩定性**：極端尺度差異會導致共變異數矩陣數值不穩定，影響特徵分解的準確性。

**標準化公式**：
```
X_standardized = (X - mean(X)) / std(X)
```

**A2：SVD 在推薦系統中的應用**

**矩陣意義**：
- **R**（m×n）：用戶-電影評分矩陣，R_ij 代表用戶 i 對電影 j 的評分
- **U**（m×k）：用戶潛在特徵矩陣，每行代表一個用戶的 k 維隱藏偏好
- **Σ**（k×k）：對角矩陣，對角線為奇異值（重要性權重）
- **V^T**（k×n）：電影潛在特徵矩陣轉置，每列代表一個電影的 k 維隱藏屬性

**預測評分**：
```
predicted_rating(user_i, movie_j) = U[i,:] × Σ × V[j,:]^T
簡化為：predicted_rating ≈ U[i,:] · V[j,:]
```

**實務做法**：
1. 對評分矩陣進行截斷 SVD（保留前 k=50 個奇異值）
2. 提取用戶 i 的潛在特徵向量 U[i,:]
3. 提取電影 j 的潛在特徵向量 V[j,:]
4. 計算兩向量內積得到預測評分
5. 推薦評分最高且用戶未觀看的電影

### 6.4 易錯點提醒

**易錯點 1：矩陣乘法維度錯誤**
- **錯誤**：(m×n) × (p×q) 其中 n ≠ p
- **提醒**：前矩陣列數必須等於後矩陣行數

**易錯點 2：混淆特徵分解與 SVD 適用範圍**
- **錯誤**：對非方陣使用特徵分解
- **提醒**：特徵分解僅方陣，SVD 適用任意矩陣

**易錯點 3：PCA 未標準化**
- **錯誤**：直接對原始資料（不同尺度）進行 PCA
- **提醒**：先標準化再 PCA，避免尺度主導

**易錯點 4：訓練集與測試集分別降維**
- **錯誤**：訓練集擬合 PCA_1，測試集擬合 PCA_2，主成分不一致
- **提醒**：訓練集擬合 PCA，測試集使用同一轉換矩陣

**易錯點 5：過度降維**
- **錯誤**：為追求速度將 1000 維降至 2 維，資訊損失巨大
- **提醒**：觀察累積變異解釋，通常保留 90-95%
