# L23303 - 模型訓練評估驗證

## 1. 核心定義 (20%)

模型訓練評估驗證是確保機器學習模型品質的關鍵階段。訓練階段優化模型參數,評估階段量化模型效能,驗證階段確認模型泛化能力。核心包含:資料集劃分(Train/Val/Test)、K-Fold交叉驗證、評估指標(分類:Accuracy/Precision/Recall/F1/ROC-AUC,迴歸:MSE/RMSE/MAE/R²)、混淆矩陣、學習曲線分析。

### CFDS分解
```
ModelEvaluation = f(C, F, D, S)
C = Training ∘ Evaluation ∘ Validation ∘ MetricsComputation
F = {TrainedWeights, EvaluationMetrics, CrossValidationFolds}
D = TrainSet + ValSet + TestSet + PredictionsVector + GroundTruth
S = Training | Evaluating | Validated | Production
```

---

## 2. 關鍵公式 (25%)

### 2.1 分類指標

**混淆矩陣**:
```
                預測正  預測負
實際正    TP      FN
實際負    FP      TN

TP: True Positive（真陽性）
FP: False Positive（偽陽性）
TN: True Negative（真陰性）
FN: False Negative（偽陰性）
```

**Accuracy（準確率）**:
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

適用: 類別平衡資料
缺點: 類別不平衡時失效（99:1資料全預測多數類也有99%準確率）
```

**Precision（精確率）**:
```
Precision = TP / (TP + FP)

解釋: 預測為正例中,真正是正例的比例
應用: 減少誤報（如垃圾郵件過濾）
```

**Recall（召回率/敏感度）**:
```
Recall = TP / (TP + FN)

解釋: 實際正例中,被正確預測的比例
應用: 減少漏報（如疾病診斷、欺詐檢測）
```

**F1-Score**:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)

調和平均: 平衡Precision和Recall
F_β = (1 + β²) × (Precision × Recall) / (β²×Precision + Recall)
  β > 1: 偏重Recall
  β < 1: 偏重Precision
```

**ROC-AUC**:
```
TPR (True Positive Rate) = TP / (TP + FN) = Recall
FPR (False Positive Rate) = FP / (FP + TN)

ROC曲線: TPR vs FPR（不同閾值）
AUC: ROC曲線下面積
  AUC = 1: 完美分類器
  AUC = 0.5: 隨機猜測
  AUC > 0.9: 優秀
  AUC 0.7-0.9: 良好
```

### 2.2 迴歸指標

**MSE（均方誤差）**:
```
MSE = (1/N) Σᵢ (yᵢ - ŷᵢ)²

特點: 懲罰大誤差（平方項）
單位: 目標變數的平方
```

**RMSE（均方根誤差）**:
```
RMSE = √MSE = √[(1/N) Σᵢ (yᵢ - ŷᵢ)²]

特點: 與目標變數同單位,易解釋
```

**MAE（平均絕對誤差）**:
```
MAE = (1/N) Σᵢ |yᵢ - ŷᵢ|

特點: 對異常值不敏感（無平方項）
vs MSE: MAE更穩健,MSE懲罰大誤差
```

**R²（決定係數）**:
```
R² = 1 - (SS_res / SS_tot)

其中:
SS_res = Σ(yᵢ - ŷᵢ)²  # 殘差平方和
SS_tot = Σ(yᵢ - ȳ)²   # 總平方和

解釋: 模型解釋的方差比例
R² = 1: 完美預測
R² = 0: 與均值預測等價
R² < 0: 比均值預測更差
```

### 2.3 交叉驗證

**K-Fold交叉驗證**:
```
CV_Score = (1/K) Σₖ₌₁ᴷ Score(Model, Fold_k)

流程:
1. 分割K份（通常K=5或10）
2. 每次用K-1份訓練,1份驗證
3. 重複K次
4. 平均K次結果

標準誤:
SE = σ / √K

其中σ為K次分數的標準差
```

**Stratified K-Fold**:
```
保持每折中類別比例與原始資料相同

適用: 類別不平衡資料
範例: 原始90%正例10%負例
      每折也維持90:10
```

---

## 3. 對比矩陣 (15%)

### 分類指標對比

| 指標 | 公式 | 適用場景 | 優點 | 缺點 |
|------|------|---------|------|------|
| Accuracy | (TP+TN)/Total | 類別平衡 | 直觀易懂 | 不平衡失效 |
| Precision | TP/(TP+FP) | 減少誤報 | 衡量正例可信度 | 忽略漏報 |
| Recall | TP/(TP+FN) | 減少漏報 | 捕獲正例能力 | 忽略誤報 |
| F1-Score | 2PR/(P+R) | 平衡P和R | 綜合指標 | 不適合極端場景 |
| ROC-AUC | 曲線下面積 | 閾值無關評估 | 魯棒性高 | 不平衡時樂觀 |

### 迴歸指標對比

| 指標 | 單位 | 對異常值敏感度 | 可解釋性 | 適用場景 |
|------|------|---------------|---------|---------|
| MSE | y² | 極高 | 低 | 懲罰大誤差 |
| RMSE | y | 高 | 高 | 與目標同單位 |
| MAE | y | 低 | 高 | 穩健估計 |
| R² | 無 | 中 | 極高 | 解釋方差比例 |

---

## 4. 實務應用 (20%)

### 4.1 不平衡分類評估

**場景**: 信用卡欺詐檢測（欺詐0.1%）

```python
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# 混淆矩陣
cm = confusion_matrix(y_test, y_pred)
print(cm)

# 詳細報告
print(classification_report(y_test, y_pred))
"""
              precision    recall  f1-score   support
           0       1.00      0.98      0.99     99900
           1       0.08      0.92      0.15       100
    accuracy                           0.98    100000
"""

# ROC-AUC
auc = roc_auc_score(y_test, y_proba)
print(f"ROC-AUC: {auc:.4f}")  # 0.95

# 關鍵指標:
# - Recall（欺詐）: 92%（捕獲92%欺詐）✓
# - Precision（欺詐）: 8%（每100次預警8次真欺詐）
# - Accuracy: 98%（無意義,全預測正常也有99.9%）✗
```

**閾值調整**:
```python
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# 選擇Recall > 95%的最大Precision
idx = np.where(recalls >= 0.95)[0][0]
threshold = thresholds[idx]
print(f"新閾值: {threshold:.4f}")

y_pred_new = (y_proba >= threshold).astype(int)
# Recall: 96%, Precision: 5%（提升召回率）
```

### 4.2 交叉驗證實現

```python
from sklearn.model_selection import cross_val_score, cross_validate

# 單指標
scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
print(f"Mean AUC: {scores.mean():.4f} ± {scores.std():.4f}")

# 多指標
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
results = cross_validate(model, X, y, cv=5, scoring=scoring)

for metric in scoring:
    scores = results[f'test_{metric}']
    print(f"{metric}: {scores.mean():.4f} ± {scores.std():.4f}")
```

### 4.3 學習曲線分析

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='accuracy'
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Training')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation')
plt.xlabel('Training Size')
plt.ylabel('Accuracy')
plt.legend()

# 診斷:
# - 訓練高驗證低 -> 過擬合（增加正則化）
# - 兩者都低 -> 欠擬合（增加複雜度）
# - 兩者都高且接近 -> 理想狀態
```

---

## 5. 記憶口訣 (10%)

**「準召精F1,混淆矩陣記心間」**
- 準: Accuracy（整體準確）
- 召: Recall（召回率,減少漏報）
- 精: Precision（精確率,減少誤報）
- F1: 調和平均（平衡兩者）

**「分類ROC,迴歸R方,不平衡看召回」**
- 分類: ROC-AUC綜合評估
- 迴歸: R²解釋方差
- 不平衡: Recall+Precision為主

**「交叉驗證5折10折,測試集用一次莫多」**
- K-Fold: 5折或10折
- 測試集: 最終評估僅一次

---

## 6. 自我驗證 (10%)

**Q1: 類別極度不平衡（99:1）時,最應該關注的指標是?**
A. Accuracy
B. Precision
C. Recall
D. F1-Score

**答案: C（Recall）**
解析: 不平衡資料中Recall衡量少數類被正確捕獲的比例,Accuracy會被多數類主導而失效。

**Q2: 以下哪個迴歸指標對異常值最不敏感?**
A. MSE
B. RMSE
C. MAE
D. R²

**答案: C（MAE）**
解析: MAE使用絕對值,MSE/RMSE使用平方項會放大異常值影響。

**Q3: K-Fold交叉驗證的主要作用是?**
A. 加速訓練
B. 減少參數量
C. 評估泛化能力
D. 處理缺失值

**答案: C（評估泛化能力）**
解析: K-Fold充分利用資料,每份都當過驗證集,評估更可靠。

**簡答: 解釋Precision和Recall的權衡,並說明何時優先Precision、何時優先Recall。**

**答案:**
**Precision vs Recall權衡:**
- 提高閾值 -> Precision↑ Recall↓（更嚴格,減少誤報但增加漏報）
- 降低閾值 -> Precision↓ Recall↑（更寬鬆,減少漏報但增加誤報）

**優先Precision場景（誤報成本高）:**
- 垃圾郵件過濾（誤判正常郵件為垃圾郵件）
- 推薦系統（推薦不相關內容損害體驗）

**優先Recall場景（漏報成本高）:**
- 疾病診斷（漏診嚴重疾病致命）
- 欺詐檢測（漏掉欺詐損失大）
- 搜尋引擎（漏掉相關結果用戶流失）

**易錯點提醒:**
1. 不平衡資料用Accuracy評估（錯誤）
2. 測試集多次使用調參（過擬合測試集）
3. 混淆Precision和Recall（Precision=TP/(TP+FP), Recall=TP/(TP+FN)）
4. K-Fold後忘記在全部訓練集重新訓練
5. ROC-AUC在極度不平衡時過於樂觀（應用PR-AUC）
