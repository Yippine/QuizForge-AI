# L23203 - 深度學習原理與框架

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

深度學習是機器學習的子領域,透過多層神經網路自動學習資料的階層式特徵表示。從基礎的感知機、多層感知機（MLP）到卷積神經網路（CNN）、循環神經網路（RNN/LSTM）、再到革命性的Transformer架構,深度學習透過反向傳播演算法訓練百萬至數十億參數,在影像辨識、自然語言處理、語音辨識等領域達到甚至超越人類水準。

深度學習的核心要素包含:神經網路架構（網路層數、神經元數量、連接方式）、啟動函數（引入非線性）、損失函數（定義優化目標）、反向傳播（計算梯度）、優化器（更新參數）。理解這些數學原理與框架實現（TensorFlow、PyTorch）是掌握深度學習的關鍵。

### 1.2 核心概念

**深度學習的八大核心概念:**

1. **神經網路**:由輸入層、隱藏層、輸出層組成的多層結構,每層包含多個神經元
2. **啟動函數**:引入非線性（ReLU、Sigmoid、Tanh）,使網路能擬合複雜函數
3. **反向傳播**:透過鏈式法則計算梯度,是訓練深度網路的核心演算法
4. **卷積神經網路（CNN）**:利用卷積、池化層處理影像資料,自動提取空間特徵
5. **循環神經網路（RNN/LSTM）**:處理序列資料（文本、時序）,具有記憶能力
6. **Transformer**:基於Self-Attention機制,取代RNN成為NLP主流架構
7. **深度學習框架**:TensorFlow、PyTorch提供自動微分、GPU加速、模型部署
8. **訓練技巧**:Dropout、Batch Normalization、殘差連接、學習率調整

### 1.3 CFDS 分解

基於 Formula-Contract 方法論,將深度學習分解為四個基本單元:

```
DeepLearning = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = ForwardPass ∘ LossComputation ∘ BackwardPass ∘ ParameterUpdate
  = 前向傳播 -> 損失計算 -> 反向傳播 -> 參數更新

例如:
CNN = ConvLayer ∘ ReLU ∘ Pooling ∘ FullyConnected ∘ Softmax
RNN = RecurrentCell ∘ StateUpdate ∘ OutputGeneration
Transformer = MultiHeadAttention ∘ FeedForward ∘ LayerNorm
```

**F (Files - 配置資源)**
```
F = {Weights, Biases, Architecture, Hyperparameters, Checkpoints}
  = {權重矩陣, 偏差向量, 網路架構, 超參數, 訓練檢查點}

例如:
CNN_Files = {conv_kernels(3×3×64), fc_weights(512×10), learning_rate(0.001)}
Transformer_Files = {Q,K,V_matrices, positional_encoding, n_heads(8), d_model(512)}
```

**D (Data - 資料結構)**
```
D = InputTensor + OutputTensor + LabelTensor + BatchData
  = 輸入張量 + 輸出張量 + 標籤張量 + 批次資料

例如:
Image_Data = Tensor(batch_size, height, width, channels)  # (32, 224, 224, 3)
Text_Data = Tensor(batch_size, sequence_length)  # (16, 512)
Label_Data = OneHot(batch_size, num_classes)  # (32, 10)
```

**S (State - 運行狀態)**
```
S = Initializing | Training | Validating | Converged | Overfitting | Deployed
  = 初始化 | 訓練中 | 驗證中 | 已收斂 | 過擬合 | 已部署

訓練狀態: Epoch_1 | Batch_100 | Loss_Decreasing | Gradient_Vanishing
模型狀態: Inference_Mode | Training_Mode | Frozen_Layers
```

### 1.4 技術定位

深度學習在技術棧中處於AI應用的核心引擎層,連接基礎數學（線性代數、微積分、最優化）與實際應用（影像辨識、自然語言理解、語音合成）。在企業應用中:

- **影像處理**:CNN（ResNet、EfficientNet、YOLO、Mask R-CNN）
- **自然語言**:Transformer（BERT、GPT、T5）
- **語音處理**:RNN/LSTM、Wav2Vec、Whisper
- **推薦系統**:Deep Learning + Collaborative Filtering
- **時序預測**:LSTM、GRU、Temporal Convolutional Networks

深度學習框架選擇:
- **TensorFlow**:Google開發、生產部署強（TensorFlow Serving）、Keras高階API
- **PyTorch**:Meta開發、研究友好、動態計算圖、易於除錯

理解深度學習原理與框架使用是成為AI工程師的必備能力。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 神經網路基礎

**感知機（Perceptron）**:
```
單層感知機:
y = f(w^T x + b) = f(Σᵢ wᵢxᵢ + b)

其中:
- w: 權重向量
- x: 輸入向量
- b: 偏差（bias）
- f: 啟動函數

階躍函數（原始感知機）:
f(z) = {1 if z ≥ 0; 0 if z < 0}

限制: 僅能解決線性可分問題（無法解決XOR問題）
```

**多層感知機（MLP）**:
```
兩層網路:
h = σ(W₁x + b₁)  # 隱藏層
ŷ = σ(W₂h + b₂)  # 輸出層

一般形式（L層網路）:
a⁰ = x  # 輸入層
a^l = σ(W^l a^(l-1) + b^l), l = 1, 2, ..., L  # 第l層

其中:
- W^l: 第l層權重矩陣 (n_l × n_(l-1))
- b^l: 第l層偏差向量 (n_l × 1)
- σ: 啟動函數
- a^l: 第l層啟動值（輸出）

參數總數:
Params = Σₗ [n_l × n_(l-1) + n_l]
```

### 2.2 啟動函數

**ReLU（Rectified Linear Unit）**:
```
ReLU(x) = max(0, x) = {x if x > 0; 0 if x ≤ 0}

導數:
d ReLU(x)/dx = {1 if x > 0; 0 if x ≤ 0}

優點:
- 計算簡單
- 緩解梯度消失
- 稀疏啟動（約50%神經元為0）

缺點:
- Dead ReLU（x<0時梯度為0,神經元永遠不更新）
```

**Leaky ReLU（解決Dead ReLU）**:
```
Leaky_ReLU(x) = {x if x > 0; αx if x ≤ 0}  # α通常0.01

導數:
d Leaky_ReLU(x)/dx = {1 if x > 0; α if x ≤ 0}
```

**Sigmoid**:
```
σ(x) = 1 / (1 + e^(-x))

導數:
σ'(x) = σ(x)(1 - σ(x))

特性:
- 輸出範圍 (0, 1)
- 可解釋為機率
- 缺點: 梯度消失（x很大或很小時梯度接近0）

應用: 二元分類輸出層
```

**Tanh（雙曲正切）**:
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))

導數:
tanh'(x) = 1 - tanh²(x)

特性:
- 輸出範圍 (-1, 1)
- 零中心化（相比Sigmoid）
- 仍有梯度消失問題

應用: RNN隱藏層
```

**Softmax（多元分類）**:
```
Softmax(z)ᵢ = e^(zᵢ) / Σⱼ e^(zⱼ)

性質:
- Σᵢ Softmax(z)ᵢ = 1（機率分佈）
- Softmax(z)ᵢ ∈ (0, 1)

應用: 多元分類輸出層
```

### 2.3 反向傳播演算法

**損失函數**:
```
分類任務（交叉熵）:
L = -Σᵢ yᵢ log(ŷᵢ)

迴歸任務（均方誤差）:
L = (1/2) Σᵢ (yᵢ - ŷᵢ)²
```

**反向傳播（Backpropagation）**:
```
目標: 計算 ∂L/∂W^l 和 ∂L/∂b^l（每層參數的梯度）

前向傳播:
z^l = W^l a^(l-1) + b^l  # 線性組合
a^l = σ(z^l)  # 啟動

反向傳播（鏈式法則）:
δ^L = ∂L/∂z^L = (∂L/∂a^L) ⊙ σ'(z^L)  # 輸出層誤差
δ^l = (W^(l+1))^T δ^(l+1) ⊙ σ'(z^l)  # 第l層誤差（反向傳遞）

梯度:
∂L/∂W^l = δ^l (a^(l-1))^T  # 權重梯度
∂L/∂b^l = δ^l  # 偏差梯度

參數更新（梯度下降）:
W^l := W^l - η ∂L/∂W^l
b^l := b^l - η ∂L/∂b^l

其中:
- ⊙: 逐元素乘法（Hadamard product）
- η: 學習率
```

**梯度消失與梯度爆炸**:
```
梯度消失（Vanishing Gradient）:
δ^l = (W^(l+1))^T δ^(l+1) ⊙ σ'(z^l)

若σ'(z^l) << 1（如Sigmoid導數最大0.25）:
δ^l會隨層數指數衰減 -> 深層網路參數無法更新

解決方案:
- ReLU啟動函數（導數0或1）
- 殘差連接（ResNet）
- Batch Normalization
- 更好的初始化（Xavier、He初始化）

梯度爆炸（Exploding Gradient）:
若W^l或σ'過大,梯度指數增長 -> 參數更新過大

解決方案:
- 梯度裁剪（Gradient Clipping）: grad = min(grad, threshold)
- 權重正則化（L2）
```

### 2.4 卷積神經網路（CNN）

**卷積層（Convolutional Layer）**:
```
卷積運算:
(f * g)[i,j] = ΣΣ f[m,n] × g[i-m, j-n]

二維卷積（影像處理）:
Y[i,j] = Σₘ Σₙ W[m,n] × X[i+m, j+n] + b

多通道卷積:
Y[i,j,k] = Σc Σₘ Σₙ W[m,n,c,k] × X[i+m, j+n, c] + b[k]

其中:
- X: 輸入特徵圖 (H, W, C_in)
- W: 卷積核（濾波器）(K_h, K_w, C_in, C_out)
- Y: 輸出特徵圖 (H', W', C_out)

輸出尺寸:
H' = (H + 2P - K_h) / S + 1
W' = (W + 2P - K_w) / S + 1

其中:
- P: Padding（填充）
- S: Stride（步長）
- K: Kernel Size（卷積核大小）

參數量:
Params = K_h × K_w × C_in × C_out + C_out（偏差）
```

**池化層（Pooling Layer）**:
```
最大池化（Max Pooling）:
Y[i,j] = max{X[i×S+m, j×S+n] | m,n ∈ pooling_region}

平均池化（Average Pooling）:
Y[i,j] = (1/K²) ΣΣ X[i×S+m, j×S+n]

常見配置: 2×2池化窗口,步長2

作用:
- 降低特徵圖尺寸
- 減少參數量
- 增加感受野
- 平移不變性
```

**典型CNN架構**:
```
LeNet-5（1998）:
Input(32×32) -> Conv(6) -> Pool -> Conv(16) -> Pool -> FC(120) -> FC(84) -> Output(10)

VGG-16（2014）:
多個3×3卷積堆疊 + 2×2池化,深度16層

ResNet（2015）:
引入殘差連接: y = F(x) + x（跳躍連接）
解決深度網路梯度消失,深度可達152層
```

### 2.5 循環神經網路（RNN/LSTM）

**RNN基本結構**:
```
標準RNN:
h_t = tanh(W_hh h_(t-1) + W_xh x_t + b_h)  # 隱藏狀態更新
y_t = W_hy h_t + b_y  # 輸出

其中:
- h_t: 第t步隱藏狀態（記憶）
- x_t: 第t步輸入
- y_t: 第t步輸出

展開形式（序列長度T）:
h₁ = tanh(W_hh h₀ + W_xh x₁ + b_h)
h₂ = tanh(W_hh h₁ + W_xh x₂ + b_h)
...
h_T = tanh(W_hh h_(T-1) + W_xh x_T + b_h)

問題: 長期依賴問題（梯度消失）
∂L/∂h₁ = ∂L/∂h_T × ∏ₜ₌₂ᵀ ∂h_t/∂h_(t-1)
若∂h_t/∂h_(t-1) < 1,梯度指數衰減
```

**LSTM（Long Short-Term Memory）**:
```
LSTM解決長期依賴問題,引入門控機制:

遺忘門（Forget Gate）:
f_t = σ(W_f [h_(t-1), x_t] + b_f)  # 決定遺忘多少舊記憶

輸入門（Input Gate）:
i_t = σ(W_i [h_(t-1), x_t] + b_i)  # 決定接受多少新資訊
C̃_t = tanh(W_C [h_(t-1), x_t] + b_C)  # 候選記憶

記憶單元更新（Cell State）:
C_t = f_t ⊙ C_(t-1) + i_t ⊙ C̃_t  # 選擇性遺忘+更新

輸出門（Output Gate）:
o_t = σ(W_o [h_(t-1), x_t] + b_o)  # 決定輸出多少資訊
h_t = o_t ⊙ tanh(C_t)  # 隱藏狀態

關鍵:
- C_t（Cell State）是長期記憶通道,梯度流動順暢
- 門控機制（σ函數）輸出[0,1],控制資訊流動
- 參數量: 4倍於標準RNN（4組權重矩陣）
```

**GRU（Gated Recurrent Unit）**:
```
GRU簡化LSTM,僅兩個門:

重置門（Reset Gate）:
r_t = σ(W_r [h_(t-1), x_t])

更新門（Update Gate）:
z_t = σ(W_z [h_(t-1), x_t])

候選隱藏狀態:
h̃_t = tanh(W [r_t ⊙ h_(t-1), x_t])

隱藏狀態更新:
h_t = (1 - z_t) ⊙ h_(t-1) + z_t ⊙ h̃_t

優勢: 參數量比LSTM少,訓練速度快,性能接近
```

### 2.6 Transformer架構

**Self-Attention機制**:
```
Query-Key-Value Attention:
Attention(Q, K, V) = softmax(QK^T / √d_k) V

其中:
- Q: Query矩陣 (seq_len, d_k)
- K: Key矩陣 (seq_len, d_k)
- V: Value矩陣 (seq_len, d_v)
- d_k: Key維度（縮放因子√d_k防止softmax梯度過小）

計算流程:
1. 計算相似度: QK^T（每個Query與所有Key的點積）
2. 縮放: QK^T / √d_k
3. Softmax歸一化: α = softmax(QK^T / √d_k)（注意力權重）
4. 加權求和: output = αV

Self-Attention:
Q = XW_Q, K = XW_K, V = XW_V（X為輸入序列）
Attention(X) = softmax(XW_Q(XW_K)^T / √d_k) XW_V
```

**Multi-Head Attention**:
```
多頭注意力（並行多個Attention）:
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

MultiHead(Q, K, V) = Concat(head₁, head₂, ..., head_h) W^O

其中:
- h: 頭數（通常8或16）
- W_i^Q, W_i^K, W_i^V: 第i個頭的投影矩陣
- W^O: 輸出投影矩陣

優勢:
- 捕捉不同子空間的特徵（如位置、語義、語法）
- 增強表達能力
```

**Transformer編碼器層**:
```
EncoderLayer(x) = LayerNorm(x + MultiHeadAttention(x))  # 注意力+殘差
                = LayerNorm(x + FeedForward(x))  # 前饋網路+殘差

FeedForward(x) = ReLU(xW₁ + b₁)W₂ + b₂  # 兩層全連接

完整Transformer編碼器:
Encoder = Stack(EncoderLayer × N)  # N層堆疊（BERT: N=12或24）
```

**位置編碼（Positional Encoding）**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

其中:
- pos: 位置（0, 1, 2, ...）
- i: 維度索引（0 到 d_model/2）
- d_model: 模型維度（如512）

作用: 注入序列位置資訊（Transformer本身無位置感知）
```

### 2.7 深度學習框架

**自動微分（Automatic Differentiation）**:
```
PyTorch範例:
import torch

x = torch.tensor(2.0, requires_grad=True)
y = x**2 + 3*x + 1

y.backward()  # 自動計算梯度
print(x.grad)  # dy/dx = 2x + 3 = 7

多變數:
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = (x**2).sum()
y.backward()
print(x.grad)  # [2.0, 4.0]
```

**計算圖（Computational Graph）**:
```
靜態圖（TensorFlow 1.x）:
1. 定義計算圖
2. 編譯優化
3. 執行（Session.run）

動態圖（PyTorch）:
1. 定義即執行（Define-by-Run）
2. 靈活除錯（可用print、debugger）
3. 易於實現複雜控制流
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 啟動函數對比

| 啟動函數 | 公式 | 範圍 | 優點 | 缺點 | 適用場景 |
|---------|------|------|------|------|---------|
| **ReLU** | max(0,x) | [0,∞) | 計算快、緩解梯度消失 | Dead ReLU | 隱藏層首選 |
| **Leaky ReLU** | max(αx,x) | (-∞,∞) | 解決Dead ReLU | 需調α | 深度網路 |
| **Sigmoid** | 1/(1+e^-x) | (0,1) | 機率解釋 | 梯度消失嚴重 | 二元分類輸出 |
| **Tanh** | (e^x-e^-x)/(e^x+e^-x) | (-1,1) | 零中心化 | 梯度消失 | RNN隱藏層 |
| **Softmax** | e^xi/Σe^xj | (0,1)且和為1 | 機率分佈 | 計算量大 | 多元分類輸出 |

### 3.2 神經網路架構對比

| 架構 | 結構特點 | 參數量 | 計算複雜度 | 適用資料 | 典型應用 |
|------|---------|--------|-----------|---------|---------|
| **MLP** | 全連接層 | 極高 | O(n²) | 表格、向量 | 分類、迴歸基準 |
| **CNN** | 卷積+池化 | 中 | O(K²·C·H·W) | 影像、網格 | 影像辨識、物件檢測 |
| **RNN** | 循環連接 | 中 | O(T·d²) | 序列、時序 | 語言模型、時序預測 |
| **LSTM** | 門控RNN | 高（4倍RNN） | O(T·d²) | 長序列 | 機器翻譯、語音辨識 |
| **Transformer** | Self-Attention | 極高 | O(T²·d) | 序列 | NLP（BERT、GPT） |

### 3.3 RNN vs LSTM vs Transformer

| 維度 | RNN | LSTM | Transformer |
|------|-----|------|-------------|
| **長期依賴** | 差（梯度消失） | 優（門控機制） | 最優（直接連接） |
| **並行性** | 無（串行計算） | 無（串行計算） | 高（並行計算） |
| **記憶機制** | 隱藏狀態 | Cell State | Self-Attention |
| **參數量** | 基準 | 4倍 | 極高 |
| **訓練速度** | 慢（串行） | 慢（串行） | 快（並行+GPU） |
| **位置資訊** | 內建（順序） | 內建（順序） | 需位置編碼 |
| **典型應用** | 簡單序列 | 機器翻譯、語音 | BERT、GPT、T5 |

**選擇指南**:
- **短序列（<100）**: RNN
- **長序列（100-1000）**: LSTM/GRU
- **極長序列（>1000）**: Transformer
- **計算資源有限**: LSTM
- **追求最佳性能**: Transformer

### 3.4 深度學習框架對比

| 框架 | 開發者 | 計算圖 | 易用性 | 部署 | 生態 | 適用場景 |
|------|--------|--------|--------|------|------|---------|
| **PyTorch** | Meta | 動態圖 | 極高 | 中（TorchServe） | 豐富 | 研究、原型開發 |
| **TensorFlow** | Google | 靜態圖（2.x動態） | 中 | 優（TF Serving） | 最豐富 | 生產部署 |
| **Keras** | 獨立（集成TF） | TensorFlow後端 | 極高 | 優 | 豐富 | 快速原型、教學 |
| **JAX** | Google | 動態圖 | 中 | 低 | 成長中 | 高性能研究 |

**PyTorch vs TensorFlow**:
```
PyTorch優勢:
- 動態圖,易於除錯（print、debugger直接可用）
- Pythonic風格,學習曲線平緩
- 研究社群首選（論文程式碼80%用PyTorch）
- torch.nn簡潔優雅

TensorFlow優勢:
- 生產部署完善（TensorFlow Serving、TF Lite、TF.js）
- 大規模分散式訓練
- TPU支援（Google Cloud）
- Keras高階API（keras.Sequential非常簡潔）

選擇建議:
- 研究/學術: PyTorch
- 工業/生產: TensorFlow
- 快速原型: Keras（TensorFlow後端）
- 兩者都學（趨勢:API趨同,遷移成本低）
```

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一:影像分類（CNN）

**場景描述**:識別貓狗影像（binary classification）,使用CIFAR-10資料集。

**架構選擇**:ResNet-18（殘差網路）

**實現要點**:
```python
import torch
import torch.nn as nn
import torchvision.models as models

# 1. 載入預訓練ResNet-18
model = models.resnet18(pretrained=True)

# 2. 修改最後一層（1000類->2類）
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 2)  # 貓狗二分類

# 3. 定義損失函數與優化器
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 4. 訓練循環
for epoch in range(10):
    for images, labels in train_loader:
        # 前向傳播
        outputs = model(images)
        loss = criterion(outputs, labels)

        # 反向傳播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**公式化流程**:
```
ImageClassification = DataAugmentation ∘ ResNet ∘ Softmax ∘ CrossEntropyLoss

ResNet = Conv1 -> ResBlock × 4 -> GlobalAvgPool -> FC

ResBlock(x) = Conv(Conv(x)) + x  # 殘差連接

損失函數:
L = -Σᵢ yᵢ log(softmax(f(xᵢ)))

梯度下降:
θ := θ - η ∇_θ L
```

**關鍵技巧**:
1. **遷移學習（Transfer Learning）**: 使用ImageNet預訓練權重,僅微調最後幾層
2. **資料增強（Data Augmentation）**: 隨機裁剪、翻轉、顏色抖動
3. **學習率調整**: Cosine Annealing或Step Decay
4. **正則化**: Dropout(0.5)、L2 Weight Decay(0.0001)

**效能結果**:
- 訓練集準確率: 99%
- 測試集準確率: 95%（遷移學習）vs 85%（從頭訓練）
- 訓練時間: 30分鐘（GPU）vs 5小時（CPU）

### 4.2 應用場景二:文本情感分析（Transformer）

**場景描述**:分析影評是正面或負面情感（IMDB資料集）。

**架構選擇**:BERT（Bidirectional Encoder Representations from Transformers）

**實現要點**:
```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# 1. 載入預訓練BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 2. 文本編碼
text = "This movie is fantastic!"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)

# 3. 前向傳播
outputs = model(**inputs)
logits = outputs.logits  # (batch_size, num_labels)
prediction = torch.argmax(logits, dim=1)  # 0: 負面, 1: 正面

# 4. 微調訓練
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
for epoch in range(3):
    for batch in train_loader:
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

**BERT公式化**:
```
BERT = TokenEmbedding + PositionalEncoding + TransformerEncoder × 12

Transformer Encoder:
x = LayerNorm(x + MultiHeadAttention(x))
x = LayerNorm(x + FeedForward(x))

Self-Attention:
Attention(Q,K,V) = softmax(QK^T / √d_k) V

分類:
[CLS] token的輸出 -> Linear -> Softmax -> {Positive, Negative}
```

**效能對比**:
| 模型 | 準確率 | 訓練時間 | 推理速度 |
|------|--------|---------|---------|
| LSTM | 87% | 2小時 | 50句/秒 |
| BERT-base | 93% | 1小時（GPU） | 10句/秒 |
| BERT-large | 95% | 4小時（GPU） | 3句/秒 |

### 4.3 應用場景三:時序預測（LSTM）

**場景描述**:預測股票價格（回歸任務）,基於過去30天價格預測未來1天。

**架構選擇**:雙層LSTM

**實現要點**:
```python
import torch.nn as nn

class StockPriceLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x: (batch_size, seq_len=30, input_size=1)
        lstm_out, (h_n, c_n) = self.lstm(x)
        # lstm_out: (batch_size, seq_len, hidden_size)
        # 取最後時間步
        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size)
        prediction = self.fc(last_output)  # (batch_size, 1)
        return prediction

# 訓練
model = StockPriceLSTM()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    for seq, target in train_loader:
        # seq: (batch, 30, 1), target: (batch, 1)
        pred = model(seq)
        loss = criterion(pred, target)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**LSTM公式詳解**:
```
輸入: x_1, x_2, ..., x_30（過去30天價格）

LSTM展開:
For t = 1 to 30:
    f_t = σ(W_f [h_(t-1), x_t] + b_f)  # 遺忘門
    i_t = σ(W_i [h_(t-1), x_t] + b_i)  # 輸入門
    C̃_t = tanh(W_C [h_(t-1), x_t] + b_C)  # 候選記憶
    C_t = f_t ⊙ C_(t-1) + i_t ⊙ C̃_t  # 記憶更新
    o_t = σ(W_o [h_(t-1), x_t] + b_o)  # 輸出門
    h_t = o_t ⊙ tanh(C_t)  # 隱藏狀態

輸出: ŷ = FC(h_30)（基於第30步隱藏狀態預測第31天價格）

損失: MSE = (y_31 - ŷ)²
```

**效能結果**:
- RMSE: 2.5%（平均預測誤差2.5%）
- MAE: 1.8%
- R²: 0.85

### 4.4 實作步驟（深度學習專案流程）

**標準八步流程**:

1. **問題定義**: 分類/迴歸/生成、評估指標、資料類型（影像/文本/序列）
2. **資料準備**: 收集、清洗、標註、劃分（Train/Val/Test = 70%/15%/15%）
3. **資料預處理**:
   - 影像: 縮放（224×224）、正規化（ImageNet均值/標準差）
   - 文本: Tokenization、Padding、詞彙表
   - 序列: 窗口切分、正規化
4. **模型選擇**:
   - 影像: CNN（ResNet、EfficientNet）
   - 文本: Transformer（BERT、GPT）
   - 序列: LSTM/GRU
5. **訓練配置**:
   - 損失函數: CrossEntropy（分類）、MSE（迴歸）
   - 優化器: Adam（學習率1e-3）、AdamW（權重衰減）
   - Batch Size: 32（小GPU）、128（大GPU）
6. **訓練與驗證**:
   - 監控訓練/驗證損失曲線
   - Early Stopping（驗證損失10 epochs未改善停止）
   - 學習率調整（ReduceLROnPlateau）
7. **超參數調校**: 網格搜尋（learning_rate、batch_size、dropout_rate）
8. **測試與部署**: 測試集最終評估、模型儲存、部署（TorchServe、TF Serving）

### 4.5 常見陷阱

**陷阱1: 梯度消失/爆炸未處理**
- **問題**: 深度網路訓練時損失不下降或NaN
- **解決**: 使用ReLU、Batch Normalization、殘差連接、梯度裁剪

**陷阱2: 過擬合（訓練準確率99%,測試60%）**
- **問題**: 訓練測試差距大
- **解決**: Dropout、L2正則化、資料增強、Early Stopping、減少模型複雜度

**陷阱3: 學習率設定不當**
- **問題**: 學習率過大（損失震盪）或過小（收斂極慢）
- **解決**: 學習率搜尋（1e-4到1e-1）、學習率調整策略

**陷阱4: 批次大小選擇錯誤**
- **問題**: 批次太小（訓練不穩定）、太大（記憶體溢位、泛化差）
- **解決**: 平衡記憶體與穩定性（通常32-128）

**陷阱5: 忽略資料不平衡**
- **問題**: 類別比例1:99,模型全預測多數類也有99%準確率
- **解決**: 加權損失、過採樣（SMOTE）、評估指標用F1/AUC

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**「前向反向兩步走,卷積循環Transformer優」**
- **前向傳播**: 輸入->隱藏層->輸出（計算預測值）
- **反向傳播**: 計算梯度->更新參數（鏈式法則）
- **CNN**: 影像處理王者（卷積+池化）
- **RNN/LSTM**: 序列資料能手（記憶機制）
- **Transformer**: NLP霸主（Self-Attention）

**「ReLU隱藏Sigmoid輸出,梯度消失找殘差」**
- **ReLU**: 隱藏層首選（max(0,x)）
- **Sigmoid**: 二元分類輸出層（0-1機率）
- **Softmax**: 多元分類輸出層（機率分佈）
- **殘差連接**: 解決梯度消失（y=F(x)+x）

### 5.2 記憶技巧

**反向傳播口訣**:「輸出誤差往回傳,鏈式法則算梯度」
```
前向: x -> z=Wx+b -> a=σ(z) -> L
反向: ∂L/∂W = ∂L/∂a × ∂a/∂z × ∂z/∂W（鏈式法則）

記憶: 誤差像水流,從輸出層「倒流」回輸入層
```

**LSTM門控記憶**:「遺忘輸入輸出三道門,記憶單元貫穿全」
```
f: Forget Gate（遺忘舊記憶）
i: Input Gate（接受新資訊）
o: Output Gate（輸出資訊）
C: Cell State（長期記憶通道）

口訣: 「進門(i)檢查,出門(o)確認,忘記(f)過去」
```

**CNN結構速記**:「卷積提特徵,池化降維度,全連接做分類」
```
影像(224×224×3)
-> Conv(64卷積核) -> ReLU -> Pool
-> Conv(128) -> ReLU -> Pool
-> Flatten -> FC(512) -> Dropout -> FC(10) -> Softmax
```

**Transformer核心**:「QKV三兄弟,點積算注意力」
```
Q: Query（問題）
K: Key（鑰匙）
V: Value（值）

Attention = softmax(QK^T/√d_k) V

記憶: Q和K點積算相似度,用V加權求和
```

**啟動函數選擇**:「分類Softmax,隱藏ReLU,舊模型Sigmoid」
```
輸出層:
  二元分類 -> Sigmoid
  多元分類 -> Softmax
  迴歸 -> Linear（無啟動函數）

隱藏層:
  現代網路 -> ReLU/Leaky ReLU
  RNN -> Tanh
```

### 5.3 快速回憶

**深度學習架構選擇樹**:
```
資料類型?
  ├─ 影像 -> CNN（ResNet、EfficientNet）
  ├─ 文本 -> Transformer（BERT、GPT）
  ├─ 序列 -> LSTM/GRU
  ├─ 語音 -> CNN+LSTM或Transformer
  └─ 表格 -> MLP或XGBoost

序列長度?（針對序列資料）
  ├─ 短(<100) -> RNN
  ├─ 中(100-1000) -> LSTM/GRU
  └─ 長(>1000) -> Transformer
```

**超參數速配**:
```
學習率:
  - Adam: 1e-3到1e-4（通用）
  - SGD: 1e-1到1e-2（需動量）
  - BERT微調: 2e-5（極小）

Batch Size:
  - 小GPU(4GB): 16-32
  - 中GPU(8-16GB): 64-128
  - 大GPU(>24GB): 256+

Dropout:
  - 全連接層: 0.5
  - 卷積層: 0.2-0.3
  - Transformer: 0.1
```

**損失函數速配**:
```
分類: CrossEntropyLoss
  L = -Σy log(ŷ)

迴歸: MSELoss
  L = (1/N)Σ(y-ŷ)²

二元分類: BCELoss（Binary Cross Entropy）
  L = -[y log(ŷ) + (1-y) log(1-ŷ)]
```

### 5.4 易混淆辨析

**前向傳播 vs 反向傳播**:
- **前向**:輸入->計算->輸出（得到預測值和損失）
- **反向**:損失->梯度->更新參數（訓練模型）

**RNN vs LSTM**:
- **RNN**:簡單循環,易梯度消失,短序列
- **LSTM**:三門控+Cell State,解決長期依賴,長序列

**Sigmoid vs Softmax**:
- **Sigmoid**:二元分類,輸出(0,1),單一神經元
- **Softmax**:多元分類,輸出和為1,多個神經元

**Dropout vs Batch Normalization**:
- **Dropout**:隨機丟棄神經元,防過擬合,訓練時啟用
- **Batch Norm**:正規化啟動值,加速訓練,訓練/推理都用

**卷積 vs 全連接**:
- **卷積**:局部連接、參數共享、平移不變性、適合影像
- **全連接**:每個神經元連接所有輸入、參數量巨大、適合向量

**PyTorch vs TensorFlow**:
- **PyTorch**:動態圖、易除錯、研究首選
- **TensorFlow**:靜態圖（2.x支援動態）、生產部署強

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**Q1:以下哪個啟動函數最適合深度神經網路隱藏層?**
A. Sigmoid
B. Tanh
C. ReLU
D. Linear

**Q2:LSTM相比標準RNN的主要優勢是?**
A. 訓練速度更快
B. 參數量更少
C. 解決長期依賴問題
D. 不需要反向傳播

**Q3:Transformer的Self-Attention機制時間複雜度是?（T=序列長度）**
A. O(T)
B. O(T log T)
C. O(T²)
D. O(T³)

**Q4:以下哪個不是解決梯度消失的方法?**
A. 使用ReLU啟動函數
B. 增加網路深度
C. 殘差連接（ResNet）
D. Batch Normalization

**Q5:卷積神經網路（CNN）的主要優勢是?（多選）**
A. 參數共享
B. 局部連接
C. 平移不變性
D. 處理序列資料

### 6.2 簡答題

**Q1:解釋反向傳播演算法的核心思想,並說明鏈式法則在其中的作用。**

**Q2:為什麼Transformer在NLP任務中逐步取代RNN/LSTM?列舉至少三個技術原因。**

### 6.3 答案解析

**選擇題答案**:

**A1:C（ReLU）**
- **解析**:ReLU（max(0,x)）計算簡單、緩解梯度消失（正值導數為1）、訓練速度快、現代深度網路首選。Sigmoid/Tanh有嚴重梯度消失問題（深層網路梯度指數衰減）,Linear無非線性（多層等於單層）。

**A2:C（解決長期依賴問題）**
- **解析**:LSTM通過門控機制（遺忘門、輸入門、輸出門）和Cell State（長期記憶通道）解決RNN的梯度消失問題,能捕捉長距離依賴（100+時間步）。訓練速度比RNN慢（參數量4倍）,仍需反向傳播。

**A3:C（O(T²)）**
- **解析**:Self-Attention計算QK^T需O(T²)時間（T×T相似度矩陣）,這是Transformer處理長序列的瓶頸。相比RNN的O(T)（串行計算）,Transformer犧牲時間複雜度換取並行性。

**A4:B（增加網路深度）**
- **解析**:增加網路深度會加劇梯度消失（梯度隨層數指數衰減）,而非解決方法。正確方法:ReLU（導數0或1）、殘差連接（梯度直接流動）、Batch Norm（穩定梯度分佈）、更好的初始化。

**A5:A,B,C（參數共享、局部連接、平移不變性）**
- **解析**:
  - **A-參數共享**:同一卷積核掃描整張影像,參數量遠小於全連接（3×3×64核 vs 224×224×64全連接）
  - **B-局部連接**:每個神經元只連接局部區域（感受野）,符合影像局部特徵性質
  - **C-平移不變性**:貓在左上角或右下角都能識別（卷積核在各位置共享）
  - D錯誤:CNN主要處理網格資料（影像）,序列資料用RNN/Transformer

**簡答題答案**:

**A1:反向傳播演算法核心思想**

**定義**:
反向傳播（Backpropagation）是訓練神經網路的核心演算法,透過鏈式法則高效計算損失函數對所有參數的梯度,進而用梯度下降更新參數。

**核心思想**:
1. **前向傳播**:輸入->隱藏層->輸出,計算預測值和損失
2. **反向傳播**:從輸出層向輸入層「倒流」,逐層計算梯度
3. **參數更新**:用梯度下降更新所有參數

**鏈式法則的作用**:

神經網路是函數的複合:
```
L = Loss(y, ŷ)
ŷ = f_L(...f_2(f_1(x, W₁), W₂)..., W_L)
```

計算∂L/∂W_l（第l層權重梯度）需鏈式法則:
```
∂L/∂W_l = ∂L/∂a_L × ∂a_L/∂a_(L-1) × ... × ∂a_(l+1)/∂a_l × ∂a_l/∂W_l

其中a_l為第l層啟動值
```

**具體流程**（兩層網路範例）:
```
前向傳播:
z₁ = W₁x + b₁
a₁ = σ(z₁)
z₂ = W₂a₁ + b₂
ŷ = σ(z₂)
L = (y - ŷ)²/2

反向傳播（鏈式法則展開）:
∂L/∂ŷ = -(y - ŷ)

輸出層誤差:
δ₂ = ∂L/∂z₂ = ∂L/∂ŷ × ∂ŷ/∂z₂ = -(y-ŷ) ⊙ σ'(z₂)

隱藏層誤差（鏈式法則關鍵）:
δ₁ = ∂L/∂z₁ = ∂L/∂z₂ × ∂z₂/∂a₁ × ∂a₁/∂z₁
   = δ₂ × W₂^T ⊙ σ'(z₁)

梯度:
∂L/∂W₂ = δ₂ a₁^T
∂L/∂W₁ = δ₁ x^T

參數更新:
W₂ := W₂ - η ∂L/∂W₂
W₁ := W₁ - η ∂L/∂W₁
```

**鏈式法則優勢**:
- **高效計算**:避免重複計算（動態規劃思想）
- **自動微分**:現代框架（PyTorch、TensorFlow）自動實現
- **可擴展**:適用任意深度網路

**類比**:
反向傳播像「責任倒追」,輸出層錯誤向前層層追溯,每層根據「責任大小」（梯度）調整參數。

**A2:Transformer取代RNN/LSTM的原因**

**技術原因（至少三個）**:

**1.並行化訓練（計算效率）**:
```
RNN/LSTM:
  h₁ = f(x₁, h₀)
  h₂ = f(x₂, h₁)  # 依賴h₁,必須串行
  ...
  h_T = f(x_T, h_(T-1))

  無法並行,訓練慢（O(T)步）

Transformer:
  所有位置同時計算Self-Attention
  Attention(Q,K,V) = softmax(QK^T/√d_k)V  # 矩陣運算,可並行

  GPU並行化,訓練快10-100倍
```

**2.直接捕捉長距離依賴**:
```
RNN/LSTM:
  資訊從t=1傳到t=100需經過100步
  梯度經過100步會衰減（即使LSTM也有限制）

Transformer:
  任意兩個位置直接連接（Self-Attention）
  位置i和位置j的關係直接計算（一步）

  範例:
  "The cat, which was very fluffy, sat on the mat"
  "cat"和"sat"距離8個詞,Transformer直接建立連接
  RNN需傳遞8步資訊
```

**3.可解釋的注意力權重**:
```
Transformer Attention:
  α_ij = softmax(QK^T/√d_k)_ij  # 位置i對位置j的注意力權重

  可視化注意力矩陣,理解模型關注哪些詞

  範例（機器翻譯）:
  "I love you" -> "我 愛 你"
  可看到"love"主要注意"愛",權重0.9

RNN: 黑箱隱藏狀態,難以解釋
```

**4.位置無關性（靈活性）**:
```
RNN: 強制順序處理（x₁->x₂->x₃）
Transformer: 位置無關（加入位置編碼即可）

優勢:
- 可處理亂序資料
- 可雙向建模（BERT）
- 可選擇性注意（Sparse Attention）
```

**5.更好的梯度流動**:
```
RNN梯度消失:
∂L/∂h₁ = ∂L/∂h_T × ∏ₜ₌₂ᵀ ∂h_t/∂h_(t-1)
若∂h_t/∂h_(t-1) < 1,梯度指數衰減

Transformer:
Self-Attention殘差連接:
y = LayerNorm(x + Attention(x))
梯度可直接通過殘差（+x）流動,不衰減
```

**6.模型容量與擴展性**:
```
LSTM侷限:
- 隱藏狀態維度受限（通常512-2048）
- 深度受限（堆疊超過6層困難）

Transformer突破:
- BERT: 12層/24層,110M-340M參數
- GPT-3: 96層,175B參數
- 可無限堆疊（只要硬體允許）
```

**實證結果**:
| 任務 | RNN/LSTM | Transformer |
|------|----------|-------------|
| 機器翻譯（BLEU） | 28.4 | 34.1（+20%） |
| 情感分析（Acc） | 89% | 94%（+5%） |
| 訓練時間（WMT） | 3.5天（GPU） | 12小時（TPU） |

**綜合優勢**:
Transformer結合了並行化（訓練快）、長距離依賴（性能好）、可解釋性（注意力可視化）、擴展性（可訓練超大模型）,成為NLP新範式（BERT、GPT系列）。

### 6.4 易錯點提醒

**易錯點1:混淆前向傳播與反向傳播**
- **錯誤**:認為反向傳播是從輸入到輸出
- **提醒**:前向（輸入->輸出）計算預測,反向（輸出->輸入）計算梯度

**易錯點2:所有任務都用Transformer**
- **錯誤**:影像分類也用Transformer
- **提醒**:影像首選CNN（計算效率高）,文本用Transformer,序列短用LSTM

**易錯點3:忽略Batch Normalization的作用**
- **錯誤**:認為Batch Norm只是正規化
- **提醒**:加速訓練、允許更大學習率、緩解梯度消失、輕微正則化效果

**易錯點4:Dropout在推理時仍啟用**
- **錯誤**:測試時仍隨機丟棄神經元
- **提醒**:Dropout僅訓練時啟用（model.train()）,推理時關閉（model.eval()）

**易錯點5:學習率設定過大**
- **錯誤**:使用預設學習率0.1導致損失震盪
- **提醒**:Adam通常1e-3,微調預訓練模型更小（1e-5到1e-4）
