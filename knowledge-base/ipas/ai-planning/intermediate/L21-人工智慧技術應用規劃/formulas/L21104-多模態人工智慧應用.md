# L21104 - 多模態人工智慧應用

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

多模態人工智慧(Multimodal Artificial Intelligence, Multimodal AI)是指能同時處理兩種以上不同資料模態(Modality)的人工智慧系統。與傳統單一模態AI(如NLP僅處理文字、CV僅處理影像)不同,多模態AI強調跨模態訊息的理解、融合與應用,進一步擴展AI系統的感知能力與智慧水平。

常見的資料模態包括:文字(Text)、影像(Image)、語音(Audio)、影片(Video)、感測器數據(Sensor Data)等。多模態AI的目標是從這些異質資料中提取有意義的資訊,並進行融合分析,以提升決策品質與模型理解力。這種技術更貼近人類的多感官感知模式,能夠處理複雜的真實世界情境。

### 1.2 核心概念

**多模態AI的五大核心概念:**

1. **跨模態表示學習(Cross-modal Embeddings)**:將不同模態資料映射至相同語意空間,使模型能比較、關聯或互相翻譯
2. **模態融合策略(Fusion Strategy)**:包括早期融合(Early Fusion)、晚期融合(Late Fusion)、混合融合(Hybrid Fusion)
3. **跨模態注意力機制(Cross-Attention)**:讓不同模態之間能相互關注與對齊
4. **多模態預訓練**:在大規模多模態資料上進行預訓練,學習跨模態共享知識
5. **模態間對齊(Modality Alignment)**:確保不同模態在語意層面的一致性與對應關係

### 1.3 CFDS 分解

基於 Formula-Contract 方法論,將多模態AI系統分解為四個基本單元:

```
MultimodalAI = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = ModalityEncoding ∘ CrossModalAlignment ∘ FusionMechanism ∘ TaskSpecificDecoding
  = 模態編碼 -> 跨模態對齊 -> 融合機制 -> 任務特定解碼
```

**F (Files - 配置資源)**
```
F = {多模態預訓練權重, 模態映射矩陣, 融合策略配置, 同步時間戳設定, 模態權重參數}
```

**D (Data - 資料結構)**
```
D = TextData + ImageData + AudioData + VideoData + SensorData + AlignedRepresentation
  = 文字資料 + 影像資料 + 語音資料 + 影片資料 + 感測資料 + 對齊表示
```

**S (State - 運行狀態)**
```
S = UnimodalProcessing | CrossModalAlignment | MultimodalFusion | InferenceState
  = 單模態處理 | 跨模態對齊 | 多模態融合 | 推理狀態
```

### 1.4 技術定位

多模態AI在AI技術棧中處於整合創新層,結合了NLP、CV、語音處理等基礎技術,是從單一感知邁向綜合理解的關鍵技術。在企業應用中,多模態AI能實現更全面的情境感知、更精準的意圖理解、更智慧的決策支援。

隨著Transformer架構的普及與大型預訓練模型的發展,多模態AI已從簡單的特徵拼接進化到深層語意對齊,代表模型如CLIP、GPT-4V、Gemini等,展現了強大的跨模態理解與生成能力。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 多模態融合核心公式

```
MultimodalFusion = EarlyFusion | LateFusion | HybridFusion

三種融合策略:
  EarlyFusion = Concat(Modality1, Modality2, ..., ModalityN) -> SharedEncoder
              = 特徵級融合,在編碼前合併原始特徵

  LateFusion = Encoder1(Modality1) + Encoder2(Modality2) + ... -> DecisionFusion
             = 決策級融合,各模態獨立處理後合併結果

  HybridFusion = EarlyFusion(Subset1) + LateFusion(Subset2) -> AdaptiveFusion
               = 混合融合,結合前兩者優勢
```

**融合權重公式:**
```
FusedRepresentation = Σ(i=1 to N) w_i × Modality_i
                     where Σw_i = 1, w_i ≥ 0

權重學習:
  w_i = Attention(Modality_i, Context) (注意力機制動態決定)
  w_i = Softmax(MLP(Modality_i)) (神經網路學習固定權重)
```

### 2.2 CLIP 核心公式

**對比學習目標:**
```
CLIP = ImageEncoder × TextEncoder × ContrastiveLoss

目標:
  max Similarity(ImageEmbed_i, TextEmbed_i)  // 配對的圖文相似度最大化
  min Similarity(ImageEmbed_i, TextEmbed_j)  // 非配對的圖文相似度最小化 (i≠j)

對比損失(InfoNCE):
L = -log(exp(sim(I_i, T_i) / τ) / Σ_j exp(sim(I_i, T_j) / τ))
```

符號說明:
- **ImageEncoder**: 影像編碼器(如Vision Transformer)
- **TextEncoder**: 文字編碼器(如Transformer)
- **sim(I, T)**: 影像與文字嵌入的餘弦相似度
- **τ**: 溫度參數,控制分佈的平滑程度

**CLIP應用:**
```
Zero-shot Classification:
  給定影像I和類別集合{C_1, C_2, ..., C_N}
  預測 = argmax_i sim(ImageEmbed(I), TextEmbed("A photo of {C_i}"))

Image-Text Retrieval:
  以圖搜文: 找到與影像最相似的文字描述
  以文搜圖: 找到與文字最相似的影像
```

### 2.3 跨模態注意力機制

**Cross-Attention 公式:**
```
CrossAttention(Query_modality1, Key_modality2, Value_modality2)
  = softmax(Q_m1 × K_m2^T / √d_k) × V_m2

範例(影像對文字的注意力):
  Q = LinearProjection(ImageFeatures)  // 影像特徵轉為Query
  K = LinearProjection(TextFeatures)   // 文字特徵轉為Key
  V = LinearProjection(TextFeatures)   // 文字特徵轉為Value
  Output = ImageFeatures + CrossAttention(Q, K, V)  // 殘差連接
```

**雙向Cross-Attention:**
```
Image-to-Text Attention: Image features attend to Text features
Text-to-Image Attention: Text features attend to Image features

最終融合:
  FusedImage = Image + CrossAttn(Image->Text)
  FusedText = Text + CrossAttn(Text->Image)
```

### 2.4 多模態預訓練任務

**常見預訓練任務:**
```
1. Masked Language Modeling (MLM):
   遮蔽部分文字,根據其他模態預測

2. Masked Region Modeling (MRM):
   遮蔽部分影像區域,根據其他模態預測

3. Image-Text Matching (ITM):
   判斷影像與文字是否配對 (二分類)

4. Contrastive Learning:
   拉近配對樣本,推開非配對樣本

5. Multimodal Masked Autoencoders (M3AE):
   同時遮蔽多種模態,進行聯合重建
```

**M3AE 公式:**
```
L_total = L_reconstruct(Image) + L_reconstruct(Text) + L_contrastive(Image, Text)
        = 影像重建損失 + 文字重建損失 + 對比學習損失
```

### 2.5 語音-文字-影像融合

**三模態對齊公式:**
```
TrimodalAlignment = Align(Audio, Text, Image)

對齊策略:
  Audio -> Text: ASR(自動語音辨識)
  Text -> Image: CLIP對齊
  Audio -> Image: 透過Text作為橋樑

三模態共享空間:
  Embed_shared = Projection(Audio_embed + Text_embed + Image_embed)
```

**感測器多模態融合:**
```
SensorFusion = TemporalAlignment ∘ SpatialAlignment ∘ Kalman Filter

自駕車範例:
  Camera(影像) + LiDAR(點雲) + Radar(雷達) + GPS(定位) -> 融合決策

  EarlyFusion: 原始感測資料直接拼接
  LateFusion: 各感測器獨立檢測後,在物件層面融合
  DeepFusion: 深度學習自動學習最佳融合策略
```

### 2.6 多模態生成公式

**文字到影像生成(Text-to-Image):**
```
DALL·E / Stable Diffusion:
  Text -> CLIP_TextEncoder -> LatentCode -> DiffusionDecoder -> Image

Flamingo (Few-shot視覺問答):
  Image + Text Query -> VisionEncoder + LanguageModel -> Answer
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 融合策略對比表

| 融合策略 | 融合時機 | 優點 | 缺點 | 適用場景 | 計算複雜度 |
|---------|---------|------|------|---------|-----------|
| **Early Fusion** | 特徵級(編碼前) | 充分利用跨模態交互 | 對時間對齊要求高 | 資料同步良好的場景 | 高 |
| **Late Fusion** | 決策級(編碼後) | 各模態獨立處理,靈活性高 | 損失跨模態交互資訊 | 模態異步或獨立性強 | 低 |
| **Hybrid Fusion** | 多層次融合 | 兼顧兩者優勢 | 架構複雜,調參困難 | 複雜多模態任務 | 中高 |

### 3.2 多模態模型對比

| 模型 | 支援模態 | 核心技術 | 預訓練規模 | 主要能力 | 開源程度 |
|-----|---------|---------|----------|---------|---------|
| **CLIP** | 影像+文字 | 對比學習 | 4億圖文對 | 零樣本分類、圖文檢索 | 開源 |
| **GPT-4V** | 影像+文字 | Transformer | 未公開 | 視覺問答、影像理解 | 閉源API |
| **Gemini** | 影像+文字+語音+影片 | Multimodal Transformer | 未公開 | 全模態理解與生成 | 閉源API |
| **Flamingo** | 影像+文字 | Few-shot Learning | 數十億樣本 | 視覺問答、影像描述 | 部分開源 |
| **BLIP-2** | 影像+文字 | Q-Former對齊 | 1.29億圖文對 | 影像描述、VQA | 開源 |

### 3.3 單模態 vs 多模態

| 維度 | 單模態AI | 多模態AI |
|-----|---------|----------|
| **資料輸入** | 單一類型(如僅文字) | 多種類型(文字+影像+語音) |
| **感知能力** | 受限於單一模態 | 多感官綜合理解 |
| **應用靈活性** | 任務範圍受限 | 可處理複雜情境 |
| **訓練複雜度** | 較低 | 較高(需對齊多模態) |
| **推理成本** | 較低 | 較高 |
| **代表模型** | BERT(文字), ResNet(影像) | CLIP, GPT-4V, Gemini |

### 3.4 應用場景對比

| 應用場景 | 所需模態 | 融合策略 | 核心技術 | 挑戰 |
|---------|---------|---------|---------|------|
| **智慧客服** | 文字+語音 | Late Fusion | ASR + NLU | 語音辨識準確度 |
| **醫療診斷** | 影像+文字+生理訊號 | Hybrid Fusion | CNN + Transformer | 資料標註成本 |
| **自動駕駛** | Camera+LiDAR+Radar | Early Fusion | 感測融合+深度學習 | 即時性要求 |
| **虛實整合(AR/VR)** | 影像+語音+姿勢 | Hybrid Fusion | SLAM + 多模態追蹤 | 系統延遲 |

### 3.5 選擇決策樹

```
任務需求分析
├─ 資料模態數量?
│  ├─ 單一模態 -> 單模態模型(BERT/ResNet)
│  └─ 多種模態 -> 多模態模型
│     ├─ 影像+文字?
│     │  ├─ 需要零樣本能力 -> CLIP
│     │  ├─ 需要問答能力 -> BLIP-2 / GPT-4V
│     │  └─ 需要生成能力 -> DALL·E / Stable Diffusion
│     ├─ 影像+語音+文字?
│     │  └─ 全模態理解 -> Gemini / GPT-4o
│     └─ 感測器資料?
│        └─ 自駕車/機器人 -> Sensor Fusion架構
├─ 資料同步性?
│  ├─ 高度同步 -> Early Fusion
│  ├─ 獨立處理 -> Late Fusion
│  └─ 混合情況 -> Hybrid Fusion
└─ 資源限制?
   ├─ 有限 -> 輕量模型(MobileVLM)
   └─ 充足 -> 大型模型(Gemini/GPT-4V)
```

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一:智慧醫療診斷輔助

**場景描述**:醫院需要整合醫療影像(X光、CT、MRI)、病歷文字、生理訊號(心電圖、血壓)進行綜合診斷。

**技術應用方式:**
```
MedicalDiagnosis = ImageAnalysis(CNN) + TextAnalysis(BERT) + SignalProcessing(LSTM)
                   + MultimodalFusion(Hybrid) -> DiagnosisDecision

工作流程:
  影像資料 -> ResNet/ViT提取特徵
  病歷文字 -> BERT語意理解
  生理訊號 -> LSTM時序分析
  -> Cross-Attention對齊 -> Fusion Layer -> 疾病預測
```

**實現要點:**
1. **模態對齊**:確保影像、文字、訊號在時間與病例層面對齊
2. **注意力機制**:讓模型自動學習不同模態的重要性權重
3. **可解釋性**:使用Grad-CAM等技術視覺化模型決策依據
4. **少樣本學習**:應對罕見疾病資料不足問題
5. **隱私保護**:去識別化處理,符合醫療法規

### 4.2 應用場景二:智慧零售與消費者洞察

**場景描述**:零售企業整合店內攝影機影像、POS交易紀錄、顧客評論文字、社群媒體回饋,進行消費行為分析。

**技術應用方式:**
```
RetailInsight = VideoAnalysis(ObjectDetection) + TransactionData(TimeSeries)
                + TextSentiment(NLP) + SocialMedia(SentimentAnalysis)
                -> MultimodalBI Dashboard

應用實例:
  1. 客流分析: 影像偵測 + 交易資料 -> 轉換率分析
  2. 情緒分析: 顧客評論 + 社群媒體 -> 品牌聲譽監控
  3. 貨架優化: 影像追蹤 + 購買紀錄 -> 陳列建議
```

**實現要點:**
1. **即時處理**:使用邊緣運算降低延遲
2. **資料同步**:統一時間戳,確保多源資料對齊
3. **隱私合規**:匿名化處理顧客影像與個資
4. **異常檢測**:跨模態識別異常行為(如盜竊)
5. **視覺化BI**:多模態資料整合為易懂的商業洞察

### 4.3 應用場景三:自動駕駛感知系統

**場景描述**:無人車需要融合攝影機、LiDAR、Radar、GPS等多感測器資料,實現環境感知與路徑規劃。

**技術應用方式:**
```
AutonomousDriving = CameraVision + LiDARPointCloud + RadarSignal + GPSLocation
                    -> SensorFusion -> SceneUnderstanding -> PathPlanning

感測融合策略:
  Camera: 語意分割(道路、行人、車輛)
  LiDAR: 3D物件偵測與距離測量
  Radar: 速度檢測與惡劣天氣適應
  GPS: 全域定位
  -> Kalman Filter融合 -> 環境建模
```

**實現要點:**
1. **時空對齊**:不同感測器採樣率不同,需精確同步
2. **互補性利用**:
   - Camera提供豐富語意資訊但受光線影響
   - LiDAR提供精確距離但成本高
   - Radar全天候運作但解析度低
3. **深度融合**:使用深度學習自動學習最佳融合策略
4. **冗餘設計**:某一感測器故障時仍能運作
5. **即時性**:融合與決策需在毫秒級完成

### 4.4 應用場景四:虛擬助理與人機互動

**場景描述**:智慧家居助理需要理解語音指令、辨識手勢、分析環境影像,提供自然互動體驗。

**技術應用方式:**
```
VirtualAssistant = ASR(語音識別) + NLU(意圖理解) + FacialRecognition(人臉辨識)
                   + GestureDetection(手勢偵測) + ContextAwareness(情境感知)
                   -> MultimodalInteraction

互動流程:
  使用者: "請開燈" + 指向特定燈具
  系統: 語音(ASR) + 手勢(視覺) -> 融合理解 -> 執行指令
```

**實現要點:**
1. **多模態意圖理解**:語音+手勢+視線方向綜合判斷
2. **情境感知**:根據時間、地點、使用者狀態調整回應
3. **個性化**:學習使用者習慣,優化互動體驗
4. **低延遲**:邊緣運算確保即時回應
5. **隱私保護**:本地處理敏感資料,減少雲端傳輸

### 4.5 實作步驟(通用流程)

**多模態AI專案實作八步驟:**
```
Step1: 需求分析 -> 確定涉及哪些模態,融合目標
Step2: 資料收集 -> 獲取多模態標註資料
Step3: 資料對齊 -> 時間戳同步、語意對應
Step4: 模態編碼 -> 選擇各模態的編碼器
Step5: 融合策略 -> 設計Early/Late/Hybrid Fusion
Step6: 模型訓練 -> 多模態預訓練 + 任務微調
Step7: 評估優化 -> 跨模態一致性驗證
Step8: 部署監控 -> 多模態協同推理優化
```

### 4.6 常見陷阱與解決方案

**陷阱1:資料對齊不一致**
- 問題:影像30fps、語音16kHz、文字非同步,時間戳不統一
- 解決:建立統一時間軸,使用插值法對齊不同頻率資料

**陷阱2:某一模態主導其他模態**
- 問題:模型過度依賴單一模態(如只看影像不看文字)
- 解決:模態平衡訓練 + 模態Dropout + 對比學習

**陷阱3:運算資源需求過高**
- 問題:多模態模型參數量大,推理慢
- 解決:知識蒸餾 + 模態剪枝 + 邊緣運算加速

**陷阱4:標註成本高昂**
- 問題:需要同時標註多種模態,人力成本極高
- 解決:自我監督學習 + 弱監督學習 + 主動學習

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**多模態融合三策略口訣:**
```
早晚混,三種融合
早期融合特徵級
晚期融合決策級
混合融合兼顧優
```
- **早期(Early)**:特徵拼接後一起編碼
- **晚期(Late)**:各模態獨立編碼再合併
- **混合(Hybrid)**:多層次融合

**CLIP核心口訣:**
```
圖文對比學語意
配對拉近不配推
零樣本分類顯神威
```

### 5.2 記憶技巧

**Cross-Attention記憶法(互相注視):**
```
想像兩個人對話:
Query(影像): "我看到什麼?"
Key(文字): "文字提供的線索"
Value(文字): "文字的實際內容"

影像向文字發問 = Image-to-Text Attention
文字向影像發問 = Text-to-Image Attention
雙向注意力 = 更深入理解
```

**融合策略記憶法(烹飪比喻):**
```
Early Fusion = 先混合食材再一起炒(融合後處理)
Late Fusion = 各自炒好再拼盤(獨立處理後合併)
Hybrid Fusion = 部分混炒,部分拼盤(靈活組合)
```

### 5.3 快速回憶提示

**考試快速回想關鍵字:**
- 多模態AI = 跨模態理解 + 融合 + 對齊
- 三大融合 = Early + Late + Hybrid
- CLIP = 影像編碼器 + 文字編碼器 + 對比學習
- Cross-Attention = Q(模態1) × K(模態2) × V(模態2)
- 應用領域 = 醫療 + 零售 + 自駕 + AR/VR

### 5.4 易混淆辨析

**Early Fusion vs Late Fusion:**
- **Early Fusion**:特徵級融合,在編碼前合併,互動充分但對齊要求高
- **Late Fusion**:決策級融合,各模態獨立編碼後合併,靈活但損失交互
- 記憶:Early像「婚前合併財產」(緊密),Late像「婚後AA制」(獨立)

**CLIP vs GPT-4V:**
- **CLIP**:對比學習預訓練,擅長零樣本分類與圖文檢索
- **GPT-4V**:自迴歸生成模型,擅長視覺問答與影像理解
- 記憶:CLIP是「配對專家」(匹配),GPT-4V是「問答高手」(生成)

**Cross-Attention vs Self-Attention:**
- **Self-Attention**:同一模態內部注意力(如文字詞彙間)
- **Cross-Attention**:不同模態間注意力(如影像對文字)
- 記憶:Self是「自省」(內部),Cross是「對話」(跨模態)

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**題目1**:多模態人工智慧的核心目標為何?
A. 提升硬體運算效能
B. 同時處理並整合來自不同感知類型的資料
C. 降低模型參數量
D. 加快訓練速度

**答案**:B
**解析**:多模態AI的核心目標是整合文字、影像、聲音等不同模態資料,提升模型對複雜情境的理解與生成能力,更貼近人類多感官感知模式。

---

**題目2**:CLIP模型是由哪個組織提出?
A. Google
B. Meta
C. OpenAI
D. Microsoft

**答案**:C
**解析**:CLIP(Contrastive Language-Image Pre-Training)是OpenAI於2021年提出,透過大量圖文配對資料進行對比學習,使模型能理解跨模態語意關聯。

---

**題目3**:在多模態融合中,Early Fusion的主要特點是?
A. 各模態獨立處理後再合併
B. 在特徵級別就進行模態融合
C. 只在最終決策時融合
D. 不需要對齊不同模態

**答案**:B
**解析**:Early Fusion在特徵級別(編碼前)就合併不同模態資料,能充分利用跨模態交互資訊,但對資料時間對齊要求較高。

---

**題目4**:Cross-Attention機制的主要作用是?
A. 加速模型訓練
B. 讓不同模態之間能相互關注與對齊
C. 降低模型參數量
D. 提升推理速度

**答案**:B
**解析**:Cross-Attention讓一個模態的Query去關注另一個模態的Key和Value,實現跨模態的語意對齊與資訊交互,是多模態模型的核心機制。

---

**題目5**:在多模態AI應用中,以下哪一情境最能展現其優勢?
A. SQL資料查詢
B. 單純的影像分類
C. 影片自動生成字幕(涉及語音、影像、文字)
D. 數值計算

**答案**:C
**解析**:影片字幕生成涉及語音(辨識)、文字(轉錄)、影像(上下文輔助)等多模態資料處理,充分展現多模態AI的整合優勢。單純影像分類或數值計算不需要多模態能力。

### 6.2 簡答題

**題目6**:請說明自動駕駛系統中,為什麼需要融合Camera、LiDAR、Radar等多種感測器,而不是單獨使用其中一種?

**答案解析**:
- **各感測器的優缺點互補**:
  1. **Camera(攝影機)**:
     - 優點:提供豐富的顏色與語意資訊,成本低
     - 缺點:受光線影響大(夜間、逆光),無法直接測距
  2. **LiDAR(光達)**:
     - 優點:精確的3D點雲與距離測量,不受光線影響
     - 缺點:成本高,雨霧天氣性能下降
  3. **Radar(雷達)**:
     - 優點:全天候運作,能穿透雨霧,測速準確
     - 缺點:解析度低,無法辨識物體細節

- **融合的必要性**:
  - **冗餘性**:某一感測器故障時其他可補位
  - **互補性**:Camera提供語意,LiDAR提供距離,Radar提供速度
  - **魯棒性**:不同環境下組合使用,提升可靠度
  - **決策品質**:多源資訊融合降低誤判風險

---

**題目7**:什麼是CLIP的零樣本分類(Zero-shot Classification)能力?請說明其工作原理。

**答案解析**:
- **零樣本分類定義**:模型能在未見過某類別訓練樣本的情況下,正確分類該類別

- **CLIP的工作原理**:
  1. **預訓練階段**:在4億圖文對上學習圖像與文字的對應關係
  2. **推理階段**:
     - 給定影像I和類別集合{貓, 狗, 鳥}
     - 為每個類別構建文字描述:["A photo of a cat", "A photo of a dog", "A photo of a bird"]
     - 計算影像嵌入與每個文字嵌入的相似度
     - 選擇相似度最高的類別作為預測

- **為何有效**:
  - CLIP學習的是通用的圖文對應關係,而非特定類別
  - 透過自然語言描述,可將任何新類別映射到語意空間
  - 不需要針對新類別重新訓練模型

- **限制**:
  - 依賴文字描述的品質
  - 細粒度分類(如狗的品種)效果較差

### 6.3 易錯點提醒

**易錯點1**:混淆Early Fusion與Late Fusion的應用時機
- **正確理解**:Early Fusion適合資料高度同步的情境(如同步錄製的影音),Late Fusion適合各模態相對獨立的情境(如醫療影像+病歷文字)
- **記憶提示**:同步用Early(緊密融合),異步用Late(獨立處理)

**易錯點2**:認為多模態模型一定比單模態好
- **正確理解**:多模態模型訓練複雜、資源消耗大,若任務只需單一模態就能解決,使用多模態反而過度設計
- **記憶提示**:奧卡姆剃刀原則 - 簡單有效即可

**易錯點3**:忽略模態間的對齊問題
- **正確理解**:不同模態的採樣率、時間戳、語意粒度可能不同,需精確對齊才能有效融合
- **記憶提示**:對齊是多模態的「地基」,不對齊則融合無效

**易錯點4**:以為Cross-Attention就是Self-Attention
- **正確理解**:Self-Attention是同一序列內部的注意力,Cross-Attention是不同序列(模態)之間的注意力
- **記憶提示**:Self是「自我對話」,Cross是「跨界交流」
