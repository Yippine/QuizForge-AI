# L21103 - 生成式AI技術與應用

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

生成式人工智慧(Generative AI)是一類能夠主動創造全新內容的AI技術,包括文字、圖像、音訊、影片等多模態資料。與傳統的鑑別式AI(Discriminative AI)專注於分類與預測不同,生成式AI強調「創造性」,能夠學習資料的分佈特性與潛在結構,進而「模擬」或「擴張」該分佈來產出新的樣本。

自2022年OpenAI推出ChatGPT以來,生成式AI不僅引發學術界與產業界的廣泛關注,也在內容創作、商業應用與技術研發方面展現極高潛力。生成式AI不再僅是資料分析工具,而是具備主動創造價值的能力,標誌著AI發展進入創造性階段。

### 1.2 核心概念

**生成式AI的五大核心概念:**

1. **對抗生成網路(GAN)**: 透過生成器與判別器的對抗訓練,從隨機噪聲生成高品質資料
2. **變分自編碼器(VAE)**: 利用變分推斷學習資料潛在分佈,實現資料生成與重建
3. **擴散模型(Diffusion Models)**: 透過逐步去噪過程,從隨機噪聲生成細節豐富的內容
4. **自迴歸模型(Autoregressive Models)**: 基於前文預測後續內容,如GPT系列
5. **提示工程(Prompt Engineering)**: 透過精心設計的輸入指令,控制生成內容的品質與方向

### 1.3 CFDS 分解

基於 Formula-Contract 方法論,將生成式AI系統分解為四個基本單元:

```
GenerativeAI = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = InputEncoding ∘ LatentSpaceMapping ∘ GenerativeDecoding ∘ OutputRefinement
  = 輸入編碼 -> 潛在空間映射 -> 生成解碼 -> 輸出優化
```

**F (Files - 配置資源)**
```
F = {預訓練權重, 提示範本, 生成參數配置, 風格控制檔案, 安全過濾器}
```

**D (Data - 資料結構)**
```
D = TrainingCorpus + PromptInput + LatentRepresentation + GeneratedOutput
  = 訓練語料 + 提示輸入 + 潛在表示 + 生成輸出
```

**S (State - 運行狀態)**
```
S = PretrainingState | FineTuningState | InferenceState | RLHFState
  = 預訓練狀態 | 微調狀態 | 推理狀態 | 人類反饋強化學習狀態
```

### 1.4 技術定位

生成式AI在AI技術棧中處於應用創新層,整合了NLP、CV、深度學習等基礎技術,是從「理解」邁向「創造」的關鍵技術。在企業應用中,生成式AI能自動化內容生產、個性化推薦、設計輔助、程式碼生成等,大幅提升創作效率與業務價值。

生成式AI的技術基礎包括大規模預訓練模型、深度神經網路、跨模態學習、強化學習與提示工程等,並與自然語言處理(NLP)、電腦視覺(CV)、語音處理等領域緊密結合。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 生成式AI核心分類公式

```
GenerativeAI = GANBased | VAEBased | DiffusionBased | AutoregressiveBased

四大技術路徑:
  GANBased = Generator × Discriminator (對抗訓練)
  VAEBased = Encoder ∘ LatentSpace ∘ Decoder (變分推斷)
  DiffusionBased = NoiseAddition ∘ DenoisingProcess (擴散去噪)
  AutoregressiveBased = P(x) = ∏(t=1 to T) P(x_t | x_{<t}) (序列生成)
```

### 2.2 GAN 核心公式

**對抗訓練目標函數:**
```
min_G max_D V(D,G) = E_{x~p_data}[log D(x)] + E_{z~p_z}[log(1 - D(G(z)))]
```

符號說明:
- **G (Generator)**: 生成器,從噪聲 z 生成假資料 G(z)
- **D (Discriminator)**: 判別器,判斷輸入是真實資料(1)或生成資料(0)
- **p_data**: 真實資料分佈
- **p_z**: 噪聲分佈(通常為高斯分佈)
- **目標**: G試圖最小化,D試圖最大化

**訓練流程:**
```
Step1: 固定G,訓練D -> max E[log D(x)] + E[log(1-D(G(z)))]
Step2: 固定D,訓練G -> min E[log(1-D(G(z)))]
Step3: 交替迭代直到收斂
```

### 2.3 VAE 核心公式

**變分自編碼器目標:**
```
VAE = Encoder(x -> z) ∘ Decoder(z -> x')

損失函數:
L(θ,φ) = Reconstruction_Loss + KL_Divergence
       = -E_{z~q}[log p(x|z)] + KL(q(z|x) || p(z))
```

符號說明:
- **Encoder (q(z|x))**: 將輸入x編碼為潛在變數z的機率分佈
- **Decoder (p(x|z))**: 從潛在變數z重建輸出x'
- **Reconstruction Loss**: 重建誤差,衡量輸出與原始輸入的差異
- **KL Divergence**: 正則化項,使潛在分佈接近標準常態分佈

### 2.4 Diffusion Models 核心公式

**擴散模型流程:**
```
Forward Process (加噪): x_0 -> x_1 -> x_2 -> ... -> x_T (純噪聲)
  q(x_t | x_{t-1}) = N(x_t; √(1-β_t)x_{t-1}, β_t I)

Reverse Process (去噪): x_T -> x_{T-1} -> ... -> x_1 -> x_0 (生成資料)
  p_θ(x_{t-1} | x_t) = N(x_{t-1}; μ_θ(x_t,t), Σ_θ(x_t,t))
```

**訓練目標:**
```
L = E_{t,x_0,ε}[||ε - ε_θ(x_t, t)||²]
  = 預測噪聲 vs 真實噪聲的均方誤差
```

**代表模型:**
- **DDPM (Denoising Diffusion Probabilistic Models)**: 基礎擴散模型
- **Stable Diffusion**: 在潛在空間進行擴散,提升效率
- **DALL·E 2**: 文字到圖像的擴散模型

### 2.5 Transformer 在生成式AI的應用

**GPT (Generative Pre-trained Transformer):**
```
GPT = UnidirectionalTransformerDecoder
P(x_1, x_2, ..., x_n) = ∏(i=1 to n) P(x_i | x_1, ..., x_{i-1})

生成流程:
  Input Prompt -> TokenEmbedding -> TransformerLayers^N -> SoftmaxPrediction -> Output
```

**T5 (Text-to-Text Transfer Transformer):**
```
T5 = Encoder-Decoder架構
任何任務 = Text -> T5 -> Text
  翻譯: "translate English to French: Hello" -> "Bonjour"
  摘要: "summarize: [long text]" -> "summary"
```

### 2.6 提示工程公式

**提示結構化公式:**
```
EffectivePrompt = Context + Instruction + Examples + Constraints
                = 背景資訊 + 明確指令 + 範例示範 + 限制條件

Few-Shot Learning:
  Prompt = Example1 + Example2 + ... + ExampleN + Query

Chain-of-Thought (CoT):
  Prompt = "Let's think step by step: " + Problem
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 生成式模型技術對比表

| 技術類型 | 代表模型 | 優點 | 缺點 | 適用場景 | 訓練難度 |
|---------|---------|------|------|---------|---------|
| **GAN** | StyleGAN, CycleGAN | 生成品質高、細節豐富 | 訓練不穩定、模式崩潰 | 圖像生成、風格轉換 | 高 |
| **VAE** | β-VAE, VQ-VAE | 訓練穩定、潛在空間可解釋 | 生成品質較模糊 | 資料壓縮、異常檢測 | 中 |
| **Diffusion** | Stable Diffusion, DALL·E 2 | 生成品質極高、穩定性好 | 生成速度慢、資源消耗大 | 高品質圖像生成 | 中高 |
| **Autoregressive** | GPT-4, Claude | 語言生成能力強、邏輯連貫 | 生成速度慢(逐字生成) | 文字生成、對話系統 | 高 |

### 3.2 文字生成模型對比

| 模型 | 架構 | 參數量 | 上下文長度 | 特色能力 | 適用任務 |
|-----|------|--------|----------|---------|---------|
| **GPT-4** | Transformer Decoder | 未公開(估計>1T) | 128K tokens | 多模態、工具調用 | 對話、程式碼生成、多模態理解 |
| **Claude** | Transformer | 未公開 | 200K tokens | 長文本處理、安全性 | 長文檔分析、專業寫作 |
| **Gemini** | Multimodal Transformer | 未公開 | 1M tokens(Gemini 1.5) | 超長上下文、多模態 | 影片理解、大量文檔分析 |
| **LLaMA** | Transformer | 7B~70B | 2K~4K tokens | 開源、可本地部署 | 研究、客製化應用 |

### 3.3 圖像生成模型對比

| 模型 | 技術基礎 | 輸入方式 | 輸出品質 | 生成速度 | 可控性 |
|-----|---------|---------|---------|---------|--------|
| **Stable Diffusion** | Latent Diffusion | 文字提示 + 控制條件 | 極高 | 中(10-50步) | 高(ControlNet) |
| **DALL·E 2** | CLIP + Diffusion | 文字提示 | 極高 | 中 | 中 |
| **Midjourney** | Diffusion(優化) | 文字提示 | 極高(藝術風格) | 中 | 中 |
| **StyleGAN** | GAN | 潛在向量 | 高 | 快 | 高(風格混合) |

### 3.4 生成式 vs 鑑別式 AI

| 維度 | 生成式AI | 鑑別式AI |
|-----|----------|----------|
| **核心目標** | 創造新內容 | 分類與預測 |
| **學習對象** | 資料分佈 P(x) 或 P(x,y) | 條件機率 P(y\|x) |
| **代表技術** | GAN, VAE, Diffusion, GPT | CNN, SVM, 決策樹, BERT(分類) |
| **輸出形式** | 新資料樣本 | 類別標籤或數值 |
| **典型應用** | 內容生成、創作輔助 | 影像分類、語音辨識 |

### 3.5 選擇決策樹

```
需求分析
├─ 生成文字內容?
│  ├─ 需要對話能力 -> GPT-4 / Claude
│  ├─ 需要超長上下文 -> Gemini / Claude
│  └─ 需要本地部署 -> LLaMA / Mistral
├─ 生成圖像內容?
│  ├─ 需要高度控制 -> Stable Diffusion + ControlNet
│  ├─ 追求藝術風格 -> Midjourney
│  └─ 快速原型設計 -> DALL·E 2
├─ 生成語音內容?
│  ├─ 情感表達 -> VITS / Bark
│  └─ 多語言支援 -> Google TTS / Azure TTS
└─ 資源有限?
   ├─ 是 -> VAE / 小型GAN
   └─ 否 -> Diffusion / 大型LLM
```

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一:智慧內容創作平台

**場景描述**:媒體公司需要快速產生大量文章、社群貼文、行銷文案,並配合視覺素材。

**技術應用方式:**
```
ContentGeneration = TextGen(GPT) + ImageGen(StableDiffusion) + QualityControl
                  = 文字生成 + 圖像生成 + 品質控制

工作流程:
  使用者輸入 -> 提示工程優化 -> GPT生成文案 -> 關鍵詞提取
  -> Stable Diffusion生成配圖 -> 人工審核 -> 發佈
```

**實現要點:**
1. **提示範本庫建立**:針對不同內容類型(新聞、行銷、教學)建立提示範本
2. **Few-Shot Learning**:提供3-5個範例引導生成風格
3. **內容安全過濾**:使用OpenAI Moderation API檢測不當內容
4. **多輪迭代優化**:透過Chain-of-Thought引導模型自我改進
5. **版權風險控管**:記錄生成來源,避免抄襲爭議

### 4.2 應用場景二:個性化商品設計輔助

**場景描述**:電商平台需要為不同客群生成個性化商品圖案、包裝設計。

**技術應用方式:**
```
DesignAssistant = UserPreference ∘ StyleTransfer ∘ ImageGeneration ∘ Refinement
                = 偏好分析 -> 風格遷移 -> 圖像生成 -> 精修

Stable Diffusion 應用:
  "A product package design, [user_style], [color_scheme], professional, high quality"
  + ControlNet(邊緣控制) + LoRA(風格微調)
```

**實現要點:**
1. **風格向量化**:將用戶歷史偏好編碼為風格向量
2. **ControlNet 精確控制**:保持商品主體結構,僅調整風格
3. **LoRA 客製化訓練**:用少量品牌素材微調模型
4. **批次生成與篩選**:一次生成10-20個變體,供設計師挑選
5. **A/B測試整合**:自動追蹤不同設計的轉換率

### 4.3 應用場景三:程式碼生成與開發輔助

**場景描述**:軟體開發團隊需要自動生成程式碼、單元測試、文件註解。

**技術應用方式:**
```
CodeAssistant = NaturalLanguageSpec -> GPT4CodeGen -> SyntaxCheck -> UnitTestGen
              = 自然語言規格 -> GPT4程式碼生成 -> 語法檢查 -> 單元測試生成

實作模式:
  Copilot模式: 根據註解自動補全程式碼
  ChatDev模式: 多Agent協作(PM + Coder + Tester + Reviewer)
```

**實現要點:**
1. **Few-Shot + Chain-of-Thought**:提供範例程式碼,引導生成邏輯
2. **語法驗證**:整合Linter自動檢查生成程式碼
3. **測試驅動開發(TDD)**:先生成測試案例,再生成實作
4. **版本控制整合**:自動產生commit message與PR描述
5. **安全性掃描**:檢測潛在的SQL Injection、XSS等漏洞

### 4.4 實作步驟(通用流程)

**生成式AI專案實作八步驟:**
```
Step1: 需求分析 -> 確定生成內容類型(文字/圖像/語音/程式碼)
Step2: 模型選擇 -> 根據品質/速度/成本選擇模型
Step3: 提示工程 -> 設計有效提示範本
Step4: 資料準備 -> 準備Few-Shot範例或微調資料
Step5: 安全過濾 -> 建立內容審核機制
Step6: 生成測試 -> 小規模測試生成品質
Step7: 人機協作 -> 設計人工審核與改進流程
Step8: 監控優化 -> 追蹤生成效果,持續改進提示
```

### 4.5 常見陷阱與解決方案

**陷阱1:生成內容幻覺(Hallucination)**
- 問題:模型生成不實資訊或虛構引用
- 解決:RAG(檢索增強生成) + 事實查核 + 明確標示「AI生成內容」

**陷阱2:提示注入攻擊(Prompt Injection)**
- 問題:惡意使用者透過特殊提示繞過安全限制
- 解決:輸入驗證 + 提示隔離 + 輸出過濾

**陷阱3:生成內容同質化**
- 問題:多次生成結果過於相似,缺乏多樣性
- 解決:調整Temperature參數(0.7-1.0) + Top-p採樣 + 引入隨機性

**陷阱4:版權與授權爭議**
- 問題:訓練資料可能包含受版權保護內容
- 解決:使用有授權的訓練資料 + 生成內容原創性檢測 + 法律諮詢

**陷阱5:計算成本過高**
- 問題:大型模型推理費用高昂
- 解決:模型蒸餾(Distillation) + 快取常見查詢 + 批次處理

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**生成式AI四大技術口訣:**
```
敵變散迴,四路生成
敵對訓練GAN領先
變分編碼VAE穩陣
擴散去噪品質精進
自迴歸式GPT縱橫
```
- **敵**:GAN(對抗生成網路)
- **變**:VAE(變分自編碼器)
- **散**:Diffusion(擴散模型)
- **迴**:Autoregressive(自迴歸模型)

**GAN對抗訓練口訣:**
```
真假對抗,生判博弈
生成器造假欺騙
判別器明察秋毫
互相較量共同進步
最終生成以假亂真
```

### 5.2 記憶技巧

**Diffusion擴散模型記憶法(加噪去噪):**
```
想像照片褪色過程:
加噪階段 = 照片逐漸褪色(加入噪聲)
  x_0(清晰) -> x_1 -> x_2 -> ... -> x_T(全白噪聲)

去噪階段 = 修復褪色照片(去除噪聲)
  x_T(噪聲) -> x_{T-1} -> ... -> x_1 -> x_0(清晰)

模型學習的是「如何從噪聲中恢復清晰圖像」
```

**提示工程記憶法(CIEC結構):**
```
Context(背景) + Instruction(指令) + Examples(範例) + Constraints(限制)

想像給助理交代任務:
Context: "你是專業文案編輯"
Instruction: "撰寫產品介紹"
Examples: "範例1:..., 範例2:..."
Constraints: "字數200字內,語氣專業"
```

### 5.3 快速回憶提示

**考試快速回想關鍵字:**
- 生成式AI = GAN + VAE + Diffusion + Autoregressive
- GAN = Generator vs Discriminator(對抗)
- VAE = Encoder -> LatentSpace -> Decoder
- Diffusion = 加噪(Forward) + 去噪(Reverse)
- GPT = 自迴歸 + Transformer Decoder
- 提示工程 = CIEC結構

### 5.4 易混淆辨析

**GAN vs VAE:**
- **GAN**:對抗訓練,生成品質高但訓練不穩定
- **VAE**:變分推斷,訓練穩定但生成較模糊
- 記憶:GAN像激烈競賽(不穩定但精彩),VAE像按部就班(穩定但平淡)

**Diffusion vs Autoregressive:**
- **Diffusion**:並行去噪,生成速度中等,適合圖像
- **Autoregressive**:逐字生成,生成速度慢,適合文字
- 記憶:Diffusion像修復照片(一次處理全圖),Autoregressive像打字(逐字輸出)

**生成式 vs 鑑別式:**
- **生成式(Generative)**:學習P(x)或P(x,y),創造新資料
- **鑑別式(Discriminative)**:學習P(y|x),分類與預測
- 記憶:生成式是「畫家」(創作),鑑別式是「評審」(判斷)

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**題目1**:GAN(生成對抗網路)的兩個核心組件是什麼?
A. 編碼器與解碼器
B. 生成器與判別器
C. 分類器與分群器
D. 編碼器與判別器

**答案**:B
**解析**:GAN由生成器(Generator)負責生成資料,判別器(Discriminator)判斷資料真偽,透過對抗訓練提升生成品質。

---

**題目2**:Diffusion Models的核心訓練目標是什麼?
A. 最大化分類準確率
B. 最小化重建誤差
C. 預測並去除噪聲
D. 最大化對抗損失

**答案**:C
**解析**:Diffusion Models透過學習預測並去除每一步的噪聲,從純噪聲逐步恢復清晰資料。訓練目標是最小化預測噪聲與真實噪聲的差異。

---

**題目3**:下列哪種技術最適合生成高品質的藝術風格圖像?
A. 決策樹
B. Stable Diffusion
C. K-means分群
D. 線性迴歸

**答案**:B
**解析**:Stable Diffusion是基於擴散模型的圖像生成技術,能根據文字提示生成高品質、細節豐富的藝術風格圖像。決策樹、K-means、線性迴歸都是鑑別式或分群演算法,不具備生成能力。

---

**題目4**:提示工程(Prompt Engineering)的CIEC結構中,「E」代表什麼?
A. Encoder(編碼器)
B. Examples(範例)
C. Evaluation(評估)
D. Embedding(嵌入)

**答案**:B
**解析**:CIEC結構為Context(背景) + Instruction(指令) + Examples(範例) + Constraints(限制)。提供範例能有效引導模型生成符合預期的內容。

---

**題目5**:GPT模型屬於哪種生成式技術?
A. GAN
B. VAE
C. Autoregressive(自迴歸)
D. Diffusion

**答案**:C
**解析**:GPT基於自迴歸(Autoregressive)模型,透過預測下一個token逐步生成文字序列,機率公式為P(x) = ∏P(x_t | x_{<t})。

### 6.2 簡答題

**題目6**:請說明GAN訓練過程中的「模式崩潰(Mode Collapse)」問題,並提出可能的解決方案。

**答案解析**:
- **模式崩潰定義**:生成器發現某些特定樣本能成功欺騙判別器,於是只生成這些樣本,導致生成多樣性喪失
- **症狀**:無論輸入什麼噪聲,生成器都產生相似的輸出
- **解決方案**:
  1. **Minibatch Discrimination**:讓判別器同時檢視多個樣本,懲罰相似度過高的生成
  2. **Unrolled GAN**:生成器考慮判別器未來幾步的反應
  3. **Wasserstein GAN(WGAN)**:改進損失函數,提供更穩定的梯度
  4. **增加訓練資料多樣性**:豐富訓練集,避免模型過度適應單一模式

---

**題目7**:為什麼Diffusion Models能生成高品質圖像,但生成速度相對較慢?請從技術原理說明。

**答案解析**:
- **高品質原因**:
  1. **逐步去噪過程**:透過多步(通常50-1000步)逐漸去除噪聲,每步只需預測微小變化,降低任務難度
  2. **穩定訓練**:不像GAN需要對抗訓練,Diffusion訓練過程穩定,不易崩潰
  3. **理論基礎紮實**:基於非平衡熱力學與隨機微分方程,數學原理清晰

- **速度慢原因**:
  1. **多步推理**:生成一張圖需要50-1000次神經網路前向傳播
  2. **無法並行**:每步依賴前一步結果,無法完全並行化

- **加速方法**:
  - DDIM(Denoising Diffusion Implicit Models):減少採樣步數至10-50步
  - Latent Diffusion(Stable Diffusion):在壓縮的潛在空間進行擴散,降低計算量

### 6.3 易錯點提醒

**易錯點1**:混淆GAN的訓練目標
- **錯誤理解**:生成器和判別器同時最小化損失
- **正確理解**:生成器最小化log(1-D(G(z))),判別器最大化log D(x) + log(1-D(G(z))),是min-max對抗
- **記憶提示**:GAN是「對抗」訓練,兩者目標相反

**易錯點2**:認為VAE生成的圖像一定比GAN模糊
- **正確理解**:傳統VAE確實較模糊,但現代變體(如VQ-VAE、VQGAN)結合了量化技術,生成品質可媲美GAN
- **記憶提示**:技術在進步,不可一概而論

**易錯點3**:以為提示越長越好
- **正確理解**:提示應精簡明確,冗長提示反而可能稀釋關鍵資訊,導致模型理解偏差
- **記憶提示**:「簡潔有力」勝過「冗長複雜」

**易錯點4**:忽略生成式AI的版權風險
- **正確理解**:生成內容可能與訓練資料相似,存在版權爭議風險,需明確標示AI生成並進行原創性檢測
- **記憶提示**:生成≠原創,需謹慎處理法律問題
