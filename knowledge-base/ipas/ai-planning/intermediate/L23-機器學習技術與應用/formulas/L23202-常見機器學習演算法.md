# L23202 - 常見機器學習演算法

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

常見機器學習演算法是指在實務應用中最廣泛使用的經典模型，涵蓋迴歸演算法（線性迴歸、Ridge、Lasso）、樹狀模型（決策樹、隨機森林、GBDT、XGBoost、LightGBM）、分類演算法（邏輯迴歸、SVM、KNN、樸素貝氏）、集成學習（Bagging、Boosting、Stacking）等。這些演算法各有特色，適用於不同的資料特性與應用場景。

機器學習演算法的選擇需考量：資料規模（小數據 vs 大數據）、資料特性（線性 vs 非線性、高維 vs 低維）、問題類型（分類 vs 迴歸）、可解釋性需求、訓練效率、預測準確率等多個維度。理解每種演算法的核心原理與數學公式，能幫助我們選擇最合適的模型並進行有效調參。

### 1.2 核心概念

**常見機器學習演算法的八大核心概念：**

1. **迴歸演算法**：預測連續數值（房價、股價、銷量），核心是最小化預測誤差
2. **分類演算法**：預測離散類別（垃圾郵件、疾病診斷、影像辨識），核心是決策邊界
3. **樹狀模型**：基於規則的分割決策（決策樹、隨機森林、梯度提升樹），可解釋性強
4. **集成學習**：組合多個弱模型提升預測能力（Bagging、Boosting、Stacking）
5. **支援向量機（SVM）**：尋找最大間隔的分類超平面，適合中小數據
6. **K近鄰（KNN）**：基於鄰近樣本投票，簡單直觀但計算成本高
7. **樸素貝氏**：基於條件獨立假設的機率分類器，訓練快速
8. **正則化技術**：L1/L2 懲罰過大參數，防止過擬合

### 1.3 CFDS 分解

基於 Formula-Contract 方法論，將機器學習演算法分解為四個基本單元：

```
MachineLearningAlgorithms = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = AlgorithmCore ∘ LossFunction ∘ OptimizationMethod ∘ PredictionLogic
  = 演算法核心 -> 損失函數 -> 優化方法 -> 預測邏輯

例如：
LinearRegression = LeastSquares ∘ GradientDescent ∘ LinearPrediction
XGBoost = GradientBoosting ∘ SecondOrderOptimization ∘ TreeEnsemble
```

**F (Files - 配置資源)**
```
F = {ModelWeights, Hyperparameters, TreeStructure, SupportVectors}
  = {模型權重, 超參數, 樹結構, 支援向量}

例如：
RandomForest_Files = {n_estimators, max_depth, min_samples_split, bootstrap}
XGBoost_Files = {learning_rate, max_depth, subsample, colsample_bytree, lambda}
```

**D (Data - 資料結構)**
```
D = FeatureMatrix + TargetVector + TrainingSet + TestSet
  = 特徵矩陣 + 目標向量 + 訓練集 + 測試集

例如：
Regression_Data = X(n_samples × n_features) + y(n_samples)
Classification_Data = X + y_categorical + class_weights
```

**S (State - 運行狀態)**
```
S = Training | Trained | Predicting | Overfitted | Converged
  = 訓練中 | 已訓練 | 預測中 | 過擬合 | 已收斂

決策樹狀態: Splitting | Pruning | LeafNode
集成學習狀態: Building_Estimators | Aggregating | Voting
```

### 1.4 技術定位

常見機器學習演算法在技術棧中處於核心實現層，連接數學基礎（線性代數、最優化）與實務應用（預測系統、推薦引擎）。在企業應用中，演算法選擇直接影響模型性能：

- **表格資料**：XGBoost/LightGBM 是首選（Kaggle 競賽常勝軍）
- **高維稀疏資料**：邏輯迴歸 + L1 正則化（文本分類、廣告點擊預測）
- **小數據高可解釋性**：決策樹（醫療診斷、信貸審批）
- **大數據快速訓練**：LightGBM（支援分散式訓練）
- **非線性複雜關係**：隨機森林、梯度提升樹
- **線性可分資料**：SVM（支援向量少時效率高）

理解每種演算法的適用場景與限制，是成為優秀機器學習工程師的關鍵能力。

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 線性迴歸與正則化

**線性迴歸（Linear Regression）**：
```
ŷ = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ = w^T x + b

損失函數（MSE）:
L(w) = (1/N) Σᵢ₌₁ᴺ (yᵢ - ŷᵢ)²

最佳解（閉式解）:
w* = (X^T X)⁻¹ X^T y
```

**Ridge 迴歸（L2 正則化）**：
```
L_Ridge(w) = MSE(w) + λ Σⱼ wⱼ²

最佳解:
w* = (X^T X + λI)⁻¹ X^T y
```
- **λ**：正則化強度（越大越平滑）
- **效果**：防止參數過大、改善多重共線性

**Lasso 迴歸（L1 正則化）**：
```
L_Lasso(w) = MSE(w) + λ Σⱼ |wⱼ|
```
- **效果**：產生稀疏解（部分 wⱼ = 0）、自動特徵選擇
- **應用**：高維資料特徵篩選

### 2.2 邏輯迴歸

**邏輯迴歸（Logistic Regression）**：
```
二元分類:
P(y=1|x) = σ(w^T x + b) = 1 / (1 + e^(-(w^T x + b)))

損失函數（交叉熵）:
L(w) = -(1/N) Σᵢ [yᵢ log(ŷᵢ) + (1-yᵢ) log(1-ŷᵢ)]

梯度:
∂L/∂w = (1/N) X^T (ŷ - y)

多元分類（Softmax）:
P(y=k|x) = exp(w_k^T x) / Σⱼ exp(w_j^T x)
```
- **σ(z)**：Sigmoid 函數，將實數映射到 [0, 1]
- **應用**：二元分類（垃圾郵件、欺詐檢測）、多元分類

### 2.3 決策樹

**決策樹分裂準則**：

**分類樹 - Gini 不純度**：
```
Gini(D) = 1 - Σₖ pₖ²

其中 pₖ = 類別 k 的樣本比例

分裂增益:
Gain = Gini(D_parent) - [|D_left|/|D| × Gini(D_left) + |D_right|/|D| × Gini(D_right)]
```

**分類樹 - 資訊增益（Information Gain）**：
```
Entropy(D) = -Σₖ pₖ log₂(pₖ)

Information_Gain = Entropy(D_parent) - Σ |D_child|/|D| × Entropy(D_child)
```

**迴歸樹 - 均方誤差**：
```
MSE(D) = (1/|D|) Σᵢ (yᵢ - ȳ)²

其中 ȳ = 區域內樣本的平均值

預測值:
ŷ = mean(y_samples_in_leaf)
```

### 2.4 隨機森林

**隨機森林（Random Forest）**：
```
RandomForest = Bagging + RandomFeatureSelection

建構過程:
1. Bootstrap 採樣: 從 N 個樣本中有放回抽取 N 個樣本（約 63% 獨特樣本）
2. 隨機特徵: 每次分裂時隨機選擇 √n 個特徵（n = 總特徵數）
3. 訓練多棵樹: 訓練 T 棵決策樹（通常 T = 100-500）
4. 聚合預測:
   - 分類: ŷ = mode(tree₁(x), tree₂(x), ..., treeₜ(x))  # 多數投票
   - 迴歸: ŷ = (1/T) Σₜ treeₜ(x)  # 平均

Out-of-Bag (OOB) 誤差:
OOB_Error = (1/N) Σᵢ I(ŷᵢ_oob ≠ yᵢ)
其中 ŷᵢ_oob 只用未包含樣本 i 的樹預測
```

### 2.5 梯度提升樹（Gradient Boosting）

**GBDT 核心公式**：
```
梯度提升框架:
F_m(x) = F_{m-1}(x) + ν × h_m(x)

其中:
- F_m(x): 第 m 輪的模型
- h_m(x): 第 m 棵樹
- ν: 學習率（縮減因子，通常 0.01-0.3）

訓練流程:
1. 初始化: F₀(x) = argmin_γ Σᵢ L(yᵢ, γ)  # 常數預測
2. For m = 1 to M:
   a. 計算負梯度（殘差）:
      rᵢₘ = -[∂L(yᵢ, F(xᵢ))/∂F(xᵢ)]_{F=F_{m-1}}
   b. 訓練樹 h_m(x) 擬合 {(xᵢ, rᵢₘ)}
   c. 更新模型:
      F_m(x) = F_{m-1}(x) + ν × h_m(x)

3. 最終模型: F_M(x) = F₀(x) + ν × Σₘ h_m(x)

常見損失函數:
- 迴歸: L(y, F) = (y - F)² / 2  →  負梯度 = y - F（殘差）
- 分類: L(y, F) = log(1 + exp(-2yF))  →  負梯度 = 2y / (1 + exp(2yF))
```

**XGBoost 改進**：
```
XGBoost 目標函數:
Obj = Σᵢ L(yᵢ, ŷᵢ) + Σₖ Ω(fₖ)

正則化項:
Ω(f) = γT + (λ/2) Σⱼ wⱼ²

其中:
- T: 葉節點數量
- wⱼ: 葉節點 j 的權重
- γ: 葉節點懲罰係數
- λ: L2 正則化係數

二階泰勒展開:
Obj ≈ Σᵢ [L(yᵢ, ŷ^(t-1)) + gᵢfₜ(xᵢ) + (1/2)hᵢfₜ²(xᵢ)] + Ω(fₜ)

其中:
- gᵢ = ∂L/∂ŷ^(t-1)  # 一階梯度
- hᵢ = ∂²L/∂(ŷ^(t-1))²  # 二階梯度

最佳葉權重:
w*ⱼ = -Σᵢ∈Iⱼ gᵢ / (Σᵢ∈Iⱼ hᵢ + λ)

分裂增益:
Gain = (Σᵢ∈I_L gᵢ)² / (Σᵢ∈I_L hᵢ + λ) + (Σᵢ∈I_R gᵢ)² / (Σᵢ∈I_R hᵢ + λ) - (Σᵢ∈I gᵢ)² / (Σᵢ∈I hᵢ + λ) - γ
```

### 2.6 支援向量機（SVM）

**線性 SVM**：
```
目標: 最大化間隔 margin = 2/||w||

優化問題:
min (1/2)||w||²
s.t. yᵢ(w^T xᵢ + b) ≥ 1, ∀i

拉格朗日對偶形式:
max Σᵢ αᵢ - (1/2) Σᵢ Σⱼ αᵢαⱼyᵢyⱼ(xᵢ^T xⱼ)
s.t. αᵢ ≥ 0, Σᵢ αᵢyᵢ = 0

決策函數:
f(x) = sign(Σᵢ αᵢyᵢ(xᵢ^T x) + b)
     = sign(Σᵢ∈SV αᵢyᵢ(xᵢ^T x) + b)  # 僅支援向量 αᵢ > 0
```

**核技巧（Kernel SVM）**：
```
K(xᵢ, xⱼ) = φ(xᵢ)^T φ(xⱼ)

常見核函數:
- 線性核: K(x, x') = x^T x'
- 多項式核: K(x, x') = (x^T x' + c)^d
- RBF（高斯）核: K(x, x') = exp(-γ||x - x'||²)
- Sigmoid 核: K(x, x') = tanh(κx^T x' + c)

決策函數:
f(x) = sign(Σᵢ∈SV αᵢyᵢK(xᵢ, x) + b)
```

### 2.7 K近鄰（KNN）

**KNN 預測公式**：
```
分類:
ŷ = mode({yᵢ | xᵢ ∈ N_k(x)})  # k 個最近鄰的多數類別

迴歸:
ŷ = (1/k) Σᵢ∈N_k(x) yᵢ  # k 個最近鄰的平均值

距離度量:
- 歐氏距離: d(x, x') = √(Σⱼ (xⱼ - x'ⱼ)²)
- 曼哈頓距離: d(x, x') = Σⱼ |xⱼ - x'ⱼ|
- 閔可夫斯基距離: d(x, x') = (Σⱼ |xⱼ - x'ⱼ|^p)^(1/p)

加權 KNN（距離加權）:
ŷ = Σᵢ∈N_k(x) wᵢyᵢ / Σᵢ∈N_k(x) wᵢ
其中 wᵢ = 1 / d(x, xᵢ)²
```

### 2.8 樸素貝氏

**樸素貝氏分類器**：
```
貝氏定理:
P(C|X) = P(X|C)P(C) / P(X)

樸素假設（條件獨立）:
P(X|C) = P(x₁, x₂, ..., xₙ|C) = Πⱼ P(xⱼ|C)

分類決策:
ŷ = argmax_C P(C|X) = argmax_C P(C) × Πⱼ P(xⱼ|C)

高斯樸素貝氏（連續特徵）:
P(xⱼ|C) = (1/√(2πσ²_Cⱼ)) exp(-(xⱼ - μ_Cⱼ)² / (2σ²_Cⱼ))

其中:
- μ_Cⱼ: 類別 C 中特徵 j 的均值
- σ²_Cⱼ: 類別 C 中特徵 j 的方差

多項式樸素貝氏（離散特徵）:
P(xⱼ|C) = (count(xⱼ, C) + α) / (count(C) + α × |V|)
其中 α 為拉普拉斯平滑參數
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 演算法全景對比表

| 演算法 | 類型 | 可解釋性 | 訓練速度 | 預測速度 | 過擬合風險 | 參數敏感度 | 最佳場景 |
|--------|------|---------|---------|---------|-----------|-----------|---------|
| **線性迴歸** | 迴歸 | 極高 | 極快 | 極快 | 低 | 低 | 線性關係、高可解釋性需求 |
| **Ridge/Lasso** | 迴歸 | 高 | 快 | 極快 | 低 | 中（λ） | 多重共線性、特徵選擇 |
| **邏輯迴歸** | 分類 | 高 | 快 | 極快 | 低 | 低 | 二元分類、基準模型 |
| **決策樹** | 分類/迴歸 | 極高 | 快 | 快 | 極高 | 高 | 需要規則解釋、小數據 |
| **隨機森林** | 分類/迴歸 | 中 | 中 | 中 | 低 | 低 | 表格資料、穩定性需求 |
| **GBDT** | 分類/迴歸 | 中 | 慢 | 快 | 中 | 高 | 高準確率需求、Kaggle |
| **XGBoost** | 分類/迴歸 | 中 | 中 | 快 | 低 | 極高 | 表格資料冠軍、工業界首選 |
| **LightGBM** | 分類/迴歸 | 中 | 快 | 極快 | 低 | 極高 | 大數據、快速訓練 |
| **SVM** | 分類 | 低 | 慢 | 中 | 低 | 高（C, γ） | 中小數據、高維空間 |
| **KNN** | 分類/迴歸 | 高 | 極快 | 極慢 | 低 | 中（k） | 小數據、簡單基準 |
| **樸素貝氏** | 分類 | 高 | 極快 | 極快 | 低 | 低 | 文本分類、快速原型 |

### 3.2 集成學習對比

| 集成方法 | 策略 | 基學習器 | 並行性 | 偏差 | 方差 | 典型演算法 |
|---------|------|---------|--------|------|------|-----------|
| **Bagging** | 並行投票/平均 | 強學習器（高方差） | 可並行 | 不變 | 降低 | 隨機森林 |
| **Boosting** | 串行加權 | 弱學習器（高偏差） | 串行 | 降低 | 可能增加 | GBDT, XGBoost, AdaBoost |
| **Stacking** | 元學習器融合 | 多樣化學習器 | 可並行 | 降低 | 降低 | Stacking Ensemble |

**Bagging vs Boosting 核心差異**：
```
Bagging（並行）:
  FinalModel = Average(Model₁ + Model₂ + ... + Modelₙ)
  特點: 降低方差、防止過擬合、可並行訓練

Boosting（串行）:
  FinalModel = α₁Model₁ + α₂Model₂ + ... + αₙModelₙ
  特點: 降低偏差、逐步修正錯誤、串行訓練
```

### 3.3 樹模型對比

| 模型 | 分裂策略 | 生長策略 | 正則化 | 處理缺失值 | 類別特徵 | 訓練速度 |
|------|---------|---------|--------|-----------|---------|---------|
| **決策樹** | Gini/Entropy | Level-wise | 剪枝 | 需預處理 | 需編碼 | 快 |
| **隨機森林** | 隨機特徵 | Level-wise | Bootstrap | 需預處理 | 需編碼 | 中 |
| **GBDT** | 梯度 | Level-wise | 學習率 | 需預處理 | 需編碼 | 慢 |
| **XGBoost** | 梯度+二階 | Level-wise | L1+L2+γ | 內建處理 | 需編碼 | 中 |
| **LightGBM** | 梯度+直方圖 | Leaf-wise | L1+L2 | 內建處理 | 內建處理 | 極快 |

### 3.4 適用場景選擇指南

**資料規模**：
- **小數據（< 10K）**：決策樹、KNN、SVM
- **中數據（10K-1M）**：隨機森林、XGBoost、邏輯迴歸
- **大數據（> 1M）**：LightGBM、邏輯迴歸（SGD）

**特徵類型**：
- **數值特徵為主**：線性迴歸、SVM、神經網路
- **類別特徵為主**：LightGBM（內建支援）、樹模型
- **高維稀疏**：Lasso、邏輯迴歸（L1）

**可解釋性需求**：
- **極高**：決策樹、線性迴歸、邏輯迴歸
- **中等**：隨機森林（特徵重要性）、GBDT（SHAP）
- **低**：SVM（核方法）、深度學習

**準確率優先**：
- **表格資料**：XGBoost > LightGBM > 隨機森林
- **影像資料**：深度學習（CNN）
- **文本資料**：深度學習（Transformer）> 樸素貝氏

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一：房價預測（迴歸問題）

**場景描述**：預測某城市房屋價格，特徵包含面積、房齡、樓層、地段、學區等。

**演算法選擇**：XGBoost 迴歸

**實現要點**：
```python
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

# 1. 資料準備
X_train, y_train = load_housing_data()  # (N, features), (N,)

# 2. 模型訓練
model = XGBRegressor(
    n_estimators=500,      # 500 棵樹
    learning_rate=0.05,    # 學習率
    max_depth=5,           # 樹深度
    min_child_weight=3,    # 最小葉節點樣本權重
    subsample=0.8,         # 樣本採樣比例
    colsample_bytree=0.8,  # 特徵採樣比例
    reg_alpha=0.1,         # L1 正則化
    reg_lambda=1.0,        # L2 正則化
    random_state=42
)

# 3. 交叉驗證
cv_scores = cross_val_score(model, X_train, y_train, cv=5,
                             scoring='neg_mean_squared_error')
print(f"CV RMSE: {np.sqrt(-cv_scores.mean()):.2f}")

# 4. 訓練最終模型
model.fit(X_train, y_train)

# 5. 預測
y_pred = model.predict(X_test)
```

**公式化流程**：
```
HousePricePredict = DataPrep -> FeatureEngineering -> XGBoost -> Evaluation
  where XGBoost = GradientBoosting(
    base_learner = DecisionTree(max_depth=5),
    n_iterations = 500,
    learning_rate = 0.05,
    regularization = L1(0.1) + L2(1.0)
  )

損失函數: L = (1/N)Σ(y_true - y_pred)² + α×Σ|w| + λ×Σw²
```

**關鍵技巧**：
1. **特徵工程**：對數轉換（價格、面積）、多項式特徵（面積²）
2. **超參數調校**：網格搜尋 learning_rate、max_depth、n_estimators
3. **防止過擬合**：Early Stopping（驗證集 50 輪未改善停止）
4. **特徵重要性分析**：識別關鍵特徵（地段、學區權重最高）

### 4.2 應用場景二：垃圾郵件分類（文本分類）

**場景描述**：判斷郵件是否為垃圾郵件（spam），基於郵件內容、主旨、發件人特徵。

**演算法選擇**：樸素貝氏（多項式樸素貝氏）

**實現要點**：
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# 1. 建立 Pipeline
model = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),
    ('nb', MultinomialNB(alpha=1.0))  # Laplace 平滑
])

# 2. 訓練
model.fit(X_train_text, y_train)  # X: 郵件文本列表, y: 0/1 標籤

# 3. 預測機率
proba = model.predict_proba(X_test_text)
```

**公式化**：
```
SpamClassification = TextToVector ∘ NaiveBayes ∘ Threshold

TextToVector(text) = TF-IDF(text) -> sparse_vector(5000維)
  TF-IDF(term, doc) = tf(term) × idf(term)
  idf(term) = log(N / df(term))

NaiveBayes:
  P(spam|text) = P(spam) × Π P(word_i|spam) / P(text)
  P(ham|text) = P(ham) × Π P(word_i|ham) / P(text)

  Decision: argmax(P(spam|text), P(ham|text))
```

**效能對比**：
| 模型 | 準確率 | 訓練時間 | 預測時間 | 記憶體 |
|------|--------|---------|---------|--------|
| 樸素貝氏 | 96.5% | 0.5s | 0.1s | 10MB |
| 邏輯迴歸 | 97.2% | 5s | 0.2s | 50MB |
| SVM（線性核） | 97.8% | 30s | 1s | 100MB |

**選擇樸素貝氏原因**：訓練極快、記憶體低、準確率可接受、適合即時更新模型。

### 4.3 應用場景三：信用卡欺詐檢測（不平衡分類）

**場景描述**：檢測信用卡交易是否為欺詐，資料極度不平衡（欺詐交易 < 0.1%）。

**演算法選擇**：隨機森林 + 類別權重調整

**實現要點**：
```python
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report, roc_auc_score

# 1. 處理不平衡（兩種策略）
# 策略A: SMOTE 過採樣
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# 策略B: 類別權重（推薦）
class_weight = {0: 1, 1: 100}  # 欺詐類別權重 100 倍

# 2. 訓練隨機森林
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight=class_weight,  # 自動平衡
    random_state=42
)
model.fit(X_train, y_train)

# 3. 評估（使用 ROC-AUC、Precision、Recall）
y_pred_proba = model.predict_proba(X_test)[:, 1]
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
```

**公式化**：
```
FraudDetection = ImbalanceHandling ∘ RandomForest ∘ ThresholdOptimization

RandomForest with Class Weight:
  Loss_weighted = Σᵢ w_yᵢ × Loss(yᵢ, ŷᵢ)
  where w_fraud = 100, w_normal = 1

Threshold Optimization:
  threshold = argmax F1(threshold)
  Decision: (P(fraud|x) > threshold) -> fraud
```

**評估指標選擇**：
| 指標 | 使用場景 | 本案例 |
|------|---------|--------|
| Accuracy | 類別平衡 | ❌ 不適用（99.9% 全預測正常也有高準確率） |
| Precision | 減少誤報 | ✓ 重要（減少誤判正常交易） |
| Recall | 減少漏報 | ✓✓ 極重要（不可漏掉欺詐） |
| F1-Score | 平衡 Precision/Recall | ✓ 綜合指標 |
| ROC-AUC | 整體分類能力 | ✓✓ 主要評估指標 |

**實際結果**：
- **Recall = 92%**（捕捉 92% 欺詐交易）
- **Precision = 8%**（每 100 筆預警有 8 筆真欺詐）
- **ROC-AUC = 0.95**（優秀的分類能力）

### 4.4 實作步驟（通用機器學習流程）

**標準七步流程**：

1. **問題定義**：分類 vs 迴歸、評估指標、業務目標
2. **資料探索**：分佈分析、缺失值、異常值、特徵相關性
3. **特徵工程**：清洗、編碼、縮放、創建新特徵
4. **基準模型**：訓練簡單模型（邏輯迴歸、決策樹）建立基準
5. **模型選擇**：嘗試多種演算法（隨機森林、XGBoost、SVM）
6. **超參數調校**：網格搜尋、隨機搜尋、貝氏優化
7. **模型驗證**：交叉驗證、測試集評估、業務指標驗證

**演算法選擇決策樹**：
```
資料類型？
  ├─ 表格資料 -> XGBoost/LightGBM/隨機森林
  ├─ 文本資料 -> 樸素貝氏/邏輯迴歸/Transformer
  ├─ 影像資料 -> CNN
  └─ 時序資料 -> LSTM/GRU

資料規模？
  ├─ < 10K -> 決策樹/KNN/SVM
  ├─ 10K-1M -> XGBoost/隨機森林
  └─ > 1M -> LightGBM/邏輯迴歸

可解釋性？
  ├─ 極高 -> 決策樹/線性迴歸/邏輯迴歸
  ├─ 中等 -> 隨機森林/GBDT
  └─ 低 -> SVM/深度學習

準確率要求？
  ├─ 極高 -> XGBoost/LightGBM（集成多模型）
  ├─ 中等 -> 隨機森林/SVM
  └─ 快速原型 -> 邏輯迴歸/樸素貝氏
```

### 4.5 常見陷阱

**陷阱 1：過度依賴單一演算法**
- **問題**：只用一種演算法（如只用 XGBoost）忽略其他可能性
- **解決**：嘗試至少 3-5 種演算法，選擇最佳者或集成

**陷阱 2：忽略資料預處理**
- **問題**：直接把原始資料丟給模型（缺失值、異常值、未縮放）
- **解決**：樹模型對縮放不敏感，但線性模型、SVM、KNN 需要標準化

**陷阱 3：超參數使用預設值**
- **問題**：XGBoost 預設參數未必最佳（如 learning_rate=0.3 可能過大）
- **解決**：至少調整核心參數（learning_rate、max_depth、n_estimators）

**陷阱 4：不平衡資料用 Accuracy 評估**
- **問題**：99:1 不平衡資料，全預測多數類也有 99% Accuracy
- **解決**：使用 F1-Score、ROC-AUC、Precision-Recall

**陷阱 5：在測試集上調參**
- **問題**：多次在測試集上調整參數，過擬合測試集
- **解決**：訓練集訓練、驗證集調參、測試集僅最終評估一次

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**「線性樹狀集成學，迴歸分類各有招」**
- **線性模型**：線性迴歸、邏輯迴歸、SVM（簡單快速）
- **樹狀模型**：決策樹、隨機森林、GBDT、XGBoost（表格資料王者)
- **集成學習**：Bagging（降方差）、Boosting（降偏差）
- **實例學習**：KNN（懶惰學習）、樸素貝氏（機率分類）

**「XGBoost 表格稱王，隨機森林穩當當」**
- **XGBoost**：Kaggle 競賽首選、準確率最高、參數複雜
- **隨機森林**：穩定可靠、參數少、不易過擬合

**「線性可解釋，樹模型準確，SVM 中庸，KNN 簡單」**
- **可解釋性**：線性迴歸 > 決策樹 > 隨機森林 > SVM
- **準確率**：XGBoost > LightGBM > 隨機森林 > SVM > 邏輯迴歸
- **訓練速度**：樸素貝氏 > KNN > 邏輯迴歸 > 隨機森林 > XGBoost > SVM

### 5.2 記憶技巧

**演算法選擇速記**：「小數據樹和 SVM，大數據 LightGBM，文本樸素貝氏，可解釋線性模型」
```
數據規模:
  小（< 10K）: 決策樹、SVM、KNN
  中（10K-1M）: XGBoost、隨機森林
  大（> 1M）: LightGBM、邏輯迴歸

資料類型:
  表格: XGBoost/LightGBM
  文本: 樸素貝氏/邏輯迴歸
  影像: CNN（深度學習）
```

**集成學習口訣**：「Bagging 並行降方差，Boosting 串行降偏差」
```
Bagging: 隨機森林 = Bootstrap + 多棵樹 + 投票
  降低方差 -> 防止過擬合

Boosting: XGBoost = 串行樹 + 梯度優化 + 殘差擬合
  降低偏差 -> 提升準確率
```

**樹模型進化史**：「決策樹 -> 隨機森林 -> GBDT -> XGBoost -> LightGBM」
```
決策樹: 單棵樹，易過擬合
隨機森林: 多棵樹並行，Bagging
GBDT: 多棵樹串行，Boosting
XGBoost: GBDT + 二階梯度 + 正則化 + 並行化
LightGBM: XGBoost + Leaf-wise + 直方圖優化 + 類別特徵
```

**SVM 核函數速記**：「線性核簡單，多項式核增維，RBF 核萬能」
```
線性核: K(x, x') = x^T x'  # 線性可分資料
多項式核: K(x, x') = (x^T x' + c)^d  # 增加維度
RBF 核: K(x, x') = exp(-γ||x - x'||²)  # 萬能核（最常用）
```

**正則化口訣**：「L1 稀疏化選特徵，L2 平滑化防過擬合」
```
Ridge (L2): λΣw² -> 平滑縮小參數
Lasso (L1): λΣ|w| -> 部分參數歸零（特徵選擇）
Elastic Net: L1 + L2 -> 結合兩者優點
```

### 5.3 快速回憶

**超參數速配**：
```
XGBoost 核心參數:
  - learning_rate (0.01-0.3): 學習率，越小越穩但需更多樹
  - max_depth (3-10): 樹深度，越深越複雜
  - n_estimators (100-1000): 樹數量
  - subsample (0.5-1.0): 樣本採樣比例
  - colsample_bytree (0.5-1.0): 特徵採樣比例
  - reg_alpha (L1), reg_lambda (L2): 正則化

隨機森林核心參數:
  - n_estimators (100-500): 樹數量
  - max_depth (None 或 5-20): 樹深度
  - min_samples_split (2-20): 最小分裂樣本數
  - max_features ('sqrt' 或 'log2'): 隨機特徵數

SVM 核心參數:
  - C (0.1-100): 懲罰係數（越大越嚴格）
  - gamma (0.001-10): RBF 核參數（越大越複雜）
  - kernel ('linear', 'rbf', 'poly'): 核函數

KNN 核心參數:
  - n_neighbors (3-20): 鄰居數量（通常選奇數避免平票）
  - metric ('euclidean', 'manhattan'): 距離度量
```

**損失函數速配**：
```
迴歸: MSE = (1/N)Σ(y - ŷ)²
分類: Cross-Entropy = -Σy log(ŷ)
SVM: Hinge Loss = max(0, 1 - y×f(x))
```

### 5.4 易混淆辨析

**隨機森林 vs XGBoost**：
- **隨機森林**：Bagging（並行）、多棵樹獨立、投票聚合、不易過擬合
- **XGBoost**：Boosting（串行）、樹之間有依賴、加權聚合、需防過擬合（正則化）

**L1 vs L2 正則化**：
- **L1（Lasso）**：Σ|w| → 稀疏解（部分 w=0）→ 特徵選擇
- **L2（Ridge）**：Σw² → 平滑解（w 接近 0 但不為 0）→ 防止過擬合

**Bagging vs Boosting**：
- **Bagging**：並行訓練、降低方差、典型：隨機森林
- **Boosting**：串行訓練、降低偏差、典型：XGBoost

**KNN vs K-Means**：
- **KNN**：分類/迴歸演算法、有監督學習、基於鄰居投票
- **K-Means**：聚類演算法、無監督學習、基於距離聚類

**分類樹 vs 迴歸樹**：
- **分類樹**：分裂準則 Gini/Entropy、葉節點投票、預測類別
- **迴歸樹**：分裂準則 MSE、葉節點平均值、預測數值

**SVM vs 邏輯迴歸**：
- **SVM**：最大間隔、支援向量、核技巧、適合中小數據
- **邏輯迴歸**：最大似然、機率輸出、線性決策邊界、適合大數據

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**Q1：以下哪個演算法最適合處理高維稀疏文本資料？**
A. 隨機森林
B. XGBoost
C. Lasso 迴歸
D. KNN

**Q2：隨機森林相比單棵決策樹的主要優勢是？**
A. 訓練速度更快
B. 可解釋性更高
C. 降低過擬合風險
D. 記憶體佔用更少

**Q3：XGBoost 相比 GBDT 的主要改進包括（多選）？**
A. 使用二階梯度資訊
B. 加入 L1/L2 正則化
C. 支援並行化訓練
D. 內建處理缺失值

**Q4：在極度不平衡資料（欺詐檢測）中，最不應該使用的評估指標是？**
A. Accuracy
B. Precision
C. Recall
D. F1-Score

**Q5：以下哪個演算法訓練時間複雜度最高（N=樣本數，D=特徵數）？**
A. 邏輯迴歸 O(ND)
B. KNN O(1)（訓練幾乎無成本）
C. SVM O(N²) 到 O(N³)
D. 隨機森林 O(N log N)

### 6.2 簡答題

**Q1：解釋 Bagging 和 Boosting 的核心差異，並說明隨機森林和 XGBoost 分別屬於哪一類。**

**Q2：為什麼 XGBoost 在 Kaggle 表格資料競賽中表現優異？列舉至少三個技術原因。**

### 6.3 答案解析

**選擇題答案**：

**A1：C（Lasso 迴歸）**
- **解析**：高維稀疏文本資料（如 TF-IDF 向量，特徵數 > 10000）中，Lasso（L1 正則化）能自動進行特徵選擇，將不重要特徵的係數歸零，產生稀疏解。隨機森林和 XGBoost 在高維稀疏資料上效率較低，KNN 在高維空間受「維度詛咒」影響嚴重。

**A2：C（降低過擬合風險）**
- **解析**：隨機森林通過 Bagging（Bootstrap 採樣 + 隨機特徵選擇）訓練多棵獨立的決策樹，並通過投票/平均聚合預測，有效降低單棵樹的高方差問題，防止過擬合。訓練速度比單棵樹慢（需訓練多棵），可解釋性較低（難以理解多棵樹的組合邏輯），記憶體佔用更多（儲存多棵樹）。

**A3：A, B, C, D（全選）**
- **解析**：XGBoost 是 GBDT 的增強版本，主要改進包括：
  - **A - 二階梯度**：使用泰勒二階展開（一階梯度 g + 二階梯度 h），比 GBDT 只用一階梯度更精確
  - **B - 正則化**：目標函數加入 L1（reg_alpha）和 L2（reg_lambda）正則化項，防止過擬合
  - **C - 並行化**：分裂點搜尋可並行化（雖然樹之間是串行，但單棵樹內並行），加速訓練
  - **D - 缺失值處理**：內建學習缺失值的最佳分裂方向，無需預填補

**A4：A（Accuracy）**
- **解析**：在極度不平衡資料（如欺詐佔 0.1%）中，即使模型預測全部為正常類別，Accuracy 也有 99.9%，無法反映模型真實能力。應使用：
  - **Precision**：預測為欺詐中真正是欺詐的比例（減少誤報）
  - **Recall**：真實欺詐中被檢測出的比例（減少漏報，關鍵指標）
  - **F1-Score**：Precision 和 Recall 的調和平均
  - **ROC-AUC**：不同閾值下的綜合分類能力

**A5：C（SVM O(N²) 到 O(N³)）**
- **解析**：
  - **SVM**：求解二次規劃問題，時間複雜度 O(N²) 到 O(N³)（取決於實現），大數據不適用
  - **邏輯迴歸**：梯度下降優化，O(ND) 每次迭代，總體 O(I×N×D)（I=迭代次數），大數據友好
  - **KNN**：訓練幾乎無成本 O(1)（只需儲存資料），但預測成本極高 O(N×D)（需計算與所有樣本距離）
  - **隨機森林**：O(M×N log N × D)（M=樹數量），中等規模資料適用

**簡答題答案**：

**A1：Bagging vs Boosting**

**核心差異**：

| 維度 | Bagging | Boosting |
|------|---------|----------|
| **訓練方式** | 並行（獨立訓練多個模型） | 串行（後一個模型依賴前一個模型） |
| **資料採樣** | Bootstrap 有放回採樣 | 根據錯誤調整樣本權重 |
| **聚合方式** | 投票（分類）或平均（迴歸） | 加權聚合（權重由訓練誤差決定） |
| **目標** | 降低方差（Variance），防止過擬合 | 降低偏差（Bias），提升準確率 |
| **基學習器** | 強學習器（如深度決策樹） | 弱學習器（如淺層決策樹） |
| **典型演算法** | 隨機森林（Random Forest） | AdaBoost、GBDT、XGBoost |

**數學表達**：
```
Bagging:
  F(x) = (1/M) Σₘ fₘ(x)  # 平均
  或 F(x) = mode({fₘ(x)})  # 投票
  其中每個 fₘ 獨立訓練

Boosting:
  F(x) = Σₘ αₘ fₘ(x)  # 加權聚合
  其中 fₘ 訓練時依賴 F_{m-1} 的錯誤
```

**隨機森林（Bagging）**：
- 訓練 M 棵決策樹，每棵樹使用 Bootstrap 採樣資料 + 隨機特徵子集
- 預測時投票（分類）或平均（迴歸）
- 降低單棵樹的高方差，防止過擬合

**XGBoost（Boosting）**：
- 串行訓練 M 棵樹，每棵樹擬合前一棵樹的殘差（負梯度）
- 預測時加權求和：F(x) = f₀(x) + ν×f₁(x) + ν×f₂(x) + ... （ν=學習率）
- 逐步降低偏差，提升準確率，但需防過擬合（正則化）

**A2：XGBoost 在 Kaggle 表格資料競賽的優勢**

**技術原因（至少三個）**：

**1. 二階梯度優化（更精確的近似）**：
```
GBDT 使用一階梯度:
  殘差 rᵢ = -∂L/∂ŷ

XGBoost 使用泰勒二階展開:
  Obj ≈ Σᵢ [gᵢfₜ(xᵢ) + (1/2)hᵢfₜ²(xᵢ)] + Ω(fₜ)
  其中 gᵢ = ∂L/∂ŷ, hᵢ = ∂²L/∂ŷ²

好處: 更準確的近似，更快的收斂
```

**2. 正則化防止過擬合（L1 + L2 + 樹複雜度）**：
```
正則化項:
  Ω(f) = γT + (λ/2)Σⱼ wⱼ²
  其中 T=葉節點數, wⱼ=葉節點權重

好處:
  - γ 控制樹的複雜度（葉節點數量）
  - λ 控制葉節點權重大小
  - 自動剪枝（分裂增益 < γ 則不分裂）
```

**3. 自動處理缺失值（學習最佳分裂方向）**：
```
處理策略:
  1. 訓練時將缺失值分別嘗試放入左子樹和右子樹
  2. 選擇增益更大的方向作為預設方向
  3. 預測時缺失值自動走預設方向

好處: 無需手動填補缺失值，模型自動學習最佳處理方式
```

**4. 分裂點並行化（加速訓練）**：
```
並行策略:
  - 雖然樹之間是串行，但單棵樹內的分裂點搜尋可並行
  - 特徵預排序 + 並行掃描分裂點
  - 支援 GPU 加速

好處: 訓練速度比 GBDT 快 10 倍以上
```

**5. 內建交叉驗證與 Early Stopping**：
```python
model.fit(X_train, y_train,
          eval_set=[(X_val, y_val)],
          early_stopping_rounds=50,  # 驗證集 50 輪未改善停止
          verbose=True)

好處: 自動防止過擬合，無需手動調整樹數量
```

**6. 靈活的目標函數與自定義損失**：
```
支援多種損失函數:
  - 迴歸: MSE, MAE, Huber
  - 分類: Logistic, Softmax
  - 排序: Pairwise, Listwise
  - 自定義: 可自定義一階/二階梯度

好處: 適應各種業務場景（點擊率預測、排序、推薦）
```

**7. 豐富的超參數調校空間**：
```
核心超參數:
  - learning_rate (0.01-0.3): 控制學習速度
  - max_depth (3-10): 控制樹複雜度
  - subsample (0.5-1.0): 樣本採樣比例
  - colsample_bytree (0.5-1.0): 特徵採樣比例
  - reg_alpha, reg_lambda: L1/L2 正則化

好處: 可精細調校以達到最佳性能
```

**綜合優勢**：
XGBoost 結合了準確率（二階優化 + Boosting）、穩定性（正則化）、效率（並行化）、易用性（自動處理缺失值 + Early Stopping），成為表格資料競賽的首選演算法。在 Kaggle 歷史競賽中，超過 70% 的冠軍方案使用了 XGBoost 或其變體（LightGBM）。

### 6.4 易錯點提醒

**易錯點 1：混淆隨機森林與 XGBoost 的訓練方式**
- **錯誤**：認為隨機森林也是串行訓練
- **提醒**：隨機森林是並行（Bagging），XGBoost 是串行（Boosting）

**易錯點 2：過度相信 XGBoost**
- **錯誤**：任何問題都直接用 XGBoost
- **提醒**：影像用 CNN、文本用 Transformer、時序用 LSTM，XGBoost 主要適合表格資料

**易錯點 3：忽略資料預處理**
- **錯誤**：認為樹模型不需要特徵縮放
- **提醒**：樹模型對縮放不敏感（正確），但仍需處理缺失值、異常值、類別編碼

**易錯點 4：KNN 用於大數據**
- **錯誤**：百萬級資料使用 KNN
- **提醒**：KNN 預測時間複雜度 O(N)，大數據極慢，適合小數據（< 10K）

**易錯點 5：SVM 用預設核函數**
- **錯誤**：不選擇核函數，使用預設 RBF 核
- **提醒**：線性可分資料用線性核（更快），非線性資料才用 RBF 核，需調整 gamma 參數
