# L23302 - 模型選擇與架構設計

## 1. 核心定義 (20%, 400-1200 字)

### 1.1 主題定義

模型選擇與架構設計是機器學習專案中決定模型效能上限的關鍵階段。模型選擇涵蓋演算法選擇（線性 vs 樹模型 vs 深度學習）、超參數配置、模型複雜度控制;架構設計則包含網路結構設計（深度、寬度、連接方式）、模型組合策略（集成學習）、資源分配優化。

核心挑戰包含:如何根據資料特性選擇合適演算法、如何平衡模型複雜度與泛化能力（過擬合 vs 欠擬合）、如何高效搜尋超參數空間（網格搜尋 vs 貝氏優化）、如何設計深度神經網路架構（殘差連接、注意力機制）。理解模型選擇準則（AIC、BIC）與架構設計原則是優化模型效能的核心能力。

### 1.2 核心概念

**模型選擇與架構設計的七大核心概念:**

1. **模型選擇準則**: AIC、BIC 量化模型複雜度與擬合度平衡
2. **超參數**:控制學習過程的配置（學習率、樹深度、正則化強度）
3. **超參數搜尋**:網格搜尋、隨機搜尋、貝氏優化
4. **神經網路架構**:深度（層數）、寬度（神經元數）、連接方式
5. **模型複雜度**:參數數量、VC 維、Rademacher 複雜度
6. **架構設計原則**:殘差連接、Batch Normalization、Dropout
7. **自動化機器學習（AutoML）**:NAS、超參數優化自動化

### 1.3 CFDS 分解

```
ModelSelection = f(C, F, D, S)

C = AlgorithmChoice ∘ HyperparameterSearch ∘ ArchitectureDesign ∘ Validation
F = {ModelConfig, Hyperparameters, Architecture, SearchSpace}
D = TrainingData + ValidationData + PerformanceMetrics
S = Searching | Training | Validated | Deployed
```

### 1.4 技術定位

模型選擇在機器學習流程中處於核心決策層,直接影響最終效能。實務經驗:好的模型選擇+簡單特徵 > 差模型+複雜特徵。典型流程:基準模型 -> 模型對比 -> 超參數調校 -> 架構優化 -> 集成融合。

---

## 2. 關鍵公式 (25%, 500-1500 字)

### 2.1 模型選擇準則

**赤池資訊準則（AIC）**:

```
AIC = 2k - 2ln(L̂)

其中:
- k: 參數數量
- L̂: 最大似然估計

目標: 最小化AIC（平衡擬合與複雜度）

解釋:
- -2ln(L̂): 擬合度（越小越好）
- 2k: 複雜度懲罰（參數越多懲罰越大）

適用: 樣本量大、預測導向
```

**貝氏資訊準則（BIC）**:

```
BIC = k×ln(N) - 2ln(L̂)

其中:
- N: 樣本數量

與AIC差異:
- BIC懲罰項 k×ln(N) > AIC的2k（當N > 8時）
- BIC更傾向簡單模型

適用: 樣本量大、模型選擇
```

**AIC vs BIC 對比**:

```
樣本量N = 100, 參數k = 5:
  AIC懲罰: 2×5 = 10
  BIC懲罰: 5×ln(100) ≈ 23

BIC更嚴格,傾向選擇更簡單模型
```

### 2.2 超參數搜尋

**網格搜尋（Grid Search）**:

```
超參數空間:
  learning_rate ∈ [0.001, 0.01, 0.1]
  max_depth ∈ [3, 5, 10]
  n_estimators ∈ [100, 200, 500]

嘗試所有組合: 3×3×3 = 27種

時間複雜度: O(|Θ₁| × |Θ₂| × ... × |Θₙ|)（指數增長）

優點: 窮舉搜尋,不會遺漏
缺點: 高維時計算爆炸
```

**隨機搜尋（Random Search）**:

```
從超參數空間隨機採樣N組配置

採樣策略:
  learning_rate ~ Uniform(0.001, 0.1)  # 連續
  max_depth ~ DiscreteUniform(3, 10)  # 離散

優勢（Bergstra & Bengio 2012）:
  假設僅2個參數重要,其餘無關
  - 網格搜尋: 重要參數採樣點有限
  - 隨機搜尋: 每次都採樣不同重要參數值

時間複雜度: O(N)（線性）

實證: 60次隨機搜尋 ≈ 窮舉網格搜尋
```

**貝氏優化（Bayesian Optimization）**:

```
核心思想: 用歷史評估結果指導下一次採樣

流程:
1. 建立替代模型（高斯過程）:
   P(y|θ) ~ GP(μ(θ), k(θ, θ'))

2. 獲取函數（Acquisition Function）:
   EI(θ) = E[max(0, y_best - y(θ))]  # Expected Improvement

3. 選擇下一個超參數:
   θ_next = argmax EI(θ)

4. 評估並更新替代模型

優勢:
- 利用歷史資訊
- 平衡探索（Exploration）與利用（Exploitation）
- 適合昂貴評估（深度學習訓練）

缺點: 高維時效果降低（> 20維）
```

### 2.3 神經網路架構設計

**殘差連接（ResNet）**:

```
標準層:
  y = F(x)

殘差層:
  y = F(x) + x  # 跳躍連接

梯度流動:
  ∂L/∂x = ∂L/∂y × (∂F/∂x + 1)

好處:
- 梯度可直接通過+1流動,緩解梯度消失
- 允許訓練極深網路（152層、1000層）
- Identity mapping使網路至少不比淺層網路差
```

**Batch Normalization**:

```
歸一化（訓練時）:
  x̂ = (x - μ_batch) / √(σ²_batch + ε)

可學習參數:
  y = γx̂ + β

其中:
- μ_batch, σ²_batch: 當前batch的均值/方差
- γ, β: 可學習參數（恢復表達能力）

推理時:
  μ, σ²使用訓練時的移動平均

好處:
- 加速訓練（允許更大學習率）
- 緩解梯度消失
- 輕微正則化效果
```

**Dropout**:

```
訓練時:
  h_dropout = h ⊙ Bernoulli(p)

  其中p為保留機率（通常0.5）

推理時:
  h_inference = p × h  # 期望值匹配

等價於模型集成:
  N個神經元,p=0.5 -> 2^N個子網路
  訓練時隨機採樣,推理時平均

好處:
- 防止過擬合
- 增加魯棒性
- 隱式集成
```

### 2.4 模型複雜度

**VC 維（Vapnik-Chervonenkis Dimension）**:

```
定義: 模型能完全打散（shatter）的最大樣本數

範例:
  線性分類器（2D）: VC = 3
  （能打散任意3點,但存在4點無法打散）

  多層感知機（H個隱藏神經元）:
  VC ≈ O(W × log W)
  （W為權重數量）

泛化界:
  R(h) ≤ R̂(h) + O(√(VC×log(N/VC) / N))

  其中:
  - R(h): 真實誤差
  - R̂(h): 訓練誤差
  - N: 樣本數

啟示: VC維越高,需要更多樣本才能泛化
```

---

## 3. 對比矩陣 (15%, 300-900 字)

### 3.1 超參數搜尋方法對比

| 方法     | 搜尋策略 | 時間複雜度 | 適用場景          | 典型迭代次數 |
| -------- | -------- | ---------- | ----------------- | ------------ |
| 網格搜尋 | 窮舉     | O(∏\|Θᵢ\|) | 低維（< 3 參數）  | 全部組合     |
| 隨機搜尋 | 隨機採樣 | O(N)       | 中維（3-10 參數） | 50-200       |
| 貝氏優化 | 基於歷史 | O(N²)      | 高成本評估        | 20-100       |

### 3.2 模型選擇準則對比

| 準則 | 公式             | 懲罰強度 | 傾向     | 適用     |
| ---- | ---------------- | -------- | -------- | -------- |
| AIC  | 2k - 2ln(L̂)      | 2k       | 預測     | 大樣本   |
| BIC  | k×ln(N) - 2ln(L̂) | k×ln(N)  | 簡單模型 | 模型選擇 |
| CV   | K-Fold 評估      | -        | 泛化     | 中小樣本 |

### 3.3 架構設計模式對比

| 模式       | 核心機制 | 解決問題 | 典型應用 | 計算成本 |
| ---------- | -------- | -------- | -------- | -------- |
| 殘差連接   | y=F(x)+x | 梯度消失 | ResNet   | +10%     |
| Batch Norm | 歸一化   | 訓練穩定 | 所有 CNN | +20%     |
| Dropout    | 隨機丟棄 | 過擬合   | 全連接層 | 訓練+50% |

---

## 4. 實務應用 (20%, 400-1200 字)

### 4.1 應用場景一:表格資料模型選擇

**場景**:客戶流失預測（表格資料、10 萬樣本、50 特徵）

**實現**:

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

models = {
    'Logistic': LogisticRegression(),
    'RF': RandomForestClassifier(n_estimators=100),
    'XGBoost': XGBClassifier(n_estimators=100)
}

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
    print(f"{name}: {scores.mean():.4f} ± {scores.std():.4f}")

# 輸出:
# Logistic: 0.8200 ± 0.015
# RF: 0.8650 ± 0.012
# XGBoost: 0.8920 ± 0.010  # 最佳

# 選擇XGBoost進入超參數調校
```

### 4.2 應用場景二:超參數調校

**場景**:XGBoost 超參數優化

**網格搜尋實現**:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 500],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

grid = GridSearchCV(
    XGBClassifier(),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)

grid.fit(X_train, y_train)
print(f"最佳參數: {grid.best_params_}")
print(f"最佳分數: {grid.best_score_:.4f}")

# 輸出:
# 最佳參數: {'learning_rate': 0.05, 'max_depth': 5, ...}
# 最佳分數: 0.8985
```

**貝氏優化實現**:

```python
from skopt import BayesSearchCV
from skopt.space import Real, Integer

search_spaces = {
    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
    'max_depth': Integer(3, 10),
    'n_estimators': Integer(50, 500),
    'subsample': Real(0.5, 1.0),
    'colsample_bytree': Real(0.5, 1.0)
}

bayes_search = BayesSearchCV(
    XGBClassifier(),
    search_spaces,
    n_iter=50,  # 僅50次評估
    cv=5,
    scoring='roc_auc'
)

bayes_search.fit(X_train, y_train)
print(f"貝氏最佳分數: {bayes_search.best_score_:.4f}")
# 貝氏最佳分數: 0.9010（超越網格搜尋,僅用50次）
```

### 4.3 應用場景三:神經網路架構設計

**場景**:CIFAR-10 影像分類

**基準 CNN**:

```python
import torch.nn as nn

class BaselineCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.fc = nn.Linear(128*8*8, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        return self.fc(x)

# 測試準確率: 75%
```

**改進架構（殘差+BatchNorm+Dropout）**:

```python
class ImprovedCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        # 殘差連接
        self.shortcut = nn.Conv2d(64, 128, 1, stride=2)
        self.fc = nn.Linear(128*8*8, 10)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        identity = self.shortcut(x)
        x = F.max_pool2d(x, 2)
        x = F.relu(self.bn2(self.conv2(x)))
        x = x + identity  # 殘差
        x = F.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        return self.fc(x)

# 測試準確率: 82%（+7%）
```

### 4.4 實作步驟

1. **基準模型**: 訓練簡單模型（邏輯迴歸、決策樹）
2. **模型對比**: 嘗試 3-5 種演算法,交叉驗證評估
3. **超參數搜尋**: 隨機搜尋粗調 -> 網格搜尋精調
4. **架構優化**: 加入 Batch Norm、Dropout、殘差
5. **集成融合**: Bagging、Stacking 提升 1-2%

### 4.5 常見陷阱

**陷阱 1**: 過度調參（在驗證集上反覆調參,過擬合驗證集）
**解決**: 保留獨立測試集,僅最終評估一次

**陷阱 2**: 網格搜尋高維爆炸（10 個參數各 3 值 = 3^10 = 59049 次）
**解決**: 先隨機搜尋縮小範圍,再網格精調

**陷阱 3**: 忽略計算資源約束（貝氏優化深度學習,每次評估數小時）
**解決**: 減少訓練 epoch、使用早期停止、小數據集驗證

---

## 5. 記憶口訣 (10%, 200-600 字)

### 5.1 核心口訣

**「AIC BIC 選模型,網格隨機貝氏調」**

- AIC/BIC: 模型選擇準則（懲罰複雜度）
- 網格搜尋: 窮舉（低維）
- 隨機搜尋: 高效（中維）
- 貝氏優化: 智慧（高成本）

**「殘差批歸丟棄三法寶」**

- 殘差連接: 緩解梯度消失
- Batch Normalization: 加速訓練
- Dropout: 防止過擬合

### 5.2 記憶技巧

**超參數搜尋選擇樹**:

```
參數數量?
  ├─ < 3參數 -> 網格搜尋（窮舉）
  ├─ 3-10參數 -> 隨機搜尋（高效）
  └─ > 10參數 -> 貝氏優化或AutoML

評估成本?
  ├─ 便宜（< 1分鐘）-> 網格/隨機
  └─ 昂貴（> 1小時）-> 貝氏優化
```

**AIC vs BIC 速記**:

```
AIC = 2k - 2ln(L̂)  # 2倍參數懲罰
BIC = k×ln(N) - 2ln(L̂)  # ln(N)倍懲罰

N > 8時: BIC > AIC（更嚴格）
選擇: 預測用AIC,模型選擇用BIC
```

### 5.3 快速回憶

**神經網路深度 vs 寬度**:

```
深度（層數）:
  - 增加表達能力（非線性組合）
  - 風險: 梯度消失（需殘差連接）
  - 經驗: 影像18-152層,NLP 12-24層

寬度（神經元數）:
  - 增加容量（記憶能力）
  - 風險: 過擬合（需Dropout）
  - 經驗: 隱藏層64-512神經元
```

### 5.4 易混淆辨析

**超參數 vs 參數**:

- 超參數: 訓練前設定（學習率、層數）
- 參數: 訓練中學習（權重、偏差）

**網格 vs 隨機搜尋**:

- 網格: 窮舉組合,低維適用
- 隨機: 隨機採樣,中維高效

**AIC vs BIC**:

- AIC: 預測導向,懲罰較輕
- BIC: 模型選擇,懲罰較重

---

## 6. 自我驗證 (10%, 200-600 字)

### 6.1 選擇題

**Q1: 以下哪個超參數搜尋方法最適合深度學習（訓練成本極高）?**
A. 網格搜尋
B. 隨機搜尋
C. 貝氏優化
D. 窮舉搜尋

**Q2: BIC 相比 AIC 的主要差異是?**
A. 懲罰更輕
B. 懲罰更重
C. 不考慮參數數量
D. 僅適用小樣本

**Q3: 殘差連接的主要作用是?**
A. 減少參數量
B. 加速訓練
C. 緩解梯度消失
D. 防止過擬合

**Q4: Dropout 在推理時應該?**
A. 保持啟用
B. 關閉（不丟棄神經元）
C. 僅丟棄 50%
D. 隨機決定

**Q5: 網格搜尋的時間複雜度是?（n 個參數,每個 k 個值）**
A. O(n×k)
B. O(n^k)
C. O(k^n)
D. O(n+k)

### 6.2 簡答題

**Q1: 解釋為什麼隨機搜尋在高維超參數空間中優於網格搜尋?**

**Q2: 描述設計卷積神經網路架構時需考慮的五個關鍵因素。**

### 6.3 答案解析

**A1: C（貝氏優化）**
解析: 深度學習訓練成本極高（數小時到數天）,貝氏優化利用歷史評估結果指導下一次採樣,僅需 20-50 次評估即可找到優質配置,遠少於網格搜尋的數百次。

**A2: B（懲罰更重）**
解析: BIC 懲罰項 k×ln(N)當 N>8 時大於 AIC 的 2k,更傾向選擇簡單模型,適合模型選擇而非預測。

**A3: C（緩解梯度消失）**
解析: 殘差連接 y=F(x)+x 使梯度可直接通過+1 流動（∂y/∂x=∂F/∂x+1）,解決深度網路梯度消失,允許訓練 152 層甚至 1000 層網路。

**A4: B（關閉）**
解析: Dropout 僅訓練時啟用（隨機丟棄），推理時需關閉並用 p×h 匹配訓練時期望值，確保預測穩定。

**A5: C（O(k^n)）**
解析: n 個參數各 k 個值需嘗試 k×k×...×k（n 次）= k^n 種組合,指數增長。

**簡答題答案**:

**A1: 隨機搜尋優於網格搜尋原因**

**核心優勢**: 隨機搜尋在重要參數上採樣點更密集。

**數學解釋**:
假設 2 個超參數 θ₁（重要）、θ₂（不重要）

- 網格搜尋（各 3 值）: 3×3=9 次評估
  - θ₁ 僅採樣 3 個不同值
  - θ₂ 也採樣 3 個值（浪費）
- 隨機搜尋（9 次）:
  - θ₁ 採樣 9 個不同值（密集探索）
  - θ₂ 採樣 9 個值（但不重要）

**實證結果（Bergstra & Bengio 2012）**:
9 次隨機搜尋覆蓋 θ₁ 空間遠優於 9 次網格搜尋（3×3）,因為隨機搜尋每次都探索不同 θ₁ 值。

**高維優勢**: 10 個參數,通常僅 2-3 個真正重要。隨機搜尋在重要參數上採樣密集,網格搜尋浪費大量評估在無關參數組合。

**A2: CNN 架構設計關鍵因素**

1. **深度（層數）**: 權衡表達能力與梯度消失（18-152 層）
2. **寬度（卷積核數）**: 每層 64-512 個核,平衡容量與計算
3. **卷積核大小**: 3×3 主流（感受野與參數平衡）
4. **池化策略**: 2×2 Max Pooling 降維
5. **正則化**: Dropout(0.5)、Batch Norm、L2 懲罰

### 6.4 易錯點提醒

**易錯點 1**: 超參數搜尋用測試集
**提醒**: 測試集僅最終評估,超參數調整用驗證集

**易錯點 2**: Batch Normalization 推理時仍用 batch 統計
**提醒**: 推理用訓練時的移動平均 μ 和 σ²

**易錯點 3**: 殘差連接維度不匹配
**提醒**: F(x)和 x 維度必須相同,否則需 1×1 卷積調整

**易錯點 4**: 過度依賴 AIC 選模型
**提醒**: AIC 適合預測,模型選擇用 BIC 或交叉驗證

**易錯點 5**: 網格搜尋高維空間
**提醒**: 超過 3 個參數用隨機搜尋或貝氏優化
