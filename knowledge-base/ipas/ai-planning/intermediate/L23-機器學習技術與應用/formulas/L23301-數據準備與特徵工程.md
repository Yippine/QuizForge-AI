# L23301 - 數據準備與特徵工程

## 1. 核心定義 (20%, 400-1200字)

### 1.1 主題定義

數據準備與特徵工程是機器學習專案中最耗時但最關鍵的階段,佔據整個專案70-80%的時間。數據準備包含數據收集、清洗、驗證、轉換等步驟,確保資料品質;特徵工程則是從原始資料中提取、創建、選擇有效特徵,將資料轉換為模型可理解的格式。

俗話說「Garbage In, Garbage Out」,再先進的演算法也無法從低品質資料中學到有效模式。特徵工程的核心包含:缺失值處理、異常值檢測、特徵縮放（標準化、正規化）、特徵編碼（One-hot、Label Encoding）、特徵選擇（過濾法、包裹法、嵌入法）、特徵提取與降維（PCA、LDA）。

### 1.2 核心概念

**數據準備與特徵工程的八大核心概念:**

1. **數據清洗**:處理缺失值、重複值、異常值、不一致資料
2. **特徵縮放**:標準化（Z-score）、正規化（Min-Max）、Robust Scaling
3. **特徵編碼**:類別變數轉數值（One-hot、Label、Target Encoding）
4. **特徵選擇**:移除冗餘特徵、保留重要特徵（降維、提升效能）
5. **特徵提取**:從原始資料中創建新特徵（多項式、交互作用、聚合）
6. **降維技術**:PCA、t-SNE、LDA（高維->低維,保留主要資訊）
7. **資料不平衡處理**:過採樣（SMOTE）、欠採樣、類別權重調整
8. **特徵工程自動化**:AutoML、特徵生成工具（Featuretools）

### 1.3 CFDS 分解

基於 Formula-Contract 方法論,將數據準備與特徵工程分解為四個基本單元:

```
DataPreparation = f(C, F, D, S)
```

**C (Code - 可執行邏輯)**
```
C = DataLoading ∘ Cleaning ∘ Transformation ∘ FeatureEngineering ∘ Validation
  = 資料載入 -> 清洗 -> 轉換 -> 特徵工程 -> 驗證

FeatureEngineering = FeatureCreation + FeatureSelection + FeatureScaling + FeatureEncoding

清洗流程:
  RemoveDuplicates -> HandleMissing -> DetectOutliers -> FixInconsistency
```

**F (Files - 配置資源)**
```
F = {RawData, CleanedData, FeatureConfig, ScalingParams, EncodingMap}
  = {原始資料, 清洗後資料, 特徵配置, 縮放參數, 編碼映射}

例如:
ScalingParams = {mean: [0.5, 1.2, 3.4], std: [0.2, 0.8, 1.1]}  # 標準化參數
EncodingMap = {'Gender': {'M': 0, 'F': 1}, 'City': {'Taipei': 0, 'Taichung': 1, ...}}
```

**D (Data - 資料結構)**
```
D = RawDataFrame + ProcessedDataFrame + FeatureMatrix + LabelVector
  = 原始資料表 + 處理後資料表 + 特徵矩陣 + 標籤向量

例如:
RawData = pd.DataFrame(columns=['Age', 'Gender', 'Income', 'City', 'Target'])
ProcessedData = pd.DataFrame(columns=['Age_scaled', 'Gender_encoded', 'Income_log', ...])
```

**S (State - 運行狀態)**
```
S = Raw | Cleaning | Transformed | FeatureEngineered | Validated | ModelReady
  = 原始 | 清洗中 | 已轉換 | 已特徵化 | 已驗證 | 模型就緒

資料品質狀態: Missing_5% | Outliers_Detected | Duplicates_Removed
特徵狀態: Scaled | Encoded | Selected | Dimension_Reduced
```

### 1.4 技術定位

數據準備與特徵工程在機器學習流程中處於前置關鍵層,直接影響模型效能上限。在實務中:

- **資料品質影響**:垃圾資料無法訓練有效模型（GIGO原則）
- **特徵優於演算法**:好的特徵+簡單模型 > 差特徵+複雜模型
- **領域知識重要**:理解業務邏輯才能創建有效特徵
- **時間佔比**:資料科學家70-80%時間在資料準備與特徵工程

典型流程:
```
原始資料 -> 數據清洗 -> 探索性分析（EDA）-> 特徵工程 -> 模型訓練 -> 評估優化
```

工具生態:
- **Python**: Pandas（資料處理）、Scikit-learn（特徵工程）、Featuretools（自動化）
- **視覺化**: Matplotlib、Seaborn、Plotly
- **大數據**: Spark MLlib、Dask

---

## 2. 關鍵公式 (25%, 500-1500字)

### 2.1 特徵縮放

**標準化（Z-score Normalization）**:
```
x_scaled = (x - μ) / σ

其中:
- μ: 均值 = (1/N) Σᵢ xᵢ
- σ: 標準差 = √[(1/N) Σᵢ (xᵢ - μ)²]

結果特性:
- 均值為0
- 標準差為1
- 分佈形狀不變
- 可能有負值

適用場景:
- 線性迴歸、邏輯迴歸、SVM、神經網路
- 資料近似常態分佈
- 對異常值敏感（異常值影響μ和σ）
```

**正規化（Min-Max Normalization）**:
```
x_scaled = (x - x_min) / (x_max - x_min)

結果範圍: [0, 1]

變體（指定範圍[a, b]）:
x_scaled = a + (x - x_min) × (b - a) / (x_max - x_min)

適用場景:
- 神經網路（輸入需在固定範圍）
- 影像處理（像素值0-255 -> 0-1）
- KNN、K-means（距離計算）
- 對異常值極敏感（x_max和x_min易受影響）
```

**Robust Scaling（抗異常值縮放）**:
```
x_scaled = (x - median) / IQR

其中:
- median: 中位數
- IQR: 四分位距 = Q₃ - Q₁
  - Q₁: 第25百分位數
  - Q₃: 第75百分位數

優勢:
- 使用中位數和IQR,對異常值不敏感
- 保留異常值資訊（不會壓縮到極端範圍）

適用場景:
- 資料包含異常值
- 金融資料、醫療資料（常有極端值）
```

**縮放方法對比**:
```
範例資料: [1, 2, 3, 4, 100]（100為異常值）

標準化:
  μ = 22, σ ≈ 44
  結果: [-0.48, -0.45, -0.43, -0.41, 1.77]  # 異常值拉高μ和σ

正規化:
  [0, 0.01, 0.02, 0.03, 1.0]  # 正常值被壓縮到[0, 0.03]

Robust:
  median = 3, IQR = 3
  [-0.67, -0.33, 0, 0.33, 32.33]  # 保留異常值特性
```

### 2.2 缺失值處理

**刪除法**:
```
列刪除（Listwise Deletion）:
  刪除包含任何缺失值的樣本
  適用: 缺失比例 < 5%

行刪除（Column Deletion）:
  刪除缺失比例過高的特徵（如 > 50%）
  適用: 該特徵不重要或可替代
```

**填補法（Imputation）**:
```
均值填補:
  x_missing = mean(x_observed)
  適用: 數值型特徵、資料近似常態分佈

中位數填補:
  x_missing = median(x_observed)
  適用: 數值型特徵、存在異常值

眾數填補:
  x_missing = mode(x_observed)
  適用: 類別型特徵

常數填補:
  x_missing = constant（如-1、'Unknown'）
  適用: 缺失本身有意義（如問卷「拒絕回答」）

前向/後向填補（時序資料）:
  x_t_missing = x_(t-1)（前向）或 x_(t+1)（後向）
  適用: 時間序列資料

插值法（Interpolation）:
  線性插值: x_t = x_(t-1) + (t - t_(t-1)) × (x_(t+1) - x_(t-1)) / (t_(t+1) - t_(t-1))
  適用: 時序資料、平滑變化
```

**進階填補**:
```
KNN填補:
  x_missing = mean({x_j | j ∈ K nearest neighbors of i})
  找最相似的K個樣本,用其均值填補

多重插補（MICE）:
  1. 初始填補（均值/中位數）
  2. 逐列迭代建模預測缺失值
  3. 重複直到收斂
  適用: 缺失模式複雜、多特徵相關
```

### 2.3 異常值檢測

**統計方法（Z-score）**:
```
Z-score = (x - μ) / σ

判定規則:
  |Z| > 3: 異常值（99.7%資料在[-3σ, +3σ]內）
  |Z| > 2: 可疑值（95%資料在[-2σ, +2σ]內）

範例:
  資料: [10, 12, 11, 13, 100]
  μ = 29.2, σ = 39.6
  Z(100) = (100 - 29.2) / 39.6 = 1.79（非異常）
  # Z-score對異常值不敏感（μ和σ被拉扯）
```

**IQR方法（Interquartile Range）**:
```
IQR = Q₃ - Q₁

異常值定義:
  x < Q₁ - 1.5×IQR  或  x > Q₃ + 1.5×IQR

嚴格異常值（極端值）:
  x < Q₁ - 3×IQR  或  x > Q₃ + 3×IQR

範例:
  資料: [1, 2, 3, 4, 5, 100]
  Q₁ = 2, Q₃ = 4.5, IQR = 2.5
  上界 = 4.5 + 1.5×2.5 = 8.25
  100 > 8.25 -> 異常值✓
```

**孤立森林（Isolation Forest）**:
```
原理: 異常點更容易被孤立（需更少分割次數）

異常分數:
  s(x) = 2^(-E[h(x)] / c(n))

  其中:
  - h(x): 樣本x被孤立的平均路徑長度
  - c(n): 正常化因子
  - s接近1: 異常值
  - s接近0: 正常值

適用: 高維資料、複雜分佈
```

### 2.4 特徵編碼

**Label Encoding（序號編碼）**:
```
類別 -> 整數映射
{'Red': 0, 'Green': 1, 'Blue': 2}

優點: 節省記憶體、簡單
缺點: 引入錯誤順序資訊（Blue > Green > Red）

適用:
- 有序類別（如'Low' < 'Medium' < 'High'）
- 樹模型（決策樹、隨機森林、XGBoost）
```

**One-Hot Encoding（獨熱編碼）**:
```
類別變數 -> 二元向量

範例:
  顏色 = ['Red', 'Green', 'Blue']

  Red   -> [1, 0, 0]
  Green -> [0, 1, 0]
  Blue  -> [0, 0, 1]

公式（k個類別）:
  x_categorical ∈ {c₁, c₂, ..., cₖ}
  x_onehot = [I(x=c₁), I(x=c₂), ..., I(x=cₖ)]  # I為指示函數

優點: 無順序假設、適用線性模型
缺點: 高基數類別維度爆炸（如城市有1000種）

適用:
- 線性迴歸、邏輯迴歸、SVM、神經網路
- 類別數量少（< 50）
```

**Target Encoding（目標編碼）**:
```
類別 -> 該類別對應目標變數的統計量（均值/中位數）

範例（預測房價）:
  City      Price_mean    Encoded
  Taipei    5000萬        5000
  Taichung  2000萬        2000
  Kaohsiung 1800萬        1800

公式:
  x_encoded = E[y | x = category]

優點: 保留類別與目標的關係、降維
缺點: 易過擬合（需平滑處理）

平滑化（避免過擬合）:
  x_encoded = (n_category × mean_category + m × mean_global) / (n_category + m)

  其中 m 為平滑參數（通常10-100）

適用: 高基數類別、樹模型
```

**Frequency Encoding（頻率編碼）**:
```
類別 -> 該類別出現頻率

範例:
  City      Count  Frequency  Encoded
  Taipei    5000   0.50       0.50
  Taichung  3000   0.30       0.30
  Kaohsiung 2000   0.20       0.20

公式:
  x_encoded = count(x = category) / N

優點: 保留類別分佈資訊、降維
適用: 類別分佈不均、文本資料
```

### 2.5 特徵選擇

**過濾法（Filter Methods）**:
```
基於統計指標評分特徵,與模型無關

相關係數（Pearson）:
  r = Σ[(xᵢ - x̄)(yᵢ - ȳ)] / √[Σ(xᵢ - x̄)² × Σ(yᵢ - ȳ)²]

  |r| > 0.7: 強相關,保留
  |r| < 0.3: 弱相關,移除

卡方檢定（χ²）:
  χ² = Σ (Observed - Expected)² / Expected

  p-value < 0.05: 顯著相關,保留

互資訊（Mutual Information）:
  MI(X, Y) = Σₓ Σᵧ p(x,y) log[p(x,y) / (p(x)p(y))]

  MI越大,X和Y越相關

優點: 計算快速、模型無關
缺點: 忽略特徵間交互作用
```

**包裹法（Wrapper Methods）**:
```
基於模型效能選擇特徵子集

前向選擇（Forward Selection）:
  1. 從空集開始
  2. 逐個加入使模型效能提升最多的特徵
  3. 直到效能不再提升

後向消除（Backward Elimination）:
  1. 從全集開始
  2. 逐個移除對效能影響最小的特徵
  3. 直到效能顯著下降

遞迴特徵消除（RFE）:
  1. 訓練模型,計算特徵重要性
  2. 移除最不重要的特徵
  3. 重複直到達到目標特徵數

優點: 考慮特徵交互作用、效能導向
缺點: 計算成本高（需多次訓練模型）
```

**嵌入法（Embedded Methods）**:
```
模型訓練過程中自動選擇特徵

Lasso（L1正則化）:
  Loss = MSE + λ Σ|wᵢ|

  L1會使部分wᵢ=0,自動特徵選擇

樹模型特徵重要性:
  Feature_Importance = Σ (節點分裂增益 × 樣本比例)

  sklearn範例:
  from sklearn.ensemble import RandomForestClassifier
  model = RandomForestClassifier()
  model.fit(X_train, y_train)
  importances = model.feature_importances_

優點: 效率高（訓練即選擇）、考慮交互作用
```

### 2.6 降維技術

**主成分分析（PCA）**:
```
目標: 找k個正交主成分,最大化方差

步驟:
1. 資料標準化: X_scaled = (X - μ) / σ

2. 計算協方差矩陣:
   Cov = (1/N) X^T X

3. 特徵分解:
   Cov = VΛV^T
   其中 V為特徵向量矩陣, Λ為特徵值對角矩陣

4. 選擇前k個主成分（特徵值最大）:
   PC = XV_k  # V_k為前k個特徵向量

5. 降維:
   X_reduced = X × V_k  # (N, d) -> (N, k)

方差保留率:
  Variance_Retained = Σ(λ₁ to λₖ) / Σ(λ₁ to λₐ)

  通常選擇保留90-95%方差的k

優點: 降維、去相關、降噪
缺點: 線性假設、可解釋性降低
```

**t-SNE（t-Distributed Stochastic Neighbor Embedding）**:
```
目標: 保留局部鄰域結構的非線性降維

高維空間相似度（高斯分佈）:
  p_ij = exp(-||xᵢ - xⱼ||² / 2σ²) / Σₖ≠ᵢ exp(-||xᵢ - xₖ||² / 2σ²)

低維空間相似度（t分佈）:
  q_ij = (1 + ||yᵢ - yⱼ||²)^(-1) / Σₖ≠ᵢ (1 + ||yᵢ - yₖ||²)^(-1)

損失函數（KL散度）:
  KL(P||Q) = Σᵢ Σⱼ p_ij log(p_ij / q_ij)

優化: 梯度下降最小化KL散度

特點:
- 非線性、保留局部結構
- 僅用於視覺化（降至2D/3D）
- 計算成本高（O(N²)）
```

---

## 3. 對比矩陣 (15%, 300-900字)

### 3.1 特徵縮放方法對比

| 方法 | 公式 | 範圍 | 對異常值敏感度 | 適用模型 | 適用場景 |
|------|------|------|---------------|---------|---------|
| **標準化** | (x-μ)/σ | 無限 | 高 | 線性模型、SVM、神經網路 | 資料近似常態分佈 |
| **正規化** | (x-min)/(max-min) | [0,1] | 極高 | 神經網路、KNN | 需固定範圍、影像 |
| **Robust** | (x-median)/IQR | 無限 | 低 | 所有模型 | 包含異常值 |
| **Log轉換** | log(x+1) | [0,∞) | 中 | 樹模型、線性模型 | 長尾分佈、偏態 |

**不需縮放的模型**:決策樹、隨機森林、XGBoost（對特徵尺度不敏感）

### 3.2 缺失值處理方法對比

| 方法 | 優點 | 缺點 | 適用場景 | 資訊損失 |
|------|------|------|---------|---------|
| **刪除** | 簡單直接 | 損失樣本/特徵 | 缺失 < 5% | 高 |
| **均值填補** | 快速簡單 | 降低方差 | 數值型、MAR缺失 | 中 |
| **中位數填補** | 抗異常值 | 降低方差 | 數值型+異常值 | 中 |
| **眾數填補** | 適用類別型 | 增加偏差 | 類別型 | 中 |
| **KNN填補** | 保留相關性 | 計算成本高 | 複雜缺失模式 | 低 |
| **MICE** | 最精確 | 計算極慢 | 缺失 > 20% | 最低 |

**缺失機制**:
- **MCAR**（完全隨機缺失）:任何方法
- **MAR**（隨機缺失）:模型填補（KNN、MICE）
- **MNAR**（非隨機缺失）:領域知識+複雜模型

### 3.3 特徵編碼方法對比

| 編碼方法 | 類別數限制 | 維度變化 | 順序假設 | 適用模型 | 過擬合風險 |
|---------|-----------|---------|---------|---------|-----------|
| **Label** | 無 | 不變（1維） | 是 | 樹模型 | 低 |
| **One-Hot** | < 50 | 增加（k維） | 否 | 線性模型、神經網路 | 中 |
| **Target** | 無 | 不變（1維） | 否 | 樹模型 | 高 |
| **Frequency** | 無 | 不變（1維） | 否 | 所有模型 | 低 |
| **Binary** | 無 | log₂(k)維 | 否 | 樹模型 | 低 |

**選擇指南**:
- **低基數（< 10類）**: One-Hot
- **中基數（10-50類）**: One-Hot或Target
- **高基數（> 50類）**: Target、Frequency、Hashing

### 3.4 特徵選擇方法對比

| 方法 | 計算成本 | 是否考慮交互作用 | 模型依賴 | 適用場景 |
|------|---------|-----------------|---------|---------|
| **過濾法** | 低 | 否 | 無關 | 高維資料快速篩選 |
| **包裹法** | 極高 | 是 | 依賴 | 中小資料、追求最佳子集 |
| **嵌入法** | 中 | 是 | 依賴 | 訓練即選擇、實務首選 |

**實務推薦**:
1. **初步篩選**:過濾法移除明顯無關特徵（相關係數 < 0.1）
2. **主要選擇**:嵌入法（Lasso、樹模型特徵重要性）
3. **精細調優**:包裹法（RFE）微調最終特徵集

---

## 4. 實務應用 (20%, 400-1200字)

### 4.1 應用場景一:房價預測資料清洗

**場景描述**:房屋資料包含缺失值、異常值、不一致資料。

**實現要點**:
```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler

# 1. 載入資料
df = pd.read_csv('housing.csv')  # (10000, 15)

# 2. 探索缺失值
print(df.isnull().sum())
"""
Age: 500 (5%)
Area: 50 (0.5%)
Price: 100 (1%)
"""

# 3. 處理重複值
df = df.drop_duplicates()  # 移除10筆重複

# 4. 處理缺失值（分策略）
# 4.1 數值型-均值填補（Age正常分佈）
imputer_mean = SimpleImputer(strategy='mean')
df['Age'] = imputer_mean.fit_transform(df[['Age']])

# 4.2 數值型-中位數填補（Price有異常值）
imputer_median = SimpleImputer(strategy='median')
df['Price'] = imputer_median.fit_transform(df[['Price']])

# 4.3 KNN填補（Area與其他特徵相關）
imputer_knn = KNNImputer(n_neighbors=5)
df['Area'] = imputer_knn.fit_transform(df[['Area', 'Rooms', 'Age']])[:, 0]

# 5. 異常值檢測與處理（IQR方法）
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return df[(df[column] < lower) | (df[column] > upper)].index

# 檢測Price異常值
outliers_idx = detect_outliers_iqr(df, 'Price')
print(f"檢測到{len(outliers_idx)}個異常值")  # 150個

# 處理策略:上下界截斷（Winsorization）
Q1 = df['Price'].quantile(0.25)
Q3 = df['Price'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df['Price'] = df['Price'].clip(lower=lower, upper=upper)

# 6. 資料一致性檢查
# 6.1 Area應 > 0
df = df[df['Area'] > 0]

# 6.2 Age應在合理範圍
df = df[(df['Age'] >= 0) & (df['Age'] <= 100)]

# 7. 資料驗證
assert df.isnull().sum().sum() == 0, "仍有缺失值"
assert len(df) > 9500, "樣本損失過多"
print(f"清洗後資料: {df.shape}")  # (9840, 15)
```

**效果**:
- 缺失值: 650 -> 0
- 異常值: 截斷150個
- 樣本數: 10000 -> 9840（損失1.6%）

### 4.2 應用場景二:分類問題特徵工程

**場景描述**:客戶流失預測,包含數值型和類別型特徵。

**實現要點**:
```python
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# 1. 原始資料
"""
Age: 數值型
Income: 數值型（偏態分佈）
Gender: 類別型（M/F）
City: 類別型（50個城市）
Product: 類別型（ProductA/B/C）
Churn: 目標變數（0/1）
"""

# 2. 特徵工程策略

# 2.1 數值型特徵-Log轉換+標準化
df['Income_log'] = np.log1p(df['Income'])  # log(1+x)避免log(0)

scaler = StandardScaler()
df[['Age_scaled', 'Income_log_scaled']] = scaler.fit_transform(
    df[['Age', 'Income_log']]
)

# 2.2 類別型特徵-分策略編碼
# 低基數（2類）-> Label Encoding
le = LabelEncoder()
df['Gender_encoded'] = le.fit_transform(df['Gender'])  # M->0, F->1

# 中基數（3類）-> One-Hot Encoding
df_product = pd.get_dummies(df['Product'], prefix='Product')
df = pd.concat([df, df_product], axis=1)
# 產生: Product_A, Product_B, Product_C（3個二元特徵）

# 高基數（50類）-> Target Encoding
def target_encoding(df, column, target, smoothing=10):
    # 計算每個類別的目標均值
    target_mean = df.groupby(column)[target].mean()
    global_mean = df[target].mean()
    # 計算每個類別的樣本數
    counts = df.groupby(column).size()
    # 平滑化
    smoothed = (counts * target_mean + smoothing * global_mean) / (counts + smoothing)
    return df[column].map(smoothed)

df['City_encoded'] = target_encoding(df, 'City', 'Churn', smoothing=20)

# 2.3 特徵創建（多項式、交互作用）
df['Age_Income'] = df['Age_scaled'] * df['Income_log_scaled']  # 交互作用
df['Age_squared'] = df['Age_scaled'] ** 2  # 多項式

# 2.4 特徵選擇（Lasso）
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import SelectFromModel

X = df.drop('Churn', axis=1)
y = df['Churn']

lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X, y)

selector = SelectFromModel(lasso, prefit=True)
X_selected = selector.transform(X)
selected_features = X.columns[selector.get_support()]
print(f"選擇特徵: {list(selected_features)}")
# ['Age_scaled', 'Income_log_scaled', 'City_encoded', 'Age_Income']
```

**效果對比**:
| 特徵集 | 特徵數 | AUC | 訓練時間 |
|--------|--------|-----|---------|
| 原始特徵 | 5 | 0.72 | 1s |
| 編碼後 | 58 | 0.78 | 3s |
| +交互作用 | 65 | 0.82 | 4s |
| Lasso選擇 | 15 | 0.83 | 2s（最佳）|

### 4.3 應用場景三:高維資料降維

**場景描述**:影像分類,原始特徵784維（28×28像素）,降至50維。

**實現要點**:
```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# 1. 載入MNIST資料
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data  # (1797, 64)
y = digits.target  # (1797,)

# 2. PCA降維（64維 -> 20維）
pca = PCA(n_components=20)
X_pca = pca.fit_transform(X)

# 方差保留率
variance_ratio = pca.explained_variance_ratio_
cumsum_variance = np.cumsum(variance_ratio)
print(f"20個主成分保留方差: {cumsum_variance[-1]:.2%}")  # 91.2%

# 自動選擇主成分數（保留95%方差）
pca_auto = PCA(n_components=0.95)
X_pca_auto = pca_auto.fit_transform(X)
print(f"保留95%方差需{pca_auto.n_components_}個主成分")  # 29個

# 3. t-SNE視覺化（64維 -> 2維）
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X[:500])  # 僅取500樣本（計算快）

# 視覺化
plt.figure(figsize=(12, 5))

# PCA前兩個主成分
plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7)
plt.title('PCA (2D)')
plt.xlabel('PC1')
plt.ylabel('PC2')

# t-SNE
plt.subplot(1, 2, 2)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y[:500], cmap='tab10', alpha=0.7)
plt.title('t-SNE (2D)')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')

plt.tight_layout()
plt.savefig('dimensionality_reduction.png')
```

**效果對比**:
| 方法 | 維度 | 方差保留 | 計算時間 | 分類準確率（KNN） |
|------|------|---------|---------|-----------------|
| 原始 | 64 | 100% | - | 97.8% |
| PCA-20 | 20 | 91.2% | 0.1s | 96.5%（-1.3%） |
| PCA-29 | 29 | 95% | 0.1s | 97.2%（-0.6%） |
| t-SNE-2 | 2 | - | 15s | 85%（視覺化用） |

### 4.4 實作步驟（特徵工程標準流程）

**七步流程**:

1. **資料理解**: EDA探索、分佈分析、缺失值統計、相關性矩陣
2. **資料清洗**:
   - 移除重複值
   - 處理缺失值（均值/中位數/KNN/MICE）
   - 檢測異常值（IQR/Z-score/Isolation Forest）
3. **特徵轉換**:
   - 數值型: 縮放（標準化/正規化）、對數轉換（偏態）
   - 類別型: 編碼（Label/One-Hot/Target）
4. **特徵創建**:
   - 多項式特徵（x², x³）
   - 交互作用（x₁×x₂）
   - 領域特徵（如年齡分組、BMI計算）
5. **特徵選擇**:
   - 移除低方差特徵
   - 相關性篩選（移除高相關特徵對）
   - Lasso/樹模型特徵重要性
6. **降維（可選）**: PCA/LDA（高維資料 > 100維）
7. **驗證**: 訓練基準模型（如邏輯迴歸）,評估特徵品質

### 4.5 常見陷阱

**陷阱1: 資料洩漏（Data Leakage）**
- **問題**: 用全部資料計算均值再劃分訓練/測試集
- **解決**: 僅在訓練集上擬合（fit）,測試集僅轉換（transform）
```python
# 錯誤
scaler = StandardScaler()
X_all_scaled = scaler.fit_transform(X)  # 洩漏測試集資訊
X_train, X_test = train_test_split(X_all_scaled)

# 正確
X_train, X_test = train_test_split(X)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # 僅訓練集擬合
X_test_scaled = scaler.transform(X_test)  # 測試集使用同一轉換
```

**陷阱2: 過度填補缺失值**
- **問題**: 缺失比例 > 80%仍用均值填補
- **解決**: 缺失 > 50%直接刪除該特徵

**陷阱3: 高基數類別用One-Hot**
- **問題**: 1000個城市產生1000個特徵（維度爆炸）
- **解決**: 用Target Encoding或Frequency Encoding

**陷阱4: 未處理偏態分佈**
- **問題**: 收入資料長尾分佈,直接標準化效果差
- **解決**: Log轉換後再標準化

**陷阱5: 忽略時序特徵的時間順序**
- **問題**: 時序資料用前向填補會洩漏未來資訊
- **解決**: 訓練集僅用歷史資料填補

---

## 5. 記憶口訣 (10%, 200-600字)

### 5.1 核心口訣

**「清洗編碼縮放選,降維創造驗收關」**
- **清洗**: 缺失值、異常值、重複值、不一致
- **編碼**: 類別變數轉數值（Label/One-Hot/Target）
- **縮放**: 標準化、正規化（線性模型必須）
- **選**: 特徵選擇（過濾/包裹/嵌入）
- **降維**: PCA、t-SNE（高維->低維）
- **創造**: 多項式、交互作用、領域特徵
- **驗收**: 訓練基準模型驗證

**「缺失值填補三字訣:均中眾KNN」**
- **均**: 均值填補（數值型、常態分佈）
- **中**: 中位數填補（數值型、有異常值）
- **眾**: 眾數填補（類別型）
- **KNN**: KNN填補（複雜相關性）

### 5.2 記憶技巧

**特徵縮放選擇樹**:
```
資料分佈?
  ├─ 常態分佈 -> 標準化（Z-score）
  ├─ 均勻分佈 -> 正規化（Min-Max）
  ├─ 偏態分佈 -> Log轉換 + 標準化
  └─ 含異常值 -> Robust Scaling

模型類型?
  ├─ 線性/SVM/神經網路 -> 必須縮放
  ├─ 樹模型 -> 不需縮放
  └─ KNN/K-means -> 必須縮放（距離敏感）
```

**特徵編碼口訣**:「低基One-Hot,高基Target,樹模Label」
```
類別數?
  ├─ < 10類 -> One-Hot Encoding
  ├─ 10-50類 -> One-Hot或Target
  └─ > 50類 -> Target/Frequency Encoding

模型類型?
  ├─ 線性模型 -> One-Hot（無順序假設）
  ├─ 樹模型 -> Label或Target（接受數值）
  └─ 神經網路 -> Embedding（學習表示）
```

**異常值檢測速記**:「統計IQR,模型Isolation」
```
IQR方法（盒鬚圖）:
  下界 = Q₁ - 1.5×IQR
  上界 = Q₃ + 1.5×IQR
  超出範圍 -> 異常值

處理策略:
  1. 刪除（< 1%異常值）
  2. 截斷（Winsorization,1-5%）
  3. 轉換（Log縮小極端值）
  4. 單獨建模（異常值檢測任務）
```

**缺失值機制**:「MCAR隨便填,MAR模型填,MNAR要小心」
```
MCAR（完全隨機）: 任何填補方法
MAR（隨機）: KNN、MICE（考慮相關性）
MNAR（非隨機）: 領域知識+建模（缺失本身有意義）
```

### 5.3 快速回憶

**資料清洗檢查表**:
```python
# 1. 載入資料
df = pd.read_csv('data.csv')

# 2. 快速診斷
df.info()  # 資料類型、缺失值
df.describe()  # 統計摘要
df.isnull().sum()  # 缺失值計數
df.duplicated().sum()  # 重複值計數

# 3. 視覺化
df.hist(figsize=(15,10))  # 分佈
sns.heatmap(df.corr())  # 相關性
df.boxplot()  # 異常值
```

**特徵工程流水線**:
```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# 數值型處理
numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# 類別型處理
categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 組合
preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# 完整流水線
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

model.fit(X_train, y_train)
```

### 5.4 易混淆辨析

**標準化 vs 正規化**:
- **標準化**: (x-μ)/σ,範圍無限,適合常態分佈
- **正規化**: (x-min)/(max-min),範圍[0,1],適合固定範圍

**Label Encoding vs One-Hot Encoding**:
- **Label**: 類別->整數（Red=0,Green=1）,引入順序,樹模型用
- **One-Hot**: 類別->二元向量（Red=[1,0,0]）,無順序,線性模型用

**過濾法 vs 包裹法 vs 嵌入法**:
- **過濾**: 統計指標（相關性）,快速,模型無關
- **包裹**: 嘗試特徵子集（RFE）,慢,模型依賴
- **嵌入**: 訓練中選擇（Lasso）,中速,模型依賴

**PCA vs t-SNE**:
- **PCA**: 線性降維,可重建,保留全局結構,快速
- **t-SNE**: 非線性降維,不可重建,保留局部結構,慢,僅視覺化

**缺失值刪除 vs 填補**:
- **刪除**: 簡單但損失資料,適合缺失 < 5%
- **填補**: 保留樣本但引入偏差,適合缺失 > 5%

---

## 6. 自我驗證 (10%, 200-600字)

### 6.1 選擇題

**Q1:以下哪種特徵縮放方法對異常值最不敏感?**
A. 標準化（Z-score）
B. 正規化（Min-Max）
C. Robust Scaling
D. Log轉換

**Q2:高基數類別變數（1000個城市）最適合的編碼方法是?**
A. Label Encoding
B. One-Hot Encoding
C. Target Encoding
D. Binary Encoding

**Q3:以下哪個模型必須進行特徵縮放?**
A. 決策樹
B. 隨機森林
C. XGBoost
D. 邏輯迴歸

**Q4:PCA降維的主要目標是?**
A. 最大化類別間距離
B. 最大化方差
C. 最小化重建誤差
D. B和C都對

**Q5:資料洩漏（Data Leakage）最可能發生在?**
A. 用訓練集均值填補訓練集缺失值
B. 用全部資料計算均值後劃分訓練/測試集
C. 用測試集評估模型
D. 用驗證集選擇超參數

### 6.2 簡答題

**Q1:解釋為什麼線性模型（如邏輯迴歸）需要特徵縮放,而樹模型（如隨機森林）不需要?**

**Q2:描述處理缺失值的完整流程,並說明何時應該刪除、何時應該填補?**

### 6.3 答案解析

**選擇題答案**:

**A1:C（Robust Scaling）**
- **解析**:Robust Scaling使用中位數和IQR（四分位距）,這兩個統計量對異常值不敏感。標準化用均值和標準差（異常值拉扯）,正規化用最小值和最大值（異常值影響極大），Log轉換能縮小但仍受影響。

**A2:C（Target Encoding）**
- **解析**:1000個城市用One-Hot會產生1000個特徵（維度爆炸、計算成本高、稀疏性極高）。Target Encoding將類別映射為目標變數均值,保持1維,適合高基數。Label Encoding引入錯誤順序,Binary需log₂(1000)≈10維。

**A3:D（邏輯迴歸）**
- **解析**:邏輯迴歸基於線性組合w^Tx,特徵尺度差異大會導致梯度下降收斂慢（大特徵主導梯度）、權重難解釋。樹模型基於分割規則（x > threshold）,對特徵尺度不敏感。

**A4:D（B和C都對）**
- **解析**:PCA的兩個等價目標:
  1. 最大化投影後方差（主成分方向=最大方差方向）
  2. 最小化重建誤差（原始資料與降維後重建資料的差異）
  數學上可證明兩者等價。

**A5:B（用全部資料計算均值後劃分訓練/測試集）**
- **解析**:用全部資料計算統計量（均值、標準差）會洩漏測試集資訊到訓練集,導致過於樂觀的評估。正確做法:僅在訓練集上fit（計算統計量）,測試集用訓練集的統計量transform。

**簡答題答案**:

**A1:線性模型需要縮放 vs 樹模型不需要**

**原因**:

**線性模型需要縮放（如邏輯迴歸）**:

1. **梯度下降收斂問題**:
```
模型: ŷ = w₁x₁ + w₂x₂ + b

假設:
  x₁範圍[0, 1]（年齡/100）
  x₂範圍[0, 100000]（收入）

損失函數對x₂的梯度遠大於x₁:
  ∂L/∂w₂ >> ∂L/∂w₁

結果:
  - 學習率小 -> w₁收斂極慢
  - 學習率大 -> w₂震盪不收斂
  - 需特徵縮放使梯度量級一致
```

2. **權重可解釋性**:
```
未縮放:
  w₁ = 1000（年齡係數）
  w₂ = 0.001（收入係數）

看起來年齡重要,實際是尺度問題:
  1000 × (Age/100) vs 0.001 × Income

縮放後:
  w₁ = 0.5, w₂ = 0.8  # 可直接比較重要性
```

3. **正則化偏差**:
```
L1/L2正則化: λΣw²

大尺度特徵的權重被過度懲罰:
  w₂²（收入）>> w₁²（年齡）
  即使收入不重要也被懲罰更多

縮放後: 所有w在同一量級,正則化公平
```

**樹模型不需要縮放（如隨機森林）**:

1. **基於分割規則,尺度無關**:
```
決策樹分割:
  if Age > 30: ...
  if Income > 50000: ...

分割閾值自動適應特徵範圍:
  Age閾值在[0,100]
  Income閾值在[0,100000]

尺度不影響分割邏輯（大小關係保持）
```

2. **特徵重要性基於資訊增益**:
```
Gini不純度: 1 - Σpₖ²

不依賴特徵絕對值,僅依賴分類純度
縮放不改變類別分佈 -> 重要性不變
```

**總結**:
- **線性模型**: 梯度下降+權重解釋+正則化 -> 需縮放
- **樹模型**: 分割規則 -> 尺度無關 -> 不需縮放

**A2:缺失值處理完整流程**

**流程**:

**1. 缺失值診斷**:
```python
# 統計缺失比例
missing_ratio = df.isnull().sum() / len(df)
print(missing_ratio[missing_ratio > 0])

# 視覺化缺失模式
import missingno as msno
msno.matrix(df)  # 缺失值分佈
msno.heatmap(df)  # 缺失值相關性
```

**2. 判斷缺失機制**:
```
MCAR（完全隨機）: 缺失與任何變數無關
  檢測: 缺失樣本與非缺失樣本的其他特徵分佈相同
  處理: 任何方法（刪除或填補）

MAR（隨機）: 缺失與已觀測變數相關
  檢測: 缺失與某些特徵相關（如年長者更不願回答收入）
  處理: 模型填補（KNN、MICE）

MNAR（非隨機）: 缺失與未觀測值本身相關
  檢測: 缺失本身有意義（如高收入者拒答）
  處理: 領域知識+建模（或單獨類別'Unknown'）
```

**3. 刪除 vs 填補決策**:

**刪除場景**:
```
列刪除（Listwise Deletion）:
  ✓ 缺失比例極低（< 5%）
  ✓ 樣本量充足（> 10000）
  ✓ MCAR機制（隨機缺失）
  ✗ 缺失 > 10%（資訊損失過多）

行刪除（Column Deletion）:
  ✓ 特徵缺失 > 50%
  ✓ 該特徵不重要（相關性 < 0.1）
  ✓ 有其他替代特徵
  ✗ 核心特徵（即使缺失50%也需填補）
```

**填補場景**:
```
簡單填補（均值/中位數/眾數）:
  ✓ 缺失5-20%
  ✓ MCAR或MAR
  ✓ 快速原型開發
  ✗ 缺失 > 20%（引入偏差大）

進階填補（KNN/MICE）:
  ✓ 缺失10-50%
  ✓ MAR機制（與其他特徵相關）
  ✓ 特徵間相關性強
  ✗ 計算資源有限

領域填補:
  ✓ MNAR機制
  ✓ 缺失本身有業務意義
  範例:
    - 問卷「拒絕回答」-> 類別'Refused'
    - 網站「未登入」-> -1
```

**4. 填補方法選擇**:
```
數值型特徵:
  - 常態分佈無異常值 -> 均值
  - 偏態或有異常值 -> 中位數
  - 與其他特徵相關 -> KNN/MICE
  - 時序資料 -> 線性插值/前向填補

類別型特徵:
  - 眾數填補
  - 新增'Missing'類別（缺失有意義）
  - KNN（用最相似樣本的類別）
```

**5. 驗證填補效果**:
```python
# 比較填補前後分佈
df['Age'].hist(label='Original', alpha=0.5)
df['Age_imputed'].hist(label='Imputed', alpha=0.5)
plt.legend()

# 比較模型效能（填補 vs 刪除）
model_delete = train_model(df_delete)  # 刪除缺失樣本
model_impute = train_model(df_impute)  # 填補後訓練
compare_performance(model_delete, model_impute)
```

**決策樹**:
```
缺失比例?
  ├─ < 5% -> 列刪除
  ├─ 5-20% -> 簡單填補（均值/中位數/眾數）
  ├─ 20-50% -> 進階填補（KNN/MICE）
  └─ > 50% -> 行刪除（或領域知識填補）

缺失機制?
  ├─ MCAR -> 刪除或簡單填補
  ├─ MAR -> KNN/MICE
  └─ MNAR -> 領域知識+單獨類別

資料類型?
  ├─ 數值型 -> 均值/中位數/KNN
  ├─ 類別型 -> 眾數/'Missing'類別
  └─ 時序 -> 插值/前向填補
```

### 6.4 易錯點提醒

**易錯點1:資料洩漏（在全部資料上縮放）**
- **錯誤**: scaler.fit(X_all)洩漏測試集
- **提醒**: 僅fit訓練集,transform測試集

**易錯點2:高基數類別用One-Hot**
- **錯誤**: 1000個城市產生1000維
- **提醒**: 用Target Encoding或Frequency Encoding

**易錯點3:忽略缺失機制直接刪除**
- **錯誤**: MNAR缺失（高收入者拒答）直接刪除導致樣本偏差
- **提醒**: 分析缺失機制,MNAR需謹慎處理

**易錯點4:樹模型也進行特徵縮放**
- **錯誤**: 隨機森林訓練前標準化（浪費時間）
- **提醒**: 樹模型對尺度不敏感,無需縮放

**易錯點5:PCA前未標準化**
- **錯誤**: 大尺度特徵主導主成分
- **提醒**: PCA前必須標準化（除非有特殊原因）
